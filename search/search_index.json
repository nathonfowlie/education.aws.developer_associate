{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS Developer Associate 2021 Exam Prep \u00b6 Living documentation that I'm using to prepare for the AWS Developer Associate exam. Needs More Focus (Prioritised) \u00b6 DynamoDB backup & recovery options. Elastic Beanstalk Lambda. API Gateway. CloudFormation resource types. CodeDeploy lifecycle hooks. Ec2 spot instance management. CloudFront. DynamoDB.","title":"Home"},{"location":"#aws-developer-associate-2021-exam-prep","text":"Living documentation that I'm using to prepare for the AWS Developer Associate exam.","title":"AWS Developer Associate 2021 Exam Prep"},{"location":"#needs-more-focus-prioritised","text":"DynamoDB backup & recovery options. Elastic Beanstalk Lambda. API Gateway. CloudFormation resource types. CodeDeploy lifecycle hooks. Ec2 spot instance management. CloudFront. DynamoDB.","title":"Needs More Focus (Prioritised)"},{"location":"cicd/codebuild/","text":"CodeBuild \u00b6 Overview \u00b6 Fully managed build service. Only pay for the time to complete a build. Uses docker under the hood. Can be extended via custom docker images. Source code from CodeCommit, GitHub or Bitbucket. Build steps are defined in code (buildspec.yml in the vcs root). Can sit in CodeBuild, or CodePipeline. Build logs are stored in S3 or CLoudWatch logs. Can be re-producted locally to troubleshoot errors. Build metrics can be captured. S3 buckets can be used as a temporary cache for build dependencies to improve build times. Needs an IAM service role to get permission to perform actions against AWS API/resources. Provide a VPC configuration to access VPC resources from CodeBuild. Supported Environments \u00b6 Java Ruby Python Go Node.js Android .NET Core PHP Docker (whatever you want) Security \u00b6 KMS to encrypt artifacts. IAM for build permissions. VPC for network security. CloudTrail for API call auditing. Reference SSM parameters/Secrets Manager secrets in environment variables. Notifications \u00b6 CloudWatch alarms to detect failed builds & trigger notifications. CloudWatch Events/Lambda as a glue for notifications. Generate SNS notifications. BuildSpec \u00b6 buildspec.yml must be at the root of the code. BuildSpec Layout \u00b6 Define environment variables Plain-text Secrets (using SSM parameter store) Phases Install: Resolve dependencies Pre-build: Final prep before build execution. Build: Build the code. Post-build: Final build actions (zip build output etc) Artifacts What to upload to S3 (encrypted with KMS) Cache Files to be cached to S3 to improve build performance (eg: build dependencies) Local Build \u00b6 Uses docker. Need to install the CodeBuild Agent. CodeBuild in VPCs \u00b6 By default, containers are built outside of the VPC, and can't access VPC resources. Specify a VPC ID, subnet ids, security group ids etc in order to access them. Public VPCs can't (shouldn't) have an internet connection. Use a private subnet with 0.0.0.0/0 destination as the target NAT gateway.","title":"CodeBuild"},{"location":"cicd/codebuild/#codebuild","text":"","title":"CodeBuild"},{"location":"cicd/codebuild/#overview","text":"Fully managed build service. Only pay for the time to complete a build. Uses docker under the hood. Can be extended via custom docker images. Source code from CodeCommit, GitHub or Bitbucket. Build steps are defined in code (buildspec.yml in the vcs root). Can sit in CodeBuild, or CodePipeline. Build logs are stored in S3 or CLoudWatch logs. Can be re-producted locally to troubleshoot errors. Build metrics can be captured. S3 buckets can be used as a temporary cache for build dependencies to improve build times. Needs an IAM service role to get permission to perform actions against AWS API/resources. Provide a VPC configuration to access VPC resources from CodeBuild.","title":"Overview"},{"location":"cicd/codebuild/#supported-environments","text":"Java Ruby Python Go Node.js Android .NET Core PHP Docker (whatever you want)","title":"Supported Environments"},{"location":"cicd/codebuild/#security","text":"KMS to encrypt artifacts. IAM for build permissions. VPC for network security. CloudTrail for API call auditing. Reference SSM parameters/Secrets Manager secrets in environment variables.","title":"Security"},{"location":"cicd/codebuild/#notifications","text":"CloudWatch alarms to detect failed builds & trigger notifications. CloudWatch Events/Lambda as a glue for notifications. Generate SNS notifications.","title":"Notifications"},{"location":"cicd/codebuild/#buildspec","text":"buildspec.yml must be at the root of the code.","title":"BuildSpec"},{"location":"cicd/codebuild/#buildspec-layout","text":"Define environment variables Plain-text Secrets (using SSM parameter store) Phases Install: Resolve dependencies Pre-build: Final prep before build execution. Build: Build the code. Post-build: Final build actions (zip build output etc) Artifacts What to upload to S3 (encrypted with KMS) Cache Files to be cached to S3 to improve build performance (eg: build dependencies)","title":"BuildSpec Layout"},{"location":"cicd/codebuild/#local-build","text":"Uses docker. Need to install the CodeBuild Agent.","title":"Local Build"},{"location":"cicd/codebuild/#codebuild-in-vpcs","text":"By default, containers are built outside of the VPC, and can't access VPC resources. Specify a VPC ID, subnet ids, security group ids etc in order to access them. Public VPCs can't (shouldn't) have an internet connection. Use a private subnet with 0.0.0.0/0 destination as the target NAT gateway.","title":"CodeBuild in VPCs"},{"location":"cicd/codecommit/","text":"CodeCommit \u00b6 Overview \u00b6 Version control (git). Repos are private. No size limit. Integrated with Jenkins, CodeBuild, other CI tools. Security \u00b6 Authentication \u00b6 Standard git commands. Git auth is via SSH keys, or HTTPS. Supports MFA. No SSH option for the root user. Must use an IAM user. There's a maximum of 2 sets of HTTPS git credentials permitted per IAM user. Authorization \u00b6 IAM polices to manage user/roles access to repos. Encryption \u00b6 Encryption at rest using KMS. Encryption in transit (HTTPS or SSH). Cross Account Access \u00b6 Use IAM roles, and AWS STS (with the AssumeRole API). Notifications \u00b6 Can be triggered using AWS SNS, AWS Lambda, or AWS CloudWatch Event rules. Use Cases \u00b6 Service Use Cases AWS SNS - Branch deletion - Trigger for push to master - Notify external build system. AWS Lambda - Branch deletion - Trigger for push to master - Notify external build system. AWS CloudWatch - Trigger for pull requests - Commit comment events - Event rules go into an SNS topic.","title":"CodeCommit"},{"location":"cicd/codecommit/#codecommit","text":"","title":"CodeCommit"},{"location":"cicd/codecommit/#overview","text":"Version control (git). Repos are private. No size limit. Integrated with Jenkins, CodeBuild, other CI tools.","title":"Overview"},{"location":"cicd/codecommit/#security","text":"","title":"Security"},{"location":"cicd/codecommit/#authentication","text":"Standard git commands. Git auth is via SSH keys, or HTTPS. Supports MFA. No SSH option for the root user. Must use an IAM user. There's a maximum of 2 sets of HTTPS git credentials permitted per IAM user.","title":"Authentication"},{"location":"cicd/codecommit/#authorization","text":"IAM polices to manage user/roles access to repos.","title":"Authorization"},{"location":"cicd/codecommit/#encryption","text":"Encryption at rest using KMS. Encryption in transit (HTTPS or SSH).","title":"Encryption"},{"location":"cicd/codecommit/#cross-account-access","text":"Use IAM roles, and AWS STS (with the AssumeRole API).","title":"Cross Account Access"},{"location":"cicd/codecommit/#notifications","text":"Can be triggered using AWS SNS, AWS Lambda, or AWS CloudWatch Event rules.","title":"Notifications"},{"location":"cicd/codecommit/#use-cases","text":"Service Use Cases AWS SNS - Branch deletion - Trigger for push to master - Notify external build system. AWS Lambda - Branch deletion - Trigger for push to master - Notify external build system. AWS CloudWatch - Trigger for pull requests - Commit comment events - Event rules go into an SNS topic.","title":"Use Cases"},{"location":"cicd/codedeploy/","text":"CodeDeploy \u00b6 Overview \u00b6 Each EC2/On-Prem machine needs to be running the CodeDeploy agent. Agents poll CodeDeploy for changes. CodeDeploy sends the appspec.yml file to the agent. The application is pulled from GitHub or S3. EC2 will run deployment instructions CodeDeploy Agent reports the success/failure of the deployment. EC2 instances are grouped by deployment group (dev, test, prod etc). Can integrate with CodePipeline, and use CodePipeline artifacts. Can use existing tools, applications. Integrated with auto-scaling. Supports Blue/Green deployments (EC2 only). Supports Lambda deployments. Doesn't provision resources. Main Components \u00b6 Application: Unique name Compute platform: EC2, On-Premise (hosts need to be tagged) or Lambda Deployment Configuration: Deployment rules for success/failure Deployment Group: group tagged instances. Deployment Type: In-place, or Blue/Green. IAM instance profile: To give EC2 permissions to pull from S3 or GitHub. Application Revision: app code + appspec.yml file. Service Role: Role for CodeDeply to perform what it needs. Target Revision: Target application version. AppSpec.yml \u00b6 File section: Where to get the source files from S3/GitHub. Hooks: Instructions on how to deploy the new version (have timeouts). The order is: ApplicationStop DownloadBundle BeforeInstall AfterInstall ApplicationStart ValidateService BeforeAllowTraffic AllowTraffic AfterAllowTraffic Example \u00b6 version: 0.0 os: linux files: - source: /index.html destination: /var/www/html/ hooks: BeforeInstall: - location: scripts/install_dependencies timeout: 300 runas: root - location: scripts/start_server timeout: 300 runsas: root ApplicationStop: - location: scripts/stop_server timeout: 300 runsas: root Deployment Configuration \u00b6 One instance at a time. Stop if an instance fails. Deploy to 50% of instances. All at once. Good for dev. Custom: min healthy hosts etc. Deployment Failures \u00b6 Instances stay in \"Failed\" state. New deployments will be deployed to failed state instances first. Rollback by re-deploying the old deployment or enable automated rollback. Deployment Targets \u00b6 Set of EC2 instances with tags Directly to an ASG Mix of ASG/tags to build deployment segments. Customised scripts with DEPLOYMENT_GROUP_NAME environment variables (if in dev, foo, else if in prod, bar). EC2 \u00b6 Define how to deploy the application using appspec.yml Will do in-place updates/blue-green deployment to EC2 instances. ASG \u00b6 In-Place Updates \u00b6 Update the existing EC2 instances. New instances created by an ASG will also get the automated deployments. Blue/Green Deployments \u00b6 A new ASG will be created with identical settings to the existing ASG. Can choose how long to keep the old instances. Must be using an ELB. Full control over deployment steps. Rollbacks \u00b6 Can specify automated rollbacks. When deployment fails. When CloudWatch alarm is triggered. Disable rollbacks completely. The last known good revision is re-deployed as a new deployment, with a new version id.","title":"CodeDeploy"},{"location":"cicd/codedeploy/#codedeploy","text":"","title":"CodeDeploy"},{"location":"cicd/codedeploy/#overview","text":"Each EC2/On-Prem machine needs to be running the CodeDeploy agent. Agents poll CodeDeploy for changes. CodeDeploy sends the appspec.yml file to the agent. The application is pulled from GitHub or S3. EC2 will run deployment instructions CodeDeploy Agent reports the success/failure of the deployment. EC2 instances are grouped by deployment group (dev, test, prod etc). Can integrate with CodePipeline, and use CodePipeline artifacts. Can use existing tools, applications. Integrated with auto-scaling. Supports Blue/Green deployments (EC2 only). Supports Lambda deployments. Doesn't provision resources.","title":"Overview"},{"location":"cicd/codedeploy/#main-components","text":"Application: Unique name Compute platform: EC2, On-Premise (hosts need to be tagged) or Lambda Deployment Configuration: Deployment rules for success/failure Deployment Group: group tagged instances. Deployment Type: In-place, or Blue/Green. IAM instance profile: To give EC2 permissions to pull from S3 or GitHub. Application Revision: app code + appspec.yml file. Service Role: Role for CodeDeply to perform what it needs. Target Revision: Target application version.","title":"Main Components"},{"location":"cicd/codedeploy/#appspecyml","text":"File section: Where to get the source files from S3/GitHub. Hooks: Instructions on how to deploy the new version (have timeouts). The order is: ApplicationStop DownloadBundle BeforeInstall AfterInstall ApplicationStart ValidateService BeforeAllowTraffic AllowTraffic AfterAllowTraffic","title":"AppSpec.yml"},{"location":"cicd/codedeploy/#example","text":"version: 0.0 os: linux files: - source: /index.html destination: /var/www/html/ hooks: BeforeInstall: - location: scripts/install_dependencies timeout: 300 runas: root - location: scripts/start_server timeout: 300 runsas: root ApplicationStop: - location: scripts/stop_server timeout: 300 runsas: root","title":"Example"},{"location":"cicd/codedeploy/#deployment-configuration","text":"One instance at a time. Stop if an instance fails. Deploy to 50% of instances. All at once. Good for dev. Custom: min healthy hosts etc.","title":"Deployment Configuration"},{"location":"cicd/codedeploy/#deployment-failures","text":"Instances stay in \"Failed\" state. New deployments will be deployed to failed state instances first. Rollback by re-deploying the old deployment or enable automated rollback.","title":"Deployment Failures"},{"location":"cicd/codedeploy/#deployment-targets","text":"Set of EC2 instances with tags Directly to an ASG Mix of ASG/tags to build deployment segments. Customised scripts with DEPLOYMENT_GROUP_NAME environment variables (if in dev, foo, else if in prod, bar).","title":"Deployment Targets"},{"location":"cicd/codedeploy/#ec2","text":"Define how to deploy the application using appspec.yml Will do in-place updates/blue-green deployment to EC2 instances.","title":"EC2"},{"location":"cicd/codedeploy/#asg","text":"","title":"ASG"},{"location":"cicd/codedeploy/#in-place-updates","text":"Update the existing EC2 instances. New instances created by an ASG will also get the automated deployments.","title":"In-Place Updates"},{"location":"cicd/codedeploy/#bluegreen-deployments","text":"A new ASG will be created with identical settings to the existing ASG. Can choose how long to keep the old instances. Must be using an ELB. Full control over deployment steps.","title":"Blue/Green Deployments"},{"location":"cicd/codedeploy/#rollbacks","text":"Can specify automated rollbacks. When deployment fails. When CloudWatch alarm is triggered. Disable rollbacks completely. The last known good revision is re-deployed as a new deployment, with a new version id.","title":"Rollbacks"},{"location":"cicd/codepipeline/","text":"CodePipeline \u00b6 Overview \u00b6 Continuous delivery Visual workflow Pull source from CodeCommit, ECR, S3, or GitHub. Build using CodeBuild, Jenkins etc. Load test using 3 rd party tools Deploy with CloudFormation, CodeDeploy, Beanstalk, Service Catalog, ECS, ECS (Blue/Green) or S3. Supports sequential and parallel actions. Supports manual approval. Action Group \u00b6 Defined within a stage. It's a build step. CodePipeline Artifacts \u00b6 Files output by each stage, for use in the next stage. Artifacts are stored in S3. S3 bucket can be created automatically, or explicitly defined. Change Detection \u00b6 CloudWatch events. This is the faster & preferred approach. Periodically poll CodePipeline for changes. Security \u00b6 Use a service role (IAM) to give the pipeline permissions to interact with AWS API/resources. Troubleshooting \u00b6 CodePipeline state changes trigger a CloudWatch event, which can create a SNS notification. Failed pipelines. Cancelled stages. Information about failed stages can be found through the console. AWS CloudTrail to audit AWS API calls. If the pipeline can't perform an action, check that the \"IAM Service Role\" has enough permissions in the IAM policy.","title":"CodePipeline"},{"location":"cicd/codepipeline/#codepipeline","text":"","title":"CodePipeline"},{"location":"cicd/codepipeline/#overview","text":"Continuous delivery Visual workflow Pull source from CodeCommit, ECR, S3, or GitHub. Build using CodeBuild, Jenkins etc. Load test using 3 rd party tools Deploy with CloudFormation, CodeDeploy, Beanstalk, Service Catalog, ECS, ECS (Blue/Green) or S3. Supports sequential and parallel actions. Supports manual approval.","title":"Overview"},{"location":"cicd/codepipeline/#action-group","text":"Defined within a stage. It's a build step.","title":"Action Group"},{"location":"cicd/codepipeline/#codepipeline-artifacts","text":"Files output by each stage, for use in the next stage. Artifacts are stored in S3. S3 bucket can be created automatically, or explicitly defined.","title":"CodePipeline Artifacts"},{"location":"cicd/codepipeline/#change-detection","text":"CloudWatch events. This is the faster & preferred approach. Periodically poll CodePipeline for changes.","title":"Change Detection"},{"location":"cicd/codepipeline/#security","text":"Use a service role (IAM) to give the pipeline permissions to interact with AWS API/resources.","title":"Security"},{"location":"cicd/codepipeline/#troubleshooting","text":"CodePipeline state changes trigger a CloudWatch event, which can create a SNS notification. Failed pipelines. Cancelled stages. Information about failed stages can be found through the console. AWS CloudTrail to audit AWS API calls. If the pipeline can't perform an action, check that the \"IAM Service Role\" has enough permissions in the IAM policy.","title":"Troubleshooting"},{"location":"cicd/codestar/","text":"CodeStar \u00b6 Overview \u00b6 Integrated solution that re-groups GitHub, CodeCommit, CodeBuild, CodeDeploy, CloudFormation, CodePipeline and CloudWatch. Quick creation of CI/CD ready projects for EC2, Lambda, Beanstalk. Has issue tracking integration with Jira/Github. Integrates with Cloud9 for a Web IDE One dashboard to view all components. Free, pay for usage of other services. Limited customization.","title":"CodeStar"},{"location":"cicd/codestar/#codestar","text":"","title":"CodeStar"},{"location":"cicd/codestar/#overview","text":"Integrated solution that re-groups GitHub, CodeCommit, CodeBuild, CodeDeploy, CloudFormation, CodePipeline and CloudWatch. Quick creation of CI/CD ready projects for EC2, Lambda, Beanstalk. Has issue tracking integration with Jira/Github. Integrates with Cloud9 for a Web IDE One dashboard to view all components. Free, pay for usage of other services. Limited customization.","title":"Overview"},{"location":"developing/appsync/","text":"AppSync \u00b6 Overview \u00b6 Managed service using GraphQL. GraphQL makes it easy to perform data queries. Can combine data from multiple sources: NoSQL, RDS, HTTP APIs. Integrates with DynamoDB, Aurora, Elasticsearch, Lambda etc. Retrieve real-time data with WebSockets or MQTT on WebSocket (AppSync, Application Load Balancer, API Gateway). Local data access and data sync for mobile apps (offline sync). Uses resolvers to communicate with data sources. Integrated with CloudWatch metrics & logs. Use CloudFront in front of AppSync to support HTTPS or use a custom domain. GraphQL \u00b6 GraphQL schema defines the data structure (objects). Response is in JSON. Security \u00b6 There's four ways to authorize applications: API_KEY AWS_IAM: IAM users/roles/cross-account access. OPENID_CONNECT: OpenID Connect provider/JSON web token. AMAZONG_COGNITO_USER_POOLS.","title":"AppSync"},{"location":"developing/appsync/#appsync","text":"","title":"AppSync"},{"location":"developing/appsync/#overview","text":"Managed service using GraphQL. GraphQL makes it easy to perform data queries. Can combine data from multiple sources: NoSQL, RDS, HTTP APIs. Integrates with DynamoDB, Aurora, Elasticsearch, Lambda etc. Retrieve real-time data with WebSockets or MQTT on WebSocket (AppSync, Application Load Balancer, API Gateway). Local data access and data sync for mobile apps (offline sync). Uses resolvers to communicate with data sources. Integrated with CloudWatch metrics & logs. Use CloudFront in front of AppSync to support HTTPS or use a custom domain.","title":"Overview"},{"location":"developing/appsync/#graphql","text":"GraphQL schema defines the data structure (objects). Response is in JSON.","title":"GraphQL"},{"location":"developing/appsync/#security","text":"There's four ways to authorize applications: API_KEY AWS_IAM: IAM users/roles/cross-account access. OPENID_CONNECT: OpenID Connect provider/JSON web token. AMAZONG_COGNITO_USER_POOLS.","title":"Security"},{"location":"developing/athena/","text":"Athena \u00b6 Overview \u00b6 Serverless services to perform analytics against S3 files. Can use SQL to query files. Has a JDBC/ODBC driver. Charged per query and amount of data scanned. Supports VSC, JSON, ORC, Avro and Parquet. Uses Presto in the back-end. Use Cases \u00b6 Business Intelligence/Analytics reporting. Analyze & Query VPC flow logs. ELB logs. CloudTrail logs.","title":"Athena"},{"location":"developing/athena/#athena","text":"","title":"Athena"},{"location":"developing/athena/#overview","text":"Serverless services to perform analytics against S3 files. Can use SQL to query files. Has a JDBC/ODBC driver. Charged per query and amount of data scanned. Supports VSC, JSON, ORC, Avro and Parquet. Uses Presto in the back-end.","title":"Overview"},{"location":"developing/athena/#use-cases","text":"Business Intelligence/Analytics reporting. Analyze & Query VPC flow logs. ELB logs. CloudTrail logs.","title":"Use Cases"},{"location":"developing/cli/","text":"Command Line Interface (CLI) \u00b6 Overview \u00b6 There's several ways to develop on AWS - AWS CLI from local machine or EC2 instance. AWS SDK from local machine or EC2 instance. AWS Instance Metadata Server for EC2. Configuration \u00b6 Local Environment \u00b6 Open up IAM console and select the Security Credentials tab. Click Create access key . Save the details somewhere secure. Run aws configure in a command prompt. Provide the access key and secret access key when prompted. Set the default region (default: us-east-1 ). Select the default output format (default: json ). Config files are written to $HOME/.aws/config and $HOME/.aws/credentials . EC2 Instance \u00b6 Use an IAM role to allow the EC2 instance to make certain API calls, don't store credentials on the instance. EC2 instances will use the profiles/roles automatically. Credentials Provider Chain \u00b6 The CLI looks for credentials in the following order: Command line options ( --region , --output , and --profile ). Environment variables ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , AWS_SESSION_TOKEN ). Credentials file ( $HOME/.aws/credentials ). Configuration file ( $HOME/.aws/config ). Container credentials (for ECS tasks). Instance profile credentials (for EC2 instance profiles). Managing Profiles \u00b6 Additional profiles can be created to make it easy to switch between different accounts. Each profile is stored as a ini section in $HOME/.aws/config and $HOME/.aws/credentials . $ aws configure --profile some_name AWS Access Key ID [None]: <some value> AWS Secret Access Key [None]: <some value> Default region name [None]: <some value> Default output format [None]: <some value> $ aws s3 ls --profile some_profile some_command ... Multi-factor Authentication (MFA) \u00b6 Use IAM to assign a MFA device to the user. Create a temporary session aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code code-from-token --duration-seconds 3600 Run aws configure again and provide the temporary credentials. aws configure --profile some_profile AWS Access Key ID [None]: <value from returned json> AWS Secret Access Key [None]: <value from returned json> Default region name [None]: <whatever> Default output format [None]: <whatever> Edit $HOME/.aws/credentials and add the session token: [some_profile] aws_acess_key_id = <value from returned json> aws_secret_access_key = <value from returned json> aws_session_token = <value from returned json> Troubleshooting \u00b6 If you get the error aws: command found , Check that the aws executable is on the PATH. Authorization errors can be decoded using STS aws sts decode-authorization-message --encoded-message <value>","title":"Command Line Interface (CLI)"},{"location":"developing/cli/#command-line-interface-cli","text":"","title":"Command Line Interface (CLI)"},{"location":"developing/cli/#overview","text":"There's several ways to develop on AWS - AWS CLI from local machine or EC2 instance. AWS SDK from local machine or EC2 instance. AWS Instance Metadata Server for EC2.","title":"Overview"},{"location":"developing/cli/#configuration","text":"","title":"Configuration"},{"location":"developing/cli/#local-environment","text":"Open up IAM console and select the Security Credentials tab. Click Create access key . Save the details somewhere secure. Run aws configure in a command prompt. Provide the access key and secret access key when prompted. Set the default region (default: us-east-1 ). Select the default output format (default: json ). Config files are written to $HOME/.aws/config and $HOME/.aws/credentials .","title":"Local Environment"},{"location":"developing/cli/#ec2-instance","text":"Use an IAM role to allow the EC2 instance to make certain API calls, don't store credentials on the instance. EC2 instances will use the profiles/roles automatically.","title":"EC2 Instance"},{"location":"developing/cli/#credentials-provider-chain","text":"The CLI looks for credentials in the following order: Command line options ( --region , --output , and --profile ). Environment variables ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , AWS_SESSION_TOKEN ). Credentials file ( $HOME/.aws/credentials ). Configuration file ( $HOME/.aws/config ). Container credentials (for ECS tasks). Instance profile credentials (for EC2 instance profiles).","title":"Credentials Provider Chain"},{"location":"developing/cli/#managing-profiles","text":"Additional profiles can be created to make it easy to switch between different accounts. Each profile is stored as a ini section in $HOME/.aws/config and $HOME/.aws/credentials . $ aws configure --profile some_name AWS Access Key ID [None]: <some value> AWS Secret Access Key [None]: <some value> Default region name [None]: <some value> Default output format [None]: <some value> $ aws s3 ls --profile some_profile some_command ...","title":"Managing Profiles"},{"location":"developing/cli/#multi-factor-authentication-mfa","text":"Use IAM to assign a MFA device to the user. Create a temporary session aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code code-from-token --duration-seconds 3600 Run aws configure again and provide the temporary credentials. aws configure --profile some_profile AWS Access Key ID [None]: <value from returned json> AWS Secret Access Key [None]: <value from returned json> Default region name [None]: <whatever> Default output format [None]: <whatever> Edit $HOME/.aws/credentials and add the session token: [some_profile] aws_acess_key_id = <value from returned json> aws_secret_access_key = <value from returned json> aws_session_token = <value from returned json>","title":"Multi-factor Authentication (MFA)"},{"location":"developing/cli/#troubleshooting","text":"If you get the error aws: command found , Check that the aws executable is on the PATH. Authorization errors can be decoded using STS aws sts decode-authorization-message --encoded-message <value>","title":"Troubleshooting"},{"location":"developing/cloudformation/","text":"CloudFormation \u00b6 Overview \u00b6 Declarative way to define infrastructure. Templates are uploaded to S3. Stacks are identified by a name. Resources created by the stack are given a tag to identify the stack that created them. When a stack is deleted, all artifacts created as part of the stack will be deleted. Use CLI to deploy templates. Benefits \u00b6 Infrastructure as Code \u00b6 No manual creation, good for control. All code is version controlled. Infra changes can be peer reviewed. Cost \u00b6 Each resource in the stack is tagged so it's easy to see how much a stack costs. Costs can be estimated using the CloudFormation template. Can safely automate the deletion of templates outside of business hours to save money. Productivity \u00b6 Destroy and re-create infra as needed. Automated generation of infrastructure diagrams. Declarative programming, so no need to figure out the ordering and orchestration). Separation of concern \u00b6 Create many stacks for many apps/layers (vpc, network, app etc). Re-usable \u00b6 Leverage existing templates, don't re-invent the wheel. Building Blocks \u00b6 Template Components \u00b6 Resources: mandatory section Parameters: Dynamic template inputs Mappings: Static inputs Outputs: References what was created. Conditions: Conditions to control resource creation Metadata: Template Helpers \u00b6 References: Link to sections in the template Functions: Transform template data. Resources \u00b6 AWS components that will be created/configured. Resources are declared and can reference each other. AWS figures out the order that resources should be created/updated/deleted. Over 224 different types of resources (AWS::product-name::data-type). Parameters \u00b6 Provide inputs to CF templates. Parameters have a type, description, constraints, constraint description, min/max length, min/max value, defaults, allow values, allowed patterns and no echo toggle. Reference a parameter using !Ref (example: !Ref MyVPC ). Pseudo parameters can be used at any time and enabled by default (AWS::AccountId, AWS::Region, AWS::StackName etc). Mappings \u00b6 Static values defined in the CF template. Use !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] to reference mapping values (example: !FindInMap [RegionMap, !Ref \"AWS::Region\", 32] ). Example \u00b6 RegionMap: us-east-1: \"32\": \"ami-6411e20d\" \"64\": \"ami-7a11e213\" us-west-1: \"32\": \"ami-c9c7978c\" \"64\": \"ami-31c2f645\" Outputs \u00b6 Optional If they're exported, they can be consumed in other stacks. Good for separating stacks (refer to VPC id created by the the network stack, in the application stack). Can't delete a stack if its outputs are referenced somewhere else. Example \u00b6 # Stack 1 Outputs: StackSSHSecurityGroup: Description: The SSH Security Group for our Company. Value: !Ref MyCompanyWideSSHSecurityGroup Export: Name: SSHSecurityGroup # Stack 2 Resources: MySecureInstance: Type: AWS::EC2::Instance Properties: SecurityGroups: - !ImportValue SSHSecurityGroup Conditions \u00b6 Control the creation of resources/outputs (not parameters). If dev, if test etc. And, Equals, If, Not, Or Example \u00b6 Conditions: CreateProdResources: !Equals [ !Ref EnvType, prod ] Resources: MountPoint: Type: \"AWS::EC2::VolumeAttachment\" Condition: CreateProdResources Instrinsic Functions \u00b6 Function Sample Notes Ref !Ref EC2Instance If referencing a parameter, returns the param value. If referencing a resources, returns the resource id) GetAtt !GetAtt EC2Instance.AvailabilityZone Get the attributes of a resource. FindInMap !FindInMap [RegionMap, !Ref \"AWS::Region\", 32] Get a name value using a key. ImportValue !ImportValue SSHSecurityGroup Import values that are exported in other templates. Join !Join [ \";\", [ a, b, c ] ] Join values using a delimiter. Sub !Sub String Substitute values in strings. Conditions (If, Not, Equals, And, Or) !And, !If, !Not, !Or, !Equals Logical operations. Rollbacks \u00b6 Stack Creation Failures \u00b6 By default, the stack will roll-back and everything is deleted. Refer to the log for error messages. Can disable the automatic rollback to troubleshoot the issue. Stack Update Failures \u00b6 Stack will rollback to the previous known working state. Refer to log for error messages. ChangeSets \u00b6 Shows what will happen if a stack is updated. Nested Stacks \u00b6 Stacks that are part of other stacks. Update/isolate repeated patterns (re-usable security groups etc). Considered best practise. Different to Cross Stacks, which are for when stacks have different lifecycles. Nested stack is only important to the parent stack. It isn't shared. StackSets \u00b6 Create/update/delete stacks across multiple accounts/regions in a single operation. Requires administration account to create them. Trusted accounts can create, update or delete stack instances from StackSets. Drift \u00b6 Detect manual changes made to resources provisioned by the Cloudformation stack.","title":"CloudFormation"},{"location":"developing/cloudformation/#cloudformation","text":"","title":"CloudFormation"},{"location":"developing/cloudformation/#overview","text":"Declarative way to define infrastructure. Templates are uploaded to S3. Stacks are identified by a name. Resources created by the stack are given a tag to identify the stack that created them. When a stack is deleted, all artifacts created as part of the stack will be deleted. Use CLI to deploy templates.","title":"Overview"},{"location":"developing/cloudformation/#benefits","text":"","title":"Benefits"},{"location":"developing/cloudformation/#infrastructure-as-code","text":"No manual creation, good for control. All code is version controlled. Infra changes can be peer reviewed.","title":"Infrastructure as Code"},{"location":"developing/cloudformation/#cost","text":"Each resource in the stack is tagged so it's easy to see how much a stack costs. Costs can be estimated using the CloudFormation template. Can safely automate the deletion of templates outside of business hours to save money.","title":"Cost"},{"location":"developing/cloudformation/#productivity","text":"Destroy and re-create infra as needed. Automated generation of infrastructure diagrams. Declarative programming, so no need to figure out the ordering and orchestration).","title":"Productivity"},{"location":"developing/cloudformation/#separation-of-concern","text":"Create many stacks for many apps/layers (vpc, network, app etc).","title":"Separation of concern"},{"location":"developing/cloudformation/#re-usable","text":"Leverage existing templates, don't re-invent the wheel.","title":"Re-usable"},{"location":"developing/cloudformation/#building-blocks","text":"","title":"Building Blocks"},{"location":"developing/cloudformation/#template-components","text":"Resources: mandatory section Parameters: Dynamic template inputs Mappings: Static inputs Outputs: References what was created. Conditions: Conditions to control resource creation Metadata:","title":"Template Components"},{"location":"developing/cloudformation/#template-helpers","text":"References: Link to sections in the template Functions: Transform template data.","title":"Template Helpers"},{"location":"developing/cloudformation/#resources","text":"AWS components that will be created/configured. Resources are declared and can reference each other. AWS figures out the order that resources should be created/updated/deleted. Over 224 different types of resources (AWS::product-name::data-type).","title":"Resources"},{"location":"developing/cloudformation/#parameters","text":"Provide inputs to CF templates. Parameters have a type, description, constraints, constraint description, min/max length, min/max value, defaults, allow values, allowed patterns and no echo toggle. Reference a parameter using !Ref (example: !Ref MyVPC ). Pseudo parameters can be used at any time and enabled by default (AWS::AccountId, AWS::Region, AWS::StackName etc).","title":"Parameters"},{"location":"developing/cloudformation/#mappings","text":"Static values defined in the CF template. Use !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] to reference mapping values (example: !FindInMap [RegionMap, !Ref \"AWS::Region\", 32] ).","title":"Mappings"},{"location":"developing/cloudformation/#example","text":"RegionMap: us-east-1: \"32\": \"ami-6411e20d\" \"64\": \"ami-7a11e213\" us-west-1: \"32\": \"ami-c9c7978c\" \"64\": \"ami-31c2f645\"","title":"Example"},{"location":"developing/cloudformation/#outputs","text":"Optional If they're exported, they can be consumed in other stacks. Good for separating stacks (refer to VPC id created by the the network stack, in the application stack). Can't delete a stack if its outputs are referenced somewhere else.","title":"Outputs"},{"location":"developing/cloudformation/#example_1","text":"# Stack 1 Outputs: StackSSHSecurityGroup: Description: The SSH Security Group for our Company. Value: !Ref MyCompanyWideSSHSecurityGroup Export: Name: SSHSecurityGroup # Stack 2 Resources: MySecureInstance: Type: AWS::EC2::Instance Properties: SecurityGroups: - !ImportValue SSHSecurityGroup","title":"Example"},{"location":"developing/cloudformation/#conditions","text":"Control the creation of resources/outputs (not parameters). If dev, if test etc. And, Equals, If, Not, Or","title":"Conditions"},{"location":"developing/cloudformation/#example_2","text":"Conditions: CreateProdResources: !Equals [ !Ref EnvType, prod ] Resources: MountPoint: Type: \"AWS::EC2::VolumeAttachment\" Condition: CreateProdResources","title":"Example"},{"location":"developing/cloudformation/#instrinsic-functions","text":"Function Sample Notes Ref !Ref EC2Instance If referencing a parameter, returns the param value. If referencing a resources, returns the resource id) GetAtt !GetAtt EC2Instance.AvailabilityZone Get the attributes of a resource. FindInMap !FindInMap [RegionMap, !Ref \"AWS::Region\", 32] Get a name value using a key. ImportValue !ImportValue SSHSecurityGroup Import values that are exported in other templates. Join !Join [ \";\", [ a, b, c ] ] Join values using a delimiter. Sub !Sub String Substitute values in strings. Conditions (If, Not, Equals, And, Or) !And, !If, !Not, !Or, !Equals Logical operations.","title":"Instrinsic Functions"},{"location":"developing/cloudformation/#rollbacks","text":"","title":"Rollbacks"},{"location":"developing/cloudformation/#stack-creation-failures","text":"By default, the stack will roll-back and everything is deleted. Refer to the log for error messages. Can disable the automatic rollback to troubleshoot the issue.","title":"Stack Creation Failures"},{"location":"developing/cloudformation/#stack-update-failures","text":"Stack will rollback to the previous known working state. Refer to log for error messages.","title":"Stack Update Failures"},{"location":"developing/cloudformation/#changesets","text":"Shows what will happen if a stack is updated.","title":"ChangeSets"},{"location":"developing/cloudformation/#nested-stacks","text":"Stacks that are part of other stacks. Update/isolate repeated patterns (re-usable security groups etc). Considered best practise. Different to Cross Stacks, which are for when stacks have different lifecycles. Nested stack is only important to the parent stack. It isn't shared.","title":"Nested Stacks"},{"location":"developing/cloudformation/#stacksets","text":"Create/update/delete stacks across multiple accounts/regions in a single operation. Requires administration account to create them. Trusted accounts can create, update or delete stack instances from StackSets.","title":"StackSets"},{"location":"developing/cloudformation/#drift","text":"Detect manual changes made to resources provisioned by the Cloudformation stack.","title":"Drift"},{"location":"developing/cloudfront/","text":"CloudFront \u00b6 Overview \u00b6 Content Delivery Network (CDN). Improves read performance, content cached at the edge. 216 edge locations globally. DDos protection Integration with Shield, Web Application Firewall. Can expose external HTTPS and talk to internal HTTPS backends. Origins \u00b6 S3 Bucket \u00b6 Distribute files globally & cache them at the edge. Better security via CloudFront Origin Access Identity (OAI). Used as an ingress to upload S3 files. Uses OAI + S3 Bucket policy. The OAI User ID is what gets inserted into the bucket policy. Can restrict bucket access to force users to go through CloudFront. Custom (HTTP \u00b6 ALB (must be public, and SG must allow public IP of edge locations) EC2 instance (must be public, and SG must allow public IP of edge locations) S3 Website Any HTTP backend High-level Flow \u00b6 Client => HTTP GET /foo.jpg => Edge Location => S3 Bucket/HTTP (if not cached) => Edge Location (add/update cache) => Client Geo Restriction \u00b6 Restrict who can access the distribution based on country. Uses third party Geo-IP database. CloudFront vs S3 CRR \u00b6 CloudFront uses a global edge network and files are cached for a TTL (default is 24hrs). Useful for static content. S3 CRR needs to be setup in each region replication needs to happen. Files are uploaded in near realtime, it's read-only and better suited for dynamic content that needs low latency. Caching \u00b6 Cache based on headers, session cookies, query string parameters. Cache sits on each Edge Location. Cache TTL can be from 0 seconds to 1 year. Controlled via Cache-Control or Expires header. CreateInvalidation API can be used to invalidate part of the cache. Maximise cache hits by separating static and dynamic content distributions (static to S3, dynamic to ALB + EC2). Security \u00b6 Viewer Protocol Policy (client to edge location) \u00b6 Redirect HTTP to HTTPS, or Use HTTPS only S3 Websites don't support HTTPS Origin Protocol Policy (edge location to backend) \u00b6 HTTPS only or, match viewer (http => http, https => https) Signed URL/Cookies \u00b6 Allows visibility and control over which users are allowed to access content. Signed URL/Cookie has a policy that: Include URL expiration (minutes to years) Inclue IP ranges to access the data from Trusted signers (which AWS accounts can create signed URLs) Signed URL gives access to a specific file. Signed Cookie gives access to many files. Private key is used by applications to sign the URLs. Public key is used by CloudFront to verify the URLs. Trusted Key Group \u00b6 The recommended way to sign. Assigned to a CloudFront distribution. Contains one or more public keys so CloudFront can verify the URLs. Can use APIs to manage/rotate the keys. CloudFront Key Pair \u00b6 An AWS account that has the CloudFront Key Pair can also be used for signing. Avoid because the root account is needed to manage the keys, and there's no API available. CloudFront Signed URL vs S3 Pre-signed URL \u00b6 CloudFront Signed URL \u00b6 Signed URL allows access to a path, regardless of origin. Only the root can manage the account wide key-pair used for signing. Can filter by IP, path, date or expiration. Can leverage caching features. S3 Pre-Signed URL \u00b6 Issue a request as the person who pre-signed the URL. Uses the IAM key of the signing IAM principal. Limited lifetime. Price Classes \u00b6 Class Description Price Class All Best performance, most expensive, uses all edge locations. Price Class 200 Excludes the most expensive regions. Price Class 100 Only uses the cheapest regions. Multiple Origin Routes \u00b6 Route to different origins based on the content type. For example, API calls go to the ALB, static content goes to an S3 bucket. Origin Groups \u00b6 Increases HA and ability to do failover. Has one primary, and one secondary origin. Can use S3 buckets to enable region-level H/A (retrieve from secondary bucket in another region if the first bucket returns an error). Field Level Encryption \u00b6 Protects sensitive user information. Information is encrypted at the edge close to the user. Up to 10 fields in the request can be encrypted.","title":"CloudFront"},{"location":"developing/cloudfront/#cloudfront","text":"","title":"CloudFront"},{"location":"developing/cloudfront/#overview","text":"Content Delivery Network (CDN). Improves read performance, content cached at the edge. 216 edge locations globally. DDos protection Integration with Shield, Web Application Firewall. Can expose external HTTPS and talk to internal HTTPS backends.","title":"Overview"},{"location":"developing/cloudfront/#origins","text":"","title":"Origins"},{"location":"developing/cloudfront/#s3-bucket","text":"Distribute files globally & cache them at the edge. Better security via CloudFront Origin Access Identity (OAI). Used as an ingress to upload S3 files. Uses OAI + S3 Bucket policy. The OAI User ID is what gets inserted into the bucket policy. Can restrict bucket access to force users to go through CloudFront.","title":"S3 Bucket"},{"location":"developing/cloudfront/#custom-http","text":"ALB (must be public, and SG must allow public IP of edge locations) EC2 instance (must be public, and SG must allow public IP of edge locations) S3 Website Any HTTP backend","title":"Custom (HTTP"},{"location":"developing/cloudfront/#high-level-flow","text":"Client => HTTP GET /foo.jpg => Edge Location => S3 Bucket/HTTP (if not cached) => Edge Location (add/update cache) => Client","title":"High-level Flow"},{"location":"developing/cloudfront/#geo-restriction","text":"Restrict who can access the distribution based on country. Uses third party Geo-IP database.","title":"Geo Restriction"},{"location":"developing/cloudfront/#cloudfront-vs-s3-crr","text":"CloudFront uses a global edge network and files are cached for a TTL (default is 24hrs). Useful for static content. S3 CRR needs to be setup in each region replication needs to happen. Files are uploaded in near realtime, it's read-only and better suited for dynamic content that needs low latency.","title":"CloudFront vs S3 CRR"},{"location":"developing/cloudfront/#caching","text":"Cache based on headers, session cookies, query string parameters. Cache sits on each Edge Location. Cache TTL can be from 0 seconds to 1 year. Controlled via Cache-Control or Expires header. CreateInvalidation API can be used to invalidate part of the cache. Maximise cache hits by separating static and dynamic content distributions (static to S3, dynamic to ALB + EC2).","title":"Caching"},{"location":"developing/cloudfront/#security","text":"","title":"Security"},{"location":"developing/cloudfront/#viewer-protocol-policy-client-to-edge-location","text":"Redirect HTTP to HTTPS, or Use HTTPS only S3 Websites don't support HTTPS","title":"Viewer Protocol Policy (client to edge location)"},{"location":"developing/cloudfront/#origin-protocol-policy-edge-location-to-backend","text":"HTTPS only or, match viewer (http => http, https => https)","title":"Origin Protocol Policy (edge location to backend)"},{"location":"developing/cloudfront/#signed-urlcookies","text":"Allows visibility and control over which users are allowed to access content. Signed URL/Cookie has a policy that: Include URL expiration (minutes to years) Inclue IP ranges to access the data from Trusted signers (which AWS accounts can create signed URLs) Signed URL gives access to a specific file. Signed Cookie gives access to many files. Private key is used by applications to sign the URLs. Public key is used by CloudFront to verify the URLs.","title":"Signed URL/Cookies"},{"location":"developing/cloudfront/#trusted-key-group","text":"The recommended way to sign. Assigned to a CloudFront distribution. Contains one or more public keys so CloudFront can verify the URLs. Can use APIs to manage/rotate the keys.","title":"Trusted Key Group"},{"location":"developing/cloudfront/#cloudfront-key-pair","text":"An AWS account that has the CloudFront Key Pair can also be used for signing. Avoid because the root account is needed to manage the keys, and there's no API available.","title":"CloudFront Key Pair"},{"location":"developing/cloudfront/#cloudfront-signed-url-vs-s3-pre-signed-url","text":"","title":"CloudFront Signed URL vs S3 Pre-signed URL"},{"location":"developing/cloudfront/#cloudfront-signed-url","text":"Signed URL allows access to a path, regardless of origin. Only the root can manage the account wide key-pair used for signing. Can filter by IP, path, date or expiration. Can leverage caching features.","title":"CloudFront Signed URL"},{"location":"developing/cloudfront/#s3-pre-signed-url","text":"Issue a request as the person who pre-signed the URL. Uses the IAM key of the signing IAM principal. Limited lifetime.","title":"S3 Pre-Signed URL"},{"location":"developing/cloudfront/#price-classes","text":"Class Description Price Class All Best performance, most expensive, uses all edge locations. Price Class 200 Excludes the most expensive regions. Price Class 100 Only uses the cheapest regions.","title":"Price Classes"},{"location":"developing/cloudfront/#multiple-origin-routes","text":"Route to different origins based on the content type. For example, API calls go to the ALB, static content goes to an S3 bucket.","title":"Multiple Origin Routes"},{"location":"developing/cloudfront/#origin-groups","text":"Increases HA and ability to do failover. Has one primary, and one secondary origin. Can use S3 buckets to enable region-level H/A (retrieve from secondary bucket in another region if the first bucket returns an error).","title":"Origin Groups"},{"location":"developing/cloudfront/#field-level-encryption","text":"Protects sensitive user information. Information is encrypted at the edge close to the user. Up to 10 fields in the request can be encrypted.","title":"Field Level Encryption"},{"location":"developing/cloudtrail/","text":"CloudTrail \u00b6 Overview \u00b6 Governance, compliant and audit for the AWS account. Enabled by default. History of events/API calls. Can put logs from CloudTrail into CloudWatch. If a resource is deleted, look at CloudTrail first. Need to chose what log events to record. Log Events \u00b6 Event Description Management Event Management operations performed on AWS resources. Can exclude KMS events. Data Event Resource operations performed on/within AWS resources (S3 or Lambda). Insights Event Unusual activity, errors, or user behaviour.","title":"CloudTrail"},{"location":"developing/cloudtrail/#cloudtrail","text":"","title":"CloudTrail"},{"location":"developing/cloudtrail/#overview","text":"Governance, compliant and audit for the AWS account. Enabled by default. History of events/API calls. Can put logs from CloudTrail into CloudWatch. If a resource is deleted, look at CloudTrail first. Need to chose what log events to record.","title":"Overview"},{"location":"developing/cloudtrail/#log-events","text":"Event Description Management Event Management operations performed on AWS resources. Can exclude KMS events. Data Event Resource operations performed on/within AWS resources (S3 or Lambda). Insights Event Unusual activity, errors, or user behaviour.","title":"Log Events"},{"location":"developing/cloudwatch/","text":"CloudWatch \u00b6 Overview \u00b6 Provides metrics for every AWS service. Metrics are grouped by namespace. Dimensions are attributes of a metric (instance id, environment etc). Max 10 dimensions per metric. Metrics have timestamps. EC2 metrics by default cover disk, CPU and network (at a high level). No RAM. EC2 Detailed Monitoring \u00b6 EC2 instance metrics have 5min granularity by default. Detailed monitoring sets granularity to 1min. 10 detailed monitoring metrics on the free tier. EC2 memory usage is not pushed by default. Needs to be a custom metric. Group metric collection is not enabled by default for EC2 instances. Custom Metrics \u00b6 1min granularity. Enable high-resolution custom metrics ( StorageResolution API parameter) to set granularity up to 1second. High resolution metrics only allow 10sec or 30sec for the alarm period. Use PutMetricData to send a custom metric to CloudWatch. Use exponential backoff in case of throttle errors. CloudWatch Alarms \u00b6 Alarms can trigger notifications for any metric. Alarms action can trigger Auto Scaling action, EC2 action, or SNS notification. Alarms can be based on sample count, percentage, max, min etc). Alarms states are OK, INSUFFICIENT_DATA, ALARM. Alarm periods are the time window used to evaluate the metric. Missing data can be treated as good (threshold not breached), bad (threshold breached), ignore (don't change alarm state) or missing. CloudWatch Logs \u00b6 Applications can send logs to CloudWatch via the SDK. Logs can be collected from Elastic Beanstalk, ECS, AWS Lambda, VPC Flows, API Gateway, CloudTrail based on a filter, CloudWatch log agents, Route53 etc. Logs can be sent to CloudWatch, S3 for archiving, streaming to ElasticSearch for analytics. Supports filter expressions for searching. Log group is a name, usually represents the application. Log stream is instances within the application/log files/containers. Log expiration policy used for data retention (never expire, 30 days etc). Logs can be tailed using the AWS CLI. Make sure IAM permissions are correct to send logs. Logs can be encrypted using KMS at the Group level (use associate-cmk-key and create-log-group in the API). Can use a Lambda subscription filter, or an ElasticSearch subscription filter to send logs to Lambda/ElasticSearch for further analysis. CloudWatch Agent \u00b6 Need to run the CloudWatch Agent to send logs from EC2 instances to CloudWatch. Requires an IAM role to allow logs to be sent to CloudWatch. CloudWatch Agent can run on-prem as well. CloudWatch Logs Agent \u00b6 Old version of the agent that can only send to CloudWatch logs. CloudWatch Unified Agent \u00b6 New version of the CloudWatch Agent. Can collect metrics like CPU, disk metrics, RAM, Netstat, processes, swap space. Can collect logs to send to CloudWatch logs. Can use centralized configuration using SSM parameter store to manage the agent. CloudWatch Logs Metric Filter \u00b6 Filter expressiones to search logs. Can trigger alarms. Filters don't retroactively filter data. Only public metric data points for events that occure after the filter was created. CloudWatch Events \u00b6 Can be scheduled via a cronjob. Can use an event pattern to react to a service doing something (eg: CodePipeline state changed). Triggers Lambda functions, SQS/SNS/Kinesis messages, CodeBuild, CodePipeline, Firehose, ECS task, EC2 TerminateInstance API call etc. Creates a JSON document with some information about the change. Amazon EventBridge \u00b6 Next evoluation of CloudWatch Events. Default event bus is generated by AWS services (CloudWatch Events). Partner event bus to recieve events from SaaS service or application (Zendesk, DataDog, Segment, Auth0 etc). Custom event buses for your own applications. Event buses can be accessed by other AWS accounts. Rules define how to process events (similar to CloudWatch Events). Amazon EventBridge Schema Registry \u00b6 EventBridge can analyze events in the bus and infer the schema. Schema Registry helps to generate code for the application, that knows in advance how data is structured in the event bus. Schemas are versioned.","title":"CloudWatch"},{"location":"developing/cloudwatch/#cloudwatch","text":"","title":"CloudWatch"},{"location":"developing/cloudwatch/#overview","text":"Provides metrics for every AWS service. Metrics are grouped by namespace. Dimensions are attributes of a metric (instance id, environment etc). Max 10 dimensions per metric. Metrics have timestamps. EC2 metrics by default cover disk, CPU and network (at a high level). No RAM.","title":"Overview"},{"location":"developing/cloudwatch/#ec2-detailed-monitoring","text":"EC2 instance metrics have 5min granularity by default. Detailed monitoring sets granularity to 1min. 10 detailed monitoring metrics on the free tier. EC2 memory usage is not pushed by default. Needs to be a custom metric. Group metric collection is not enabled by default for EC2 instances.","title":"EC2 Detailed Monitoring"},{"location":"developing/cloudwatch/#custom-metrics","text":"1min granularity. Enable high-resolution custom metrics ( StorageResolution API parameter) to set granularity up to 1second. High resolution metrics only allow 10sec or 30sec for the alarm period. Use PutMetricData to send a custom metric to CloudWatch. Use exponential backoff in case of throttle errors.","title":"Custom Metrics"},{"location":"developing/cloudwatch/#cloudwatch-alarms","text":"Alarms can trigger notifications for any metric. Alarms action can trigger Auto Scaling action, EC2 action, or SNS notification. Alarms can be based on sample count, percentage, max, min etc). Alarms states are OK, INSUFFICIENT_DATA, ALARM. Alarm periods are the time window used to evaluate the metric. Missing data can be treated as good (threshold not breached), bad (threshold breached), ignore (don't change alarm state) or missing.","title":"CloudWatch Alarms"},{"location":"developing/cloudwatch/#cloudwatch-logs","text":"Applications can send logs to CloudWatch via the SDK. Logs can be collected from Elastic Beanstalk, ECS, AWS Lambda, VPC Flows, API Gateway, CloudTrail based on a filter, CloudWatch log agents, Route53 etc. Logs can be sent to CloudWatch, S3 for archiving, streaming to ElasticSearch for analytics. Supports filter expressions for searching. Log group is a name, usually represents the application. Log stream is instances within the application/log files/containers. Log expiration policy used for data retention (never expire, 30 days etc). Logs can be tailed using the AWS CLI. Make sure IAM permissions are correct to send logs. Logs can be encrypted using KMS at the Group level (use associate-cmk-key and create-log-group in the API). Can use a Lambda subscription filter, or an ElasticSearch subscription filter to send logs to Lambda/ElasticSearch for further analysis.","title":"CloudWatch Logs"},{"location":"developing/cloudwatch/#cloudwatch-agent","text":"Need to run the CloudWatch Agent to send logs from EC2 instances to CloudWatch. Requires an IAM role to allow logs to be sent to CloudWatch. CloudWatch Agent can run on-prem as well.","title":"CloudWatch Agent"},{"location":"developing/cloudwatch/#cloudwatch-logs-agent","text":"Old version of the agent that can only send to CloudWatch logs.","title":"CloudWatch Logs Agent"},{"location":"developing/cloudwatch/#cloudwatch-unified-agent","text":"New version of the CloudWatch Agent. Can collect metrics like CPU, disk metrics, RAM, Netstat, processes, swap space. Can collect logs to send to CloudWatch logs. Can use centralized configuration using SSM parameter store to manage the agent.","title":"CloudWatch Unified Agent"},{"location":"developing/cloudwatch/#cloudwatch-logs-metric-filter","text":"Filter expressiones to search logs. Can trigger alarms. Filters don't retroactively filter data. Only public metric data points for events that occure after the filter was created.","title":"CloudWatch Logs Metric Filter"},{"location":"developing/cloudwatch/#cloudwatch-events","text":"Can be scheduled via a cronjob. Can use an event pattern to react to a service doing something (eg: CodePipeline state changed). Triggers Lambda functions, SQS/SNS/Kinesis messages, CodeBuild, CodePipeline, Firehose, ECS task, EC2 TerminateInstance API call etc. Creates a JSON document with some information about the change.","title":"CloudWatch Events"},{"location":"developing/cloudwatch/#amazon-eventbridge","text":"Next evoluation of CloudWatch Events. Default event bus is generated by AWS services (CloudWatch Events). Partner event bus to recieve events from SaaS service or application (Zendesk, DataDog, Segment, Auth0 etc). Custom event buses for your own applications. Event buses can be accessed by other AWS accounts. Rules define how to process events (similar to CloudWatch Events).","title":"Amazon EventBridge"},{"location":"developing/cloudwatch/#amazon-eventbridge-schema-registry","text":"EventBridge can analyze events in the bus and infer the schema. Schema Registry helps to generate code for the application, that knows in advance how data is structured in the event bus. Schemas are versioned.","title":"Amazon EventBridge Schema Registry"},{"location":"developing/ds/","text":"Directory Services \u00b6 Overview \u00b6 Microsoft Active Directory (AD) \u00b6 Database of objects (user accounts, computers, printers, file shares, security groups). Provides centralized security management. Objects organised in trees. Group of trees is a forest. AWS Directory Services \u00b6 AWS Managed Microsoft AD \u00b6 Create AD in AWS. Manage users locally. Supports MFA. Create trust relationships with on-prem AD. Standard version allows up to 30,000 objects, 1GB storage. Enterprise version allows up to 500,000 objects, 17GB storage. AD Connector \u00b6 Directory Gateway (proxy) to redirect to the on-prem AD. Users managed locally. Two connectors, one for up to 500 users, another for up to 5,000 users. Simple AD \u00b6 AD-compatible managed directory on AWS. No MFA. Can't join on-prem AD. Amazon Cognito User Pools \u00b6 Redirect requests to cognito.","title":"Directory Services"},{"location":"developing/ds/#directory-services","text":"","title":"Directory Services"},{"location":"developing/ds/#overview","text":"","title":"Overview"},{"location":"developing/ds/#microsoft-active-directory-ad","text":"Database of objects (user accounts, computers, printers, file shares, security groups). Provides centralized security management. Objects organised in trees. Group of trees is a forest.","title":"Microsoft Active Directory (AD)"},{"location":"developing/ds/#aws-directory-services","text":"","title":"AWS Directory Services"},{"location":"developing/ds/#aws-managed-microsoft-ad","text":"Create AD in AWS. Manage users locally. Supports MFA. Create trust relationships with on-prem AD. Standard version allows up to 30,000 objects, 1GB storage. Enterprise version allows up to 500,000 objects, 17GB storage.","title":"AWS Managed Microsoft AD"},{"location":"developing/ds/#ad-connector","text":"Directory Gateway (proxy) to redirect to the on-prem AD. Users managed locally. Two connectors, one for up to 500 users, another for up to 5,000 users.","title":"AD Connector"},{"location":"developing/ds/#simple-ad","text":"AD-compatible managed directory on AWS. No MFA. Can't join on-prem AD.","title":"Simple AD"},{"location":"developing/ds/#amazon-cognito-user-pools","text":"Redirect requests to cognito.","title":"Amazon Cognito User Pools"},{"location":"developing/dynamodb/","text":"DynamoDB \u00b6 Overview \u00b6 NoSQL, serverless database. Fully managed with auto-scaling. Doesn't support join. All data needed for a query should be in one row. Don't perform aggregations like \"SUM\". Multi-AZ replication. Very high capacity. Auto-scales, low cost. Tables \u00b6 Each table has a primary key. Supports an infinite number of items. Table attributes can be nested and added over time. Maximum size of an item is 400KB. Uses provisioned read/write capacity units. Data Types \u00b6 String, Number, Binary, Boolean, Null. List, Map, Set. Primary Keys \u00b6 Partition Key \u00b6 Unique for each item. Must be unique so that it can be distributed. Partition Key + Sort Key \u00b6 Combination must be unique. Data is grouped by partition key, then sorted by sort key. Throughput \u00b6 RCU/WCU can be auto-scaled. \"Burst Credit\" allows throughput to be temporarily exceeded. A ProvisionedThroughputException will be thrown if burst credit is exceeded. Fix via exponential back-off. A ProvisionedThroughputExceededException will be thrown if RCU/WCU is exceeded. Could be caused by hot keys, hot partitions of very large items. Fix via exponential back-off, distributing partition keys better, or use DynamoDB Accelerator. RCU/WCU is spread evenly between partitions. Round up to nearest KB when calculating RCU/WCU. Eventually consistent reads may return old data due to replication delays. Can use ConsistentRead on GetItem , Query and Scan . Strongly consistent reads will return the latest data. Calculating RCU/WCU: Operation Reads/Second Item Size Eventually Consistent Read 2 4KB Strongly Consistent Read 1 4KB Write 1 1KB Partitions \u00b6 Data is divided into partitions. Partition keys are hashed to determine which partition they go to. Calculating the number of partions: How Method By Capacity (total RCU/3000) + (total WCU/1000) By Size Total Size/10GB Total number of partitions is the max of capacity and size. Reading Data \u00b6 GetItem to read based on a primary key (hash or hash range). ProjectionExpression can be used to limit the item attributes that are returned. BatchGetItem to retrieve upto 100 items (16MB data). Writing Data \u00b6 PutItem to create or replace an item. UpdateItem to update an item. Can use an atomic counter to consistently increment/decrement a value. BatchWriteItem to write up to 25 PutItem or DeleteItem in one call. Reduces latency, and cost. Maximum batch size is 16MB. Part of the batch can fail, and the failed items would have to be retried. Conditional Writes \u00b6 Accept a write/update only if conditions are met. Good for concurrent acess. Deleting Data \u00b6 DeleteItem to delete a row. Can do conditional deletes. DeleteTable to delete an entire table. Querying Data \u00b6 Query returns items based on the partion key and (optional) sort key. Use FilterExpression to perform client-side filtering. Up to 1MB of data. Supports limit/pagination. Can query a table, local secondary index, or a global secondary index. Scan scans the whole table, then filters data. Inefficient. Max 1MB of data. Supports limit/pagination. Parallel scan will scan faster. Supports ProjectionExpression and FilterExpression . Indexes \u00b6 Local Secondary Index \u00b6 Range key for the table. Bound to the partition key. Must be defined when the table is created. Up to five LSI per table. Sort key must be string, number or binary attribute. Global Secondary Index (GSI) \u00b6 Can be added/modified on the fly. Speeds up queries on non-key attributes. Uses partition key + sort key. Generates a new table that item attributes are projected on to using the INCLUDE , ALL or KEYS_ONLY parameters. Have to define a WCU/RCU for the GSI. If GSI writes are throttled, it will throttle the main table. Concurrency \u00b6 DynamoDB uses optimistic locking. The first concurrent request to modify an object \"wins\". The remaining concurrent requests will fail. DynamoDB Accelerator (DAX) \u00b6 Caching techology. Effectively transparent to the client. Writes go through DAX, which allows for sub-millisecond latency for cached reads and queries. Solves the hot key problem to reducing the amount of times the key is read. Default TTL is 5mins. Up to 10 nodes in a cluster. Multi AZ. Supports encryption at rest. For aggregation results, use ElastiCache. DynamoDB Streams \u00b6 Changes to DynamoDB can be sent to a DynamoDB stream, to be read by Lambda, EC2 instances etc. Good for reacting to changes in realtime, analytics, creating derived tables/views, inserting into elasticsearch. Supports cross-region replication. 24hr data retention (can't be changed). Chose what data to send to the stream: KEYS_ONLY to send just the primary key. NEW_IMAGE to send the modified item. OLD_IMAGE to send the unmodified item. NEW_AND_OLD_IMAGES to send the modified, and unmodified items. Streams consist of shards, that are fully managed by AWS. Only new records will be added to a stream. DynamoDB Streams + Lambda \u00b6 Have to define an Event Source Mapping for Lambda to read from a stream. Create a trigger from DynamoDB Streams that executes a Lambda function. Lambda function is invoked asynchronously. Time to Live (TTL) \u00b6 Automatically delete an item after an expiry date/time. No cost, doesn't use WCU/RCU. Reduces storage capacity, helps adhere to regulatory requirements re data retention. Enabled per row (select which attribute represents the epoch timestamp). Expired items usually deleted within 48hrs, and are also delete from any indexes (GSI/LSI). Could use DynamoDB Streams to help recover expired items. DynamoDB CLI \u00b6 --project-expression to specify a list of attributes to retrieve from the table. --filter-expression to filter the results. The filter expression filter is defined using --expression-attribute-values . --page-size to limit the number of items per page. Happens in the background, the full set of results are returned, it just consumes more RCU by running the query multiple times. --max-items to enable pagination and set the maximum number of results to be returned. Returns NextToken . --starting-token to read the next page. Transactions \u00b6 Execute CRUD operations across multiple tables at the same time. All or nothing operation. If any operation fails, the entire transaction fails. Uses twice the RCU/WCU. eg: 3 writes/sec @ 5KB = (5KB/1KB) * 2 * 3 = 30WCU. 3 reads/sec @ 5KB = (5KB/4KB) * 2 * 3 = 12RCU. TransactWriteItems to write. TransactGetItems to read. DynamoDB as a Session State Cache \u00b6 Common use case. Use ElastiCache when it's ok to store the cache in memory. Use DynamoDB when serveless/auto scaling is needed. Use EFS to store session cache when session state needs to be shared across multiple EC2 instances. Use EBS/Instance store for local caching (not shared). Write Sharding \u00b6 Add a random/calculated suffix to the partition keys to scale writes across multiple shards. Write Types \u00b6 Type Description Concurrent The last concurrent write wins. Conditional Only update the item based on a condition (eg: only update if the items value = \\ ). Atomic Uses an atomic counter to allow concurrent writes to increment/decrement an items attribute. Batch Update many items at the same time. DynamoDB Patterns for S3 \u00b6 Large Objects Pattern \u00b6 Send the large data to S3. Insert metadata into DynamoDB that points to the location of the S3 object. Indexing S3 Object Metadata \u00b6 Write to S3. Trigger a lambda function, and write metadata about the S3 object into DynamoDB. Now you can query DynamoDB for object metadata. Operations \u00b6 Fastest way to clean up a table is to use DeleteTable , then re-create it. Copy a table using AWS DataPipeline (via EMR), which will export a table to S3, then load the S3 object into a new DynamoDB table. Can also copy by running a backup, then restoring it into a new table, or scanning the entire table and writing the items into the new one. Security \u00b6 Use VPC endpoints to access DynamoDB without going over the internet. Use IAM for access control. Encryption at rest using KMS. Encryption in transit using SSL/TLS. There's a point-in-time Backup & Restore feature. Global tables are multi-region, high performance and fully replicated. use Amazon Database Migration Service (DMS) to migrate databases to DynamoDB. DynamoDB can be run locally. Use Web Identity Federation (WAF) to get temporary credentials that can be used by web/mobile apps to directly talk to DynamoDB. Use a condition on the IAM policy to limit what tables the user can access. LeadingKeys set row-level access limits. Attributes to limit what attributes the user is allowed to see.","title":"DynamoDB"},{"location":"developing/dynamodb/#dynamodb","text":"","title":"DynamoDB"},{"location":"developing/dynamodb/#overview","text":"NoSQL, serverless database. Fully managed with auto-scaling. Doesn't support join. All data needed for a query should be in one row. Don't perform aggregations like \"SUM\". Multi-AZ replication. Very high capacity. Auto-scales, low cost.","title":"Overview"},{"location":"developing/dynamodb/#tables","text":"Each table has a primary key. Supports an infinite number of items. Table attributes can be nested and added over time. Maximum size of an item is 400KB. Uses provisioned read/write capacity units.","title":"Tables"},{"location":"developing/dynamodb/#data-types","text":"String, Number, Binary, Boolean, Null. List, Map, Set.","title":"Data Types"},{"location":"developing/dynamodb/#primary-keys","text":"","title":"Primary Keys"},{"location":"developing/dynamodb/#partition-key","text":"Unique for each item. Must be unique so that it can be distributed.","title":"Partition Key"},{"location":"developing/dynamodb/#partition-key-sort-key","text":"Combination must be unique. Data is grouped by partition key, then sorted by sort key.","title":"Partition Key + Sort Key"},{"location":"developing/dynamodb/#throughput","text":"RCU/WCU can be auto-scaled. \"Burst Credit\" allows throughput to be temporarily exceeded. A ProvisionedThroughputException will be thrown if burst credit is exceeded. Fix via exponential back-off. A ProvisionedThroughputExceededException will be thrown if RCU/WCU is exceeded. Could be caused by hot keys, hot partitions of very large items. Fix via exponential back-off, distributing partition keys better, or use DynamoDB Accelerator. RCU/WCU is spread evenly between partitions. Round up to nearest KB when calculating RCU/WCU. Eventually consistent reads may return old data due to replication delays. Can use ConsistentRead on GetItem , Query and Scan . Strongly consistent reads will return the latest data. Calculating RCU/WCU: Operation Reads/Second Item Size Eventually Consistent Read 2 4KB Strongly Consistent Read 1 4KB Write 1 1KB","title":"Throughput"},{"location":"developing/dynamodb/#partitions","text":"Data is divided into partitions. Partition keys are hashed to determine which partition they go to. Calculating the number of partions: How Method By Capacity (total RCU/3000) + (total WCU/1000) By Size Total Size/10GB Total number of partitions is the max of capacity and size.","title":"Partitions"},{"location":"developing/dynamodb/#reading-data","text":"GetItem to read based on a primary key (hash or hash range). ProjectionExpression can be used to limit the item attributes that are returned. BatchGetItem to retrieve upto 100 items (16MB data).","title":"Reading Data"},{"location":"developing/dynamodb/#writing-data","text":"PutItem to create or replace an item. UpdateItem to update an item. Can use an atomic counter to consistently increment/decrement a value. BatchWriteItem to write up to 25 PutItem or DeleteItem in one call. Reduces latency, and cost. Maximum batch size is 16MB. Part of the batch can fail, and the failed items would have to be retried.","title":"Writing Data"},{"location":"developing/dynamodb/#conditional-writes","text":"Accept a write/update only if conditions are met. Good for concurrent acess.","title":"Conditional Writes"},{"location":"developing/dynamodb/#deleting-data","text":"DeleteItem to delete a row. Can do conditional deletes. DeleteTable to delete an entire table.","title":"Deleting Data"},{"location":"developing/dynamodb/#querying-data","text":"Query returns items based on the partion key and (optional) sort key. Use FilterExpression to perform client-side filtering. Up to 1MB of data. Supports limit/pagination. Can query a table, local secondary index, or a global secondary index. Scan scans the whole table, then filters data. Inefficient. Max 1MB of data. Supports limit/pagination. Parallel scan will scan faster. Supports ProjectionExpression and FilterExpression .","title":"Querying Data"},{"location":"developing/dynamodb/#indexes","text":"","title":"Indexes"},{"location":"developing/dynamodb/#local-secondary-index","text":"Range key for the table. Bound to the partition key. Must be defined when the table is created. Up to five LSI per table. Sort key must be string, number or binary attribute.","title":"Local Secondary Index"},{"location":"developing/dynamodb/#global-secondary-index-gsi","text":"Can be added/modified on the fly. Speeds up queries on non-key attributes. Uses partition key + sort key. Generates a new table that item attributes are projected on to using the INCLUDE , ALL or KEYS_ONLY parameters. Have to define a WCU/RCU for the GSI. If GSI writes are throttled, it will throttle the main table.","title":"Global Secondary Index (GSI)"},{"location":"developing/dynamodb/#concurrency","text":"DynamoDB uses optimistic locking. The first concurrent request to modify an object \"wins\". The remaining concurrent requests will fail.","title":"Concurrency"},{"location":"developing/dynamodb/#dynamodb-accelerator-dax","text":"Caching techology. Effectively transparent to the client. Writes go through DAX, which allows for sub-millisecond latency for cached reads and queries. Solves the hot key problem to reducing the amount of times the key is read. Default TTL is 5mins. Up to 10 nodes in a cluster. Multi AZ. Supports encryption at rest. For aggregation results, use ElastiCache.","title":"DynamoDB Accelerator (DAX)"},{"location":"developing/dynamodb/#dynamodb-streams","text":"Changes to DynamoDB can be sent to a DynamoDB stream, to be read by Lambda, EC2 instances etc. Good for reacting to changes in realtime, analytics, creating derived tables/views, inserting into elasticsearch. Supports cross-region replication. 24hr data retention (can't be changed). Chose what data to send to the stream: KEYS_ONLY to send just the primary key. NEW_IMAGE to send the modified item. OLD_IMAGE to send the unmodified item. NEW_AND_OLD_IMAGES to send the modified, and unmodified items. Streams consist of shards, that are fully managed by AWS. Only new records will be added to a stream.","title":"DynamoDB Streams"},{"location":"developing/dynamodb/#dynamodb-streams-lambda","text":"Have to define an Event Source Mapping for Lambda to read from a stream. Create a trigger from DynamoDB Streams that executes a Lambda function. Lambda function is invoked asynchronously.","title":"DynamoDB Streams + Lambda"},{"location":"developing/dynamodb/#time-to-live-ttl","text":"Automatically delete an item after an expiry date/time. No cost, doesn't use WCU/RCU. Reduces storage capacity, helps adhere to regulatory requirements re data retention. Enabled per row (select which attribute represents the epoch timestamp). Expired items usually deleted within 48hrs, and are also delete from any indexes (GSI/LSI). Could use DynamoDB Streams to help recover expired items.","title":"Time to Live (TTL)"},{"location":"developing/dynamodb/#dynamodb-cli","text":"--project-expression to specify a list of attributes to retrieve from the table. --filter-expression to filter the results. The filter expression filter is defined using --expression-attribute-values . --page-size to limit the number of items per page. Happens in the background, the full set of results are returned, it just consumes more RCU by running the query multiple times. --max-items to enable pagination and set the maximum number of results to be returned. Returns NextToken . --starting-token to read the next page.","title":"DynamoDB CLI"},{"location":"developing/dynamodb/#transactions","text":"Execute CRUD operations across multiple tables at the same time. All or nothing operation. If any operation fails, the entire transaction fails. Uses twice the RCU/WCU. eg: 3 writes/sec @ 5KB = (5KB/1KB) * 2 * 3 = 30WCU. 3 reads/sec @ 5KB = (5KB/4KB) * 2 * 3 = 12RCU. TransactWriteItems to write. TransactGetItems to read.","title":"Transactions"},{"location":"developing/dynamodb/#dynamodb-as-a-session-state-cache","text":"Common use case. Use ElastiCache when it's ok to store the cache in memory. Use DynamoDB when serveless/auto scaling is needed. Use EFS to store session cache when session state needs to be shared across multiple EC2 instances. Use EBS/Instance store for local caching (not shared).","title":"DynamoDB as a Session State Cache"},{"location":"developing/dynamodb/#write-sharding","text":"Add a random/calculated suffix to the partition keys to scale writes across multiple shards.","title":"Write Sharding"},{"location":"developing/dynamodb/#write-types","text":"Type Description Concurrent The last concurrent write wins. Conditional Only update the item based on a condition (eg: only update if the items value = \\ ). Atomic Uses an atomic counter to allow concurrent writes to increment/decrement an items attribute. Batch Update many items at the same time.","title":"Write Types"},{"location":"developing/dynamodb/#dynamodb-patterns-for-s3","text":"","title":"DynamoDB Patterns for S3"},{"location":"developing/dynamodb/#large-objects-pattern","text":"Send the large data to S3. Insert metadata into DynamoDB that points to the location of the S3 object.","title":"Large Objects Pattern"},{"location":"developing/dynamodb/#indexing-s3-object-metadata","text":"Write to S3. Trigger a lambda function, and write metadata about the S3 object into DynamoDB. Now you can query DynamoDB for object metadata.","title":"Indexing S3 Object Metadata"},{"location":"developing/dynamodb/#operations","text":"Fastest way to clean up a table is to use DeleteTable , then re-create it. Copy a table using AWS DataPipeline (via EMR), which will export a table to S3, then load the S3 object into a new DynamoDB table. Can also copy by running a backup, then restoring it into a new table, or scanning the entire table and writing the items into the new one.","title":"Operations"},{"location":"developing/dynamodb/#security","text":"Use VPC endpoints to access DynamoDB without going over the internet. Use IAM for access control. Encryption at rest using KMS. Encryption in transit using SSL/TLS. There's a point-in-time Backup & Restore feature. Global tables are multi-region, high performance and fully replicated. use Amazon Database Migration Service (DMS) to migrate databases to DynamoDB. DynamoDB can be run locally. Use Web Identity Federation (WAF) to get temporary credentials that can be used by web/mobile apps to directly talk to DynamoDB. Use a condition on the IAM policy to limit what tables the user can access. LeadingKeys set row-level access limits. Attributes to limit what attributes the user is allowed to see.","title":"Security"},{"location":"developing/ecr/","text":"ECR \u00b6 Overview \u00b6 Private docker registry. Access controlled via IAM policy. Can enable tag immutability to prevent tags being over-written. Can scan images the moment that they're pushed to the registry. ECS cluster instances need an IAM role that allows docker images to be pulled from ECR. CLI authentication \u00b6 AWS CLI v1 \u00b6 The command output is executed, so need to wrap it as a shell command - $(aws ecr get-login --no-include-email --region <region>) AWS CLI v2 \u00b6 Uses pipes to do a docker login - aws ecr get-login-password --region <region> | docker login --username AWS --password-stdin <registry> Docker Push/Pull \u00b6 Same as usual.","title":"ECR"},{"location":"developing/ecr/#ecr","text":"","title":"ECR"},{"location":"developing/ecr/#overview","text":"Private docker registry. Access controlled via IAM policy. Can enable tag immutability to prevent tags being over-written. Can scan images the moment that they're pushed to the registry. ECS cluster instances need an IAM role that allows docker images to be pulled from ECR.","title":"Overview"},{"location":"developing/ecr/#cli-authentication","text":"","title":"CLI authentication"},{"location":"developing/ecr/#aws-cli-v1","text":"The command output is executed, so need to wrap it as a shell command - $(aws ecr get-login --no-include-email --region <region>)","title":"AWS CLI v1"},{"location":"developing/ecr/#aws-cli-v2","text":"Uses pipes to do a docker login - aws ecr get-login-password --region <region> | docker login --username AWS --password-stdin <registry>","title":"AWS CLI v2"},{"location":"developing/ecr/#docker-pushpull","text":"Same as usual.","title":"Docker Push/Pull"},{"location":"developing/ecs/","text":"ECS \u00b6 Overview \u00b6 Three different flavours - Flavour Description ECS Classic Provision EC2 instances to run containers on. Fargate Serverless. EKS Managed Kubernetes by AWS. ECS Classic \u00b6 EC2 instances must be created. Must configure ECS_CLUSTER in /etc/ecs/ecs.config to provide the cluster name. Must configure EC_ENABLE_TASK_IAM_ROLE in /etc/ecs/ecs.config to allow the ECS task to endorse an IAM role. EC2 instances must run an ECS agent. EC2 instance can run multiple containers of the same type, by NOT specifying a host port and using an ALB with dynamic port mapping. EC2 instance needs to allow traffic from the ALB on all ports. ECS tasks must have IAM roles to execute actions against AWS. Security groups operate at the instance level, not the task level. ECS Clusters \u00b6 Logical grouping of EC2 instances. Each EC2 instance runs the ECS agent (docker container). Each ECS agent registers itself to the ECS cluster. EC2 instances use a special AMI made specifically for ECS. 3 cluster creation templates Networking only (use Fargate) EC2 Linux + Networking (uses auto-scaling) EC2 Windows + Networking (uses auto-scaling) ECS Task Definition \u00b6 JSON metadata that tells ECS how to run a docker container. Includes image name, port binding for container/host, memory/cpu requirements, env vars, network info + more. Need to assign an IAM role to allow it to make API requests to AWS services. Are versioned. ECS Service \u00b6 Defines how many/what tasks should be run. Can be linked to ELB/NLB/ALB if needed. Assign the task definition, and it's revision (version). Set minimum healthy percent to 0 to allow for rolling restarts. Can use Code Deploy to deploy to ECS (blue/green deployment), as an alternative to a rolling deployment. Can use placement strategies to control where the tasks are placed. Supports integration with service discovery. Supports auto-scaling. Two service types - daemon good for monitoring services, or replica, for 'normal' clustered containers. Needs a Service IAM role to allow it to communicate with the API. Role can be auto-created at the time of service creation. ECS Service with Load Balancer \u00b6 Use ALB with dynamic port forwarding. Only specify the container port in the task definition to use a random port (bridge networking). Load balancer can only be added during service creation. Have to create the load balancer before configuring the service. The security group used by the clusters EC2 instances, needs an inbound rule that permits all traffic on any port, coming from the ALB security group. This will allow the load balancer to forward traffic via the dynamically mapped ports. ECS Data Volumes \u00b6 Task Strategies \u00b6 Shared EBS Volume \u00b6 Share an EBS volume between EC2 instances. Allows the docker containers to mount the EBS volume and increases storage capacity of the task. If the task moves to another EC2 instance in another availability zone, it won't have the same EBS volume/data. Shared EFS Volume \u00b6 Share an EFS volume between EC2 instances. Same EFS volume can mounted on tasks in any availability zone. Persistent, multi-AZ storage for containers. Fargate+EFS allows for data storage without the need for servers. Bind Mounts \u00b6 Use local EC2 instance storage for EC2 Tasks, or Fargate tasks (4G storage). Useful to store ephemeral data on the same task. Good for side-car pattern. Container A writes application logs to storage, Container B ingests the logs/metrics and forwards them to Splunk/Prometheus etc. ALB access logs will contain information about latency, URLs, status codes and client IPs. ALB \u00b6 Uses dynamic port forwarding to forward to the task instances running in swarm mode. Rule-based routing & paths supported. Define the listener port, protocol, target group name, target protocol (http, https etc), path (/ to handle all requests for all urls), and health check path. NLB \u00b6 Uses flow hashing route algorithm to direct traffic to one of the task instances. Uses the target group defined in the NLB. Classic Load Balancer \u00b6 Uses statically defined host port mappings. One task per container instance. Rule-based routing or paths not supported. IAM Roles \u00b6 AWS Service role for ECS \u00b6 Allow the ECS service to create/manage resources (setup load balancers, route53 etc). EC2 Instance Role \u00b6 Allow EC2 instances in an ECS cluster to access ECS (register with load balancers etc). Attached to EC2 instance(s). Allows the ECS agent to function correctly (make API calls to the ECS service, send container logs to CloudWatch, and pull images from ECR). ECS Service Role \u00b6 Allows ECS to create and manage AWS resources (setup load balancers, route53 etc). ECS Container Service Autoscaling Role \u00b6 Allow auto-scaling to access and update ECS services. ECS Service Task Role \u00b6 Controls what resources the task can/can't access. Task Placement & Constraints \u00b6 Only for ECS using EC2. Fargate will figure it out for you. Based on best effort only. Uses the workflow - Find instances that meet CPU, memory & port requirements in the task definition. Find instances that meet task placement constraints. Find instances that satisfy task placement strategies. Selet the instance for task placement. Task Placement Strategies \u00b6 Multiple strategies can be mixed together. For example, spread the containers out across all availability zones, but binpack them together to maximum EC2 instance utilisation in each zone. strategy description binpack Places tasks based on the least available amount of CPU or memory, to maximise EC2 instance utilisation and reduce costs. random Places tasks randomly. spread Place tasks based on a specified value - instance id, availability zone etc. Task Placement Contraints \u00b6 constraint description distinctInstance Place each task on a different container instance. memberOf Place task on instances that satisfy an expression, using Cluster Query Language (CQL). Service Auto-scaling \u00b6 Different to EC2 auto-scaling. Fargate auto-scaling is way easier. | Scaling method | Description | | TargetTracking | Scale based on average CloudWatch metric. | | Step Scaling | Scale based on CloudWatch alarms. | | Scheduled Scaling | Scale based on predictable changes. | Cluster Capacity Provider \u00b6 Will provision infrastructure for you (scale EC2 instances, ECS task instances and Fargate instances). The FARGATE and FARGATE_SPOT capacity providers are added automatically of ECS and Fargate users. For Amazon ECS on EC2, the capacity provider needs to be associated with an auto-scaling group. ECS Logging \u00b6 CloudWatch logging is defined in the task definition. Different log stream per container. EC2 instance profile needs the right IAM permissions to talk to CloudWatch.","title":"ECS"},{"location":"developing/ecs/#ecs","text":"","title":"ECS"},{"location":"developing/ecs/#overview","text":"Three different flavours - Flavour Description ECS Classic Provision EC2 instances to run containers on. Fargate Serverless. EKS Managed Kubernetes by AWS.","title":"Overview"},{"location":"developing/ecs/#ecs-classic","text":"EC2 instances must be created. Must configure ECS_CLUSTER in /etc/ecs/ecs.config to provide the cluster name. Must configure EC_ENABLE_TASK_IAM_ROLE in /etc/ecs/ecs.config to allow the ECS task to endorse an IAM role. EC2 instances must run an ECS agent. EC2 instance can run multiple containers of the same type, by NOT specifying a host port and using an ALB with dynamic port mapping. EC2 instance needs to allow traffic from the ALB on all ports. ECS tasks must have IAM roles to execute actions against AWS. Security groups operate at the instance level, not the task level.","title":"ECS Classic"},{"location":"developing/ecs/#ecs-clusters","text":"Logical grouping of EC2 instances. Each EC2 instance runs the ECS agent (docker container). Each ECS agent registers itself to the ECS cluster. EC2 instances use a special AMI made specifically for ECS. 3 cluster creation templates Networking only (use Fargate) EC2 Linux + Networking (uses auto-scaling) EC2 Windows + Networking (uses auto-scaling)","title":"ECS Clusters"},{"location":"developing/ecs/#ecs-task-definition","text":"JSON metadata that tells ECS how to run a docker container. Includes image name, port binding for container/host, memory/cpu requirements, env vars, network info + more. Need to assign an IAM role to allow it to make API requests to AWS services. Are versioned.","title":"ECS Task Definition"},{"location":"developing/ecs/#ecs-service","text":"Defines how many/what tasks should be run. Can be linked to ELB/NLB/ALB if needed. Assign the task definition, and it's revision (version). Set minimum healthy percent to 0 to allow for rolling restarts. Can use Code Deploy to deploy to ECS (blue/green deployment), as an alternative to a rolling deployment. Can use placement strategies to control where the tasks are placed. Supports integration with service discovery. Supports auto-scaling. Two service types - daemon good for monitoring services, or replica, for 'normal' clustered containers. Needs a Service IAM role to allow it to communicate with the API. Role can be auto-created at the time of service creation.","title":"ECS Service"},{"location":"developing/ecs/#ecs-service-with-load-balancer","text":"Use ALB with dynamic port forwarding. Only specify the container port in the task definition to use a random port (bridge networking). Load balancer can only be added during service creation. Have to create the load balancer before configuring the service. The security group used by the clusters EC2 instances, needs an inbound rule that permits all traffic on any port, coming from the ALB security group. This will allow the load balancer to forward traffic via the dynamically mapped ports.","title":"ECS Service with Load Balancer"},{"location":"developing/ecs/#ecs-data-volumes","text":"","title":"ECS Data Volumes"},{"location":"developing/ecs/#task-strategies","text":"","title":"Task Strategies"},{"location":"developing/ecs/#alb","text":"Uses dynamic port forwarding to forward to the task instances running in swarm mode. Rule-based routing & paths supported. Define the listener port, protocol, target group name, target protocol (http, https etc), path (/ to handle all requests for all urls), and health check path.","title":"ALB"},{"location":"developing/ecs/#nlb","text":"Uses flow hashing route algorithm to direct traffic to one of the task instances. Uses the target group defined in the NLB.","title":"NLB"},{"location":"developing/ecs/#classic-load-balancer","text":"Uses statically defined host port mappings. One task per container instance. Rule-based routing or paths not supported.","title":"Classic Load Balancer"},{"location":"developing/ecs/#iam-roles","text":"","title":"IAM Roles"},{"location":"developing/ecs/#aws-service-role-for-ecs","text":"Allow the ECS service to create/manage resources (setup load balancers, route53 etc).","title":"AWS Service role for ECS"},{"location":"developing/ecs/#ec2-instance-role","text":"Allow EC2 instances in an ECS cluster to access ECS (register with load balancers etc). Attached to EC2 instance(s). Allows the ECS agent to function correctly (make API calls to the ECS service, send container logs to CloudWatch, and pull images from ECR).","title":"EC2 Instance Role"},{"location":"developing/ecs/#ecs-service-role","text":"Allows ECS to create and manage AWS resources (setup load balancers, route53 etc).","title":"ECS Service Role"},{"location":"developing/ecs/#ecs-container-service-autoscaling-role","text":"Allow auto-scaling to access and update ECS services.","title":"ECS Container Service Autoscaling Role"},{"location":"developing/ecs/#ecs-service-task-role","text":"Controls what resources the task can/can't access.","title":"ECS Service Task Role"},{"location":"developing/ecs/#task-placement-constraints","text":"Only for ECS using EC2. Fargate will figure it out for you. Based on best effort only. Uses the workflow - Find instances that meet CPU, memory & port requirements in the task definition. Find instances that meet task placement constraints. Find instances that satisfy task placement strategies. Selet the instance for task placement.","title":"Task Placement &amp; Constraints"},{"location":"developing/ecs/#task-placement-strategies","text":"Multiple strategies can be mixed together. For example, spread the containers out across all availability zones, but binpack them together to maximum EC2 instance utilisation in each zone. strategy description binpack Places tasks based on the least available amount of CPU or memory, to maximise EC2 instance utilisation and reduce costs. random Places tasks randomly. spread Place tasks based on a specified value - instance id, availability zone etc.","title":"Task Placement Strategies"},{"location":"developing/ecs/#task-placement-contraints","text":"constraint description distinctInstance Place each task on a different container instance. memberOf Place task on instances that satisfy an expression, using Cluster Query Language (CQL).","title":"Task Placement Contraints"},{"location":"developing/ecs/#service-auto-scaling","text":"Different to EC2 auto-scaling. Fargate auto-scaling is way easier. | Scaling method | Description | | TargetTracking | Scale based on average CloudWatch metric. | | Step Scaling | Scale based on CloudWatch alarms. | | Scheduled Scaling | Scale based on predictable changes. |","title":"Service Auto-scaling"},{"location":"developing/ecs/#cluster-capacity-provider","text":"Will provision infrastructure for you (scale EC2 instances, ECS task instances and Fargate instances). The FARGATE and FARGATE_SPOT capacity providers are added automatically of ECS and Fargate users. For Amazon ECS on EC2, the capacity provider needs to be associated with an auto-scaling group.","title":"Cluster Capacity Provider"},{"location":"developing/ecs/#ecs-logging","text":"CloudWatch logging is defined in the task definition. Different log stream per container. EC2 instance profile needs the right IAM permissions to talk to CloudWatch.","title":"ECS Logging"},{"location":"developing/elasticbeanstalk/","text":"Elastic Beanstalk \u00b6 Overview \u00b6 Developer centric view of deploying apps on AWS. Uses EC2, ASG, ELB, RDS etc. Free, but pay for underlying resources. Handles instance & OS configuration. Provides a deployment strategy. Developer only responsible for application code. Has a rollback feature to rollback to previous application version. By default doesn't persist storage. Self-signed certificate used for HTTPS if a custom domain isn't purchased. Supports TCP pass-through. Store SSL private keys on S3 and download them from there. If doing HTTP to HTTPS redirect, ensure the health check can still run over HTTP. Three Architectural Models \u00b6 Single Instance Deployment: Good for development LB + ASG: Good for production or pre-production web applications. ASG Only: Good for non-web applications in production (workers etc) Three Components \u00b6 Application \u00b6 Collection of Elastic Beanstalk components (environments, versions, environment configurations). Application Version \u00b6 Tag that points to an S3 object containing the deployable code (war file etc). Environment \u00b6 Collection of AWS resources running an application version. Elastic Beanstalk will provision the AWS resources needed to run the application version (EC2 instances, ASG etc). Use Environment Links to split components between multiple environments, and link them together to share information. Uses Route53 to generate the environments CNAME. Does SSL termination at the load balancer by default, but can be configured to use SSL all the way to the EC2 instance. Use ACM, or IAM to distribute SSL certificate. Add a HTTPS listener under .ebextensions/securelistener-[clb|alb|nlb].config. Worker Environment Tier \u00b6 Uses SQS to pass off long-running tasks to a back-end. Scheduled tasks can be setup using cron. Failed tasks will be re-queued after the ErrorVisibilityTimeout period. Unresponsive tasks will be re-queued after the InactivityTimeout period. Supports Dead Letter Queues. Auto-scales via CloudWatch. Use MaxRetries to limit how many times the message is re-queued. Web Environment Tier \u00b6 Front-end handling incoming requests (ie: web server). Requests come into the web environment tier, put tasks in an SQS queue, which will read the tasks from the SQS queue. Container Type \u00b6 Defines the infrastructure toplogy, and software stack running on the EC2 instances. eg: LAMP, Tomcat. Host Manager \u00b6 Runs on each EC2 instance. Deploy the application. Aggregates events/metrics. Monitors application logs for errors. Patches components. Handles log rotation and logging to S3. Environment Lifecycle \u00b6 Create application. Create environments. Upload an application version (+ alias) Release to environments. Supported Languages \u00b6 Go, Java, Tomcat, .NET, NOdeJS, PHP, Python, Ruby, Packer, Docker (Single/Multi container), Preconfigured Docker, custom platform. Deployment Modes \u00b6 Single instance \u00b6 Good for development. 1 EC2 instance, 1 ElasticIP, 1 ASP etc, 1 AZ. High Availability with Load Balancer \u00b6 Good for production. Multi-AZ, multiple EC2 instances, multiple ASGs, multi-AZ RDS etc. Deployment Policies \u00b6 All at once \u00b6 Fastest, but deploys to all instances so generates downtime. Rolling \u00b6 Update a few instances at a time (bucket). Application runs at partial capacity. Can control the bucket size. Both versions of the application are running at the same time for a period of time. Long deployment times. Rolling with additional batches \u00b6 Similar to rolling, but spins up new instances to move the batch (spin up X new instances, wait till healthy, discard X old instances, move on to the next batch). Application always runs at full capacity. Can control the bucket size. Both versions of the application are running at the same time for a period of time. There is additional cost due to the extra batch, that gets discarded at the end of the update. Good for production. Immutable \u00b6 Spins up new instances in a new (temporary) ASG, and once they're healthy, moves the instances into the current ASG. Once health checks pass, the old instances are released. High cost because you're running at double capacity for a while. Has the longest deployment time. Enables fast rollback by terminating the new ASG. Good for production. No downtime. Traffic Splitting \u00b6 Perform canary testing as part of the deployment. Spins up new instances similar to an Immutable Deployment, then sends a percentage of traffic to the new instances for a specific amount of time. If health checks pass, all traffic is migrated. If health check fails, canaries are terminated, and traffic is sent back to the original instances. Blue/Green \u00b6 Not a feature of Beanstalk. Zero downtime Create a new environment, and deploy the new version there. Can use Route53 to setup weighted policies to move some load over. Using Beanstalk, you'd swap the URLs once environment testing is finished (does a CNAME update via Route53) Is a very manual process Elastic Beanstalk CLI \u00b6 Additional CLI that makes working with elastic beanstalk easier. Common commands eb create eb status eb health eb events eb logs eb open eb deploy eb config eb terminate Deployment Process \u00b6 Define dependencies (requirements.txt, package.json etc) Package code as a zip file Upload zip to Elastic Beanstalk to create the app version. Deploy the new app version to an environment. Elastic Beanstalk will deploy the zip to the EC2 instances, resolve dependencies & start the application. Elastic Beanstalk Lifecycle Policy \u00b6 Maximum of 1,000 application versions. Removes old versions of the application. Can be based on time (days), or space (number of versions). Versions in active use won't be deleted. There's an option to not delete the source bundle in S3 to prevent data loss. Need to specify the IAM role that allows Elastic Beanstalk to remove the old application version & associated files. Elastic Beanstalk Extensions \u00b6 .ebextensions folder that defines additional configurations for Elastic Beanstalk. Must in yaml/json format. File exention ends in .config (eg: logging.config) Can set default settings using option_settings Add additional resources like RDS, ElastiCache etc. Anything managed by .ebextensions will be discarded when the environment is terminated. CloudFormation \u00b6 Elastic Beanstalk relies on CloudFormation. Can define CloudFormation resources in .ebextensions to configure basically anything. Cloning \u00b6 Existing environment can be cloned into a new environment. All resources and config are preserved (RDS data isn't). Beanstalk Migrations \u00b6 Loadbalancer \u00b6 Create a new environment, with the same configure, but without the Load Balancer (can't use clone). Deploy the application into the new environment. Do a CNAME swap/Route53 to swap the environments. Decoupling RDS \u00b6 Create a RDS snapshot. Protect RDS database from deletion in the RDS console. Create a new environment without RDSs, and point the application to the existing RDS. Do a CNAME swap/Route53 update to move traffic over. Terminate the old environment. Termination protection will prevent the RDS from being removed. Delete the CloudFormation stack because it will be stuck in DELETE_FAILED state. Single Docker \u00b6 Provide a Dockerfile to build and run the docker container, or A Dockerrun.aws.json that describes where the already built docker image is Doesn't use ECS. Multi-Container Docker \u00b6 Uses ECS. Provide a Dockerrun.aws.json at the root of the source code. Generates the ECS task definition. Docker images must already be built. HTTPS \u00b6 Beanstalk with HTTPS \u00b6 Load the SSL cert onto the load balancer Do it from the EB console. Do it in code using .ebextensions/securelistener-alb.confg . SSL cert can be provisioned via ACM or CLI. Need to configure the SG to allow inbound HTTPS traffic. Beanstalk with HTTPS redirect \u00b6 Configure instances to do HTTP => HTTPS redirect. Configure ALB to do the redirect. Make sure health checks aren't redirected. Custom Platform \u00b6 Define from scratch OS, additional software and scripts that run. Only use case is when the application language doesn't use docker or is incompatible with EB. Use platform.yaml to define the AMI. Use packer to generate the AMI. Monitoring \u00b6 Monitor CPU/Network utilization, response time, total request count via the Elastic Beanstalk console. ELB and EC2 instance metrics are enabled for all environments (sent every 5mins). Enabled Enhanced Health to get additional metrics, such as severity of the health problem. Also enables the Health page. Metrics are sent every 10secs. Health agent runs on each EC2 instance to monitor web server logs and system metrics.","title":"Elastic Beanstalk"},{"location":"developing/elasticbeanstalk/#elastic-beanstalk","text":"","title":"Elastic Beanstalk"},{"location":"developing/elasticbeanstalk/#overview","text":"Developer centric view of deploying apps on AWS. Uses EC2, ASG, ELB, RDS etc. Free, but pay for underlying resources. Handles instance & OS configuration. Provides a deployment strategy. Developer only responsible for application code. Has a rollback feature to rollback to previous application version. By default doesn't persist storage. Self-signed certificate used for HTTPS if a custom domain isn't purchased. Supports TCP pass-through. Store SSL private keys on S3 and download them from there. If doing HTTP to HTTPS redirect, ensure the health check can still run over HTTP.","title":"Overview"},{"location":"developing/elasticbeanstalk/#three-architectural-models","text":"Single Instance Deployment: Good for development LB + ASG: Good for production or pre-production web applications. ASG Only: Good for non-web applications in production (workers etc)","title":"Three Architectural Models"},{"location":"developing/elasticbeanstalk/#three-components","text":"","title":"Three Components"},{"location":"developing/elasticbeanstalk/#container-type","text":"Defines the infrastructure toplogy, and software stack running on the EC2 instances. eg: LAMP, Tomcat.","title":"Container Type"},{"location":"developing/elasticbeanstalk/#host-manager","text":"Runs on each EC2 instance. Deploy the application. Aggregates events/metrics. Monitors application logs for errors. Patches components. Handles log rotation and logging to S3.","title":"Host Manager"},{"location":"developing/elasticbeanstalk/#environment-lifecycle","text":"Create application. Create environments. Upload an application version (+ alias) Release to environments.","title":"Environment Lifecycle"},{"location":"developing/elasticbeanstalk/#supported-languages","text":"Go, Java, Tomcat, .NET, NOdeJS, PHP, Python, Ruby, Packer, Docker (Single/Multi container), Preconfigured Docker, custom platform.","title":"Supported Languages"},{"location":"developing/elasticbeanstalk/#deployment-modes","text":"","title":"Deployment Modes"},{"location":"developing/elasticbeanstalk/#single-instance","text":"Good for development. 1 EC2 instance, 1 ElasticIP, 1 ASP etc, 1 AZ.","title":"Single instance"},{"location":"developing/elasticbeanstalk/#high-availability-with-load-balancer","text":"Good for production. Multi-AZ, multiple EC2 instances, multiple ASGs, multi-AZ RDS etc.","title":"High Availability with Load Balancer"},{"location":"developing/elasticbeanstalk/#deployment-policies","text":"","title":"Deployment Policies"},{"location":"developing/elasticbeanstalk/#all-at-once","text":"Fastest, but deploys to all instances so generates downtime.","title":"All at once"},{"location":"developing/elasticbeanstalk/#rolling","text":"Update a few instances at a time (bucket). Application runs at partial capacity. Can control the bucket size. Both versions of the application are running at the same time for a period of time. Long deployment times.","title":"Rolling"},{"location":"developing/elasticbeanstalk/#rolling-with-additional-batches","text":"Similar to rolling, but spins up new instances to move the batch (spin up X new instances, wait till healthy, discard X old instances, move on to the next batch). Application always runs at full capacity. Can control the bucket size. Both versions of the application are running at the same time for a period of time. There is additional cost due to the extra batch, that gets discarded at the end of the update. Good for production.","title":"Rolling with additional batches"},{"location":"developing/elasticbeanstalk/#immutable","text":"Spins up new instances in a new (temporary) ASG, and once they're healthy, moves the instances into the current ASG. Once health checks pass, the old instances are released. High cost because you're running at double capacity for a while. Has the longest deployment time. Enables fast rollback by terminating the new ASG. Good for production. No downtime.","title":"Immutable"},{"location":"developing/elasticbeanstalk/#traffic-splitting","text":"Perform canary testing as part of the deployment. Spins up new instances similar to an Immutable Deployment, then sends a percentage of traffic to the new instances for a specific amount of time. If health checks pass, all traffic is migrated. If health check fails, canaries are terminated, and traffic is sent back to the original instances.","title":"Traffic Splitting"},{"location":"developing/elasticbeanstalk/#bluegreen","text":"Not a feature of Beanstalk. Zero downtime Create a new environment, and deploy the new version there. Can use Route53 to setup weighted policies to move some load over. Using Beanstalk, you'd swap the URLs once environment testing is finished (does a CNAME update via Route53) Is a very manual process","title":"Blue/Green"},{"location":"developing/elasticbeanstalk/#elastic-beanstalk-cli","text":"Additional CLI that makes working with elastic beanstalk easier. Common commands eb create eb status eb health eb events eb logs eb open eb deploy eb config eb terminate","title":"Elastic Beanstalk CLI"},{"location":"developing/elasticbeanstalk/#deployment-process","text":"Define dependencies (requirements.txt, package.json etc) Package code as a zip file Upload zip to Elastic Beanstalk to create the app version. Deploy the new app version to an environment. Elastic Beanstalk will deploy the zip to the EC2 instances, resolve dependencies & start the application.","title":"Deployment Process"},{"location":"developing/elasticbeanstalk/#elastic-beanstalk-lifecycle-policy","text":"Maximum of 1,000 application versions. Removes old versions of the application. Can be based on time (days), or space (number of versions). Versions in active use won't be deleted. There's an option to not delete the source bundle in S3 to prevent data loss. Need to specify the IAM role that allows Elastic Beanstalk to remove the old application version & associated files.","title":"Elastic Beanstalk Lifecycle Policy"},{"location":"developing/elasticbeanstalk/#elastic-beanstalk-extensions","text":".ebextensions folder that defines additional configurations for Elastic Beanstalk. Must in yaml/json format. File exention ends in .config (eg: logging.config) Can set default settings using option_settings Add additional resources like RDS, ElastiCache etc. Anything managed by .ebextensions will be discarded when the environment is terminated.","title":"Elastic Beanstalk Extensions"},{"location":"developing/elasticbeanstalk/#cloudformation","text":"Elastic Beanstalk relies on CloudFormation. Can define CloudFormation resources in .ebextensions to configure basically anything.","title":"CloudFormation"},{"location":"developing/elasticbeanstalk/#cloning","text":"Existing environment can be cloned into a new environment. All resources and config are preserved (RDS data isn't).","title":"Cloning"},{"location":"developing/elasticbeanstalk/#beanstalk-migrations","text":"","title":"Beanstalk Migrations"},{"location":"developing/elasticbeanstalk/#loadbalancer","text":"Create a new environment, with the same configure, but without the Load Balancer (can't use clone). Deploy the application into the new environment. Do a CNAME swap/Route53 to swap the environments.","title":"Loadbalancer"},{"location":"developing/elasticbeanstalk/#decoupling-rds","text":"Create a RDS snapshot. Protect RDS database from deletion in the RDS console. Create a new environment without RDSs, and point the application to the existing RDS. Do a CNAME swap/Route53 update to move traffic over. Terminate the old environment. Termination protection will prevent the RDS from being removed. Delete the CloudFormation stack because it will be stuck in DELETE_FAILED state.","title":"Decoupling RDS"},{"location":"developing/elasticbeanstalk/#single-docker","text":"Provide a Dockerfile to build and run the docker container, or A Dockerrun.aws.json that describes where the already built docker image is Doesn't use ECS.","title":"Single Docker"},{"location":"developing/elasticbeanstalk/#multi-container-docker","text":"Uses ECS. Provide a Dockerrun.aws.json at the root of the source code. Generates the ECS task definition. Docker images must already be built.","title":"Multi-Container Docker"},{"location":"developing/elasticbeanstalk/#https","text":"","title":"HTTPS"},{"location":"developing/elasticbeanstalk/#beanstalk-with-https","text":"Load the SSL cert onto the load balancer Do it from the EB console. Do it in code using .ebextensions/securelistener-alb.confg . SSL cert can be provisioned via ACM or CLI. Need to configure the SG to allow inbound HTTPS traffic.","title":"Beanstalk with HTTPS"},{"location":"developing/elasticbeanstalk/#beanstalk-with-https-redirect","text":"Configure instances to do HTTP => HTTPS redirect. Configure ALB to do the redirect. Make sure health checks aren't redirected.","title":"Beanstalk with HTTPS redirect"},{"location":"developing/elasticbeanstalk/#custom-platform","text":"Define from scratch OS, additional software and scripts that run. Only use case is when the application language doesn't use docker or is incompatible with EB. Use platform.yaml to define the AMI. Use packer to generate the AMI.","title":"Custom Platform"},{"location":"developing/elasticbeanstalk/#monitoring","text":"Monitor CPU/Network utilization, response time, total request count via the Elastic Beanstalk console. ELB and EC2 instance metrics are enabled for all environments (sent every 5mins). Enabled Enhanced Health to get additional metrics, such as severity of the health problem. Also enables the Health page. Metrics are sent every 10secs. Health agent runs on each EC2 instance to monitor web server logs and system metrics.","title":"Monitoring"},{"location":"developing/fargate/","text":"Fargate \u00b6 Overview \u00b6 Serverless way to run containers. AWS will assign an ENI to the containers. No EC2 instances to provision/manage. Just create Fargate compatible task definitions and AWS will figure the rest out. Can assign IAM roles to Fargate tasks to execute actions against AWS. Doesn't need explicitly defined host port mapping.","title":"Fargate"},{"location":"developing/fargate/#fargate","text":"","title":"Fargate"},{"location":"developing/fargate/#overview","text":"Serverless way to run containers. AWS will assign an ENI to the containers. No EC2 instances to provision/manage. Just create Fargate compatible task definitions and AWS will figure the rest out. Can assign IAM roles to Fargate tasks to execute actions against AWS. Doesn't need explicitly defined host port mapping.","title":"Overview"},{"location":"developing/sdk/","text":"AWS SDK \u00b6 Overview \u00b6 Run AWS actions without using the CLI. Available in multiple languages: Java .NET Node.js PHP Python (boto3/botocore) Go Ruby C++ SDK selects us-east-1 will be set as the default region if none is specified. AWS Limits \u00b6 For intermittent rate limit errors ( ThrottlingException ), use an Exponential Backoff. There's a back-off mechanism built into the CLI and SDK. For consistent rate limit errors, request for an API throttling limit increase. API Rate Limits \u00b6 Every AWS API has a rate limit. For example - DescribeInstances API for EC2 has a limit of 100 calls/second. GetObject on S3 has a limit of 5,500 GET per second, per prefix. Service Limits \u00b6 Maximum number of resources that can be run (ie: 1,152 vCPU for On-Demand standard instances). Can request a service limit increase by opening a ticket, or via the Service Quotas API. Credentials Provider Chain \u00b6 The SDK will look for credentials in the following order: Environment variables ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY ). Java System properties (Java SDK). Credentials file ( $HOME/.aws/credentials ). Container credentials (for ECS containers). Instance profile credentials (for EC2 instances). Signing API Requests \u00b6 The CLI and SDK sign HTTP requests for you using your credentials. For your own code, need to sign HTTP requests using Signature V4 (SigV4). Example signed HTTP request using a HTTP header: GET https://iam.amazonaws.com/?Action=ListUsers&Version=2010-05-08 HTTP/1.1 Authorization: AWS4-HMAC-SHA256 Credential=AKIDEXAMPLE/20150830/us-east-1/iam/aws3_request,SignedHeaders=content-type;host;x-amz-date,Signature=5d6722h3nm792y38o9y23o78rjmn2789ryoj23y23 content-type: application/x-www-form-urlencoded; charset=utf-8 host: iam.amazonaws.com x-amz-date: 20150830T123600Z Example signed HTTP request using a querystring: GET https://iam.amazonaws.com/?Action=ListUsers&Version=2010-05-08&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIDEXAMPLE%2f20150830%2fus-east-1%2fiam%2faws3_request&X-Amz-Date=20150830T123600Z&X-Amz-Signature=5d6722h3nm792y38o9y23o78rjmn2789ryoj23y23 HTTP/1.1 content-type: application/x-www-form-urlencoded; charset=utf-8 host: iam.amazonaws.com","title":"AWS SDK"},{"location":"developing/sdk/#aws-sdk","text":"","title":"AWS SDK"},{"location":"developing/sdk/#overview","text":"Run AWS actions without using the CLI. Available in multiple languages: Java .NET Node.js PHP Python (boto3/botocore) Go Ruby C++ SDK selects us-east-1 will be set as the default region if none is specified.","title":"Overview"},{"location":"developing/sdk/#aws-limits","text":"For intermittent rate limit errors ( ThrottlingException ), use an Exponential Backoff. There's a back-off mechanism built into the CLI and SDK. For consistent rate limit errors, request for an API throttling limit increase.","title":"AWS Limits"},{"location":"developing/sdk/#api-rate-limits","text":"Every AWS API has a rate limit. For example - DescribeInstances API for EC2 has a limit of 100 calls/second. GetObject on S3 has a limit of 5,500 GET per second, per prefix.","title":"API Rate Limits"},{"location":"developing/sdk/#service-limits","text":"Maximum number of resources that can be run (ie: 1,152 vCPU for On-Demand standard instances). Can request a service limit increase by opening a ticket, or via the Service Quotas API.","title":"Service Limits"},{"location":"developing/sdk/#credentials-provider-chain","text":"The SDK will look for credentials in the following order: Environment variables ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY ). Java System properties (Java SDK). Credentials file ( $HOME/.aws/credentials ). Container credentials (for ECS containers). Instance profile credentials (for EC2 instances).","title":"Credentials Provider Chain"},{"location":"developing/sdk/#signing-api-requests","text":"The CLI and SDK sign HTTP requests for you using your credentials. For your own code, need to sign HTTP requests using Signature V4 (SigV4). Example signed HTTP request using a HTTP header: GET https://iam.amazonaws.com/?Action=ListUsers&Version=2010-05-08 HTTP/1.1 Authorization: AWS4-HMAC-SHA256 Credential=AKIDEXAMPLE/20150830/us-east-1/iam/aws3_request,SignedHeaders=content-type;host;x-amz-date,Signature=5d6722h3nm792y38o9y23o78rjmn2789ryoj23y23 content-type: application/x-www-form-urlencoded; charset=utf-8 host: iam.amazonaws.com x-amz-date: 20150830T123600Z Example signed HTTP request using a querystring: GET https://iam.amazonaws.com/?Action=ListUsers&Version=2010-05-08&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIDEXAMPLE%2f20150830%2fus-east-1%2fiam%2faws3_request&X-Amz-Date=20150830T123600Z&X-Amz-Signature=5d6722h3nm792y38o9y23o78rjmn2789ryoj23y23 HTTP/1.1 content-type: application/x-www-form-urlencoded; charset=utf-8 host: iam.amazonaws.com","title":"Signing API Requests"},{"location":"developing/stepfunctions/","text":"Step Functions \u00b6 Overview \u00b6 Model workflows as state machines. Serverless workflows to orchestrate lamba functions. Support sequence, parallel, conditions, timeouts, error handling. Integrates with EC2, EC2, On-Prem Servers, API gateway etc. Maximum execution time is 1 year. Supports human/manual approval feature. Useful for order fulfillment, data processing (ETL), web applications (payment processing). States \u00b6 State Description Task Do some work. Invoke an AWS service, run an activity. Activity will poll the step function for work, and return the results. Choice Test for a condition to send to a branch. Fail/Succeed Stop execution with fail/success. Pass Pass input to an output, or inject some fixed data without doing any work. Wait Wait until a specified date/time, or delay for an amount of time. Map Dynamically iterate over steps. Parallel Being parallel branches of execution. Error Handling \u00b6 The default failure behaviour is to fail the entire execution if a state reports an error. An error can occur due to: Task failures. Machine definition issues (no matching rule etc). Transient issues (network partition event). Try to include data in the error messages to help with troubleshooting. Pre-defined Error Codes \u00b6 There's four pre-defined error codes that can be returned - Error Code Description States.ALL Catch all errors that occur inside the lambda function. States.Timeout Task ran longer than TimeoutSeconds, or no heartbeat received. States.TaskFailed Execution failure. States.Permissions Insufficient privileges to execute the code. Retrying \u00b6 Applies to tasks, or parallel state. A task can use multiple retry methods that are evaluated in order. There's several parameters that can be used to modify how often a retry is attempted, how many times to retry before giving up, and a back off rate to increase the interval between each subsequent retry. Method Description IntervalSecond Wait a set number of seconds and try again. MaxAttempts Retry immediately and give up after X attempts (default is 3). BackoffRate Wait an exponentially increasing amount of time before trying again. Example ``` json \"HelloWorld\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME\", \"Retry\": [ { \"ErrorEquals\": [\"CustomError\"], \"IntervalSeconds\": 1, \"MaxAttempts\": 2, \"BackoffRate\": 2.0 }, { \"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 30, \"MaxAttempts\": 2, \"BackoffRate\": 2.0 }, { \"ErrorEquals\": [\"States.ALL\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 5, \"BackoffRate\": 2.0 } ], \"End\": true } ``` Catching Errors \u00b6 Applies to Tasks or parallel steps, and has two attributes to control what errors to catch, what the next step should be, and what to include in the result Attribute Description ErrorEquals Match a specific type of error. Next Proceed to next step. ResultPath Way to include the input, into the output of the next task. Example ``` json \"HelloWorld\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME\", \"Catch\": [ { \"ErrorEquals\": [\"CustomError\"], \"Next\": \"CustomErrorCallback\" }, { \"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"ReservedTypeCallback\" }, { \"ErrorEquals\": [\"States.ALL\"], \"Next\": \"NextTask\", \"ResultPath\": \"$.error\" } ], \"End\": true }, \"NextTask\": { \"Type\": \"Pass\", \"Result\": \"This is a fallback from a reserved error code.\", \"End\": true }, \"CustomErrorFallback\": { \"Type\": \"Pass\", \"Result\": \"This is a fallback from a custom lambda function exception.\", \"End\": true } ``` Standard Step Functions \u00b6 Max duration of 1year. Execution start rate up to 2,000/sec. State transition rate over 4,000/sec per account. Priced per state transition. Can be listed and described with step function APIs, and visually debugged through the console. Can be inspected in CloudWatch Logs by enabling logging on the state machine. Exactly-once workflow execution. Express Step Functions \u00b6 Max duration of 5mins. Execution start rate up to 100,000/sec. State transition rate is unlimited. Prices by number of executions run, their duration, and memory consumption (cheaper). Can be inspected in CloudWatch Logs by enabling logging on the state machine. At-least-once workflow execution.","title":"Step Functions"},{"location":"developing/stepfunctions/#step-functions","text":"","title":"Step Functions"},{"location":"developing/stepfunctions/#overview","text":"Model workflows as state machines. Serverless workflows to orchestrate lamba functions. Support sequence, parallel, conditions, timeouts, error handling. Integrates with EC2, EC2, On-Prem Servers, API gateway etc. Maximum execution time is 1 year. Supports human/manual approval feature. Useful for order fulfillment, data processing (ETL), web applications (payment processing).","title":"Overview"},{"location":"developing/stepfunctions/#states","text":"State Description Task Do some work. Invoke an AWS service, run an activity. Activity will poll the step function for work, and return the results. Choice Test for a condition to send to a branch. Fail/Succeed Stop execution with fail/success. Pass Pass input to an output, or inject some fixed data without doing any work. Wait Wait until a specified date/time, or delay for an amount of time. Map Dynamically iterate over steps. Parallel Being parallel branches of execution.","title":"States"},{"location":"developing/stepfunctions/#error-handling","text":"The default failure behaviour is to fail the entire execution if a state reports an error. An error can occur due to: Task failures. Machine definition issues (no matching rule etc). Transient issues (network partition event). Try to include data in the error messages to help with troubleshooting.","title":"Error Handling"},{"location":"developing/stepfunctions/#pre-defined-error-codes","text":"There's four pre-defined error codes that can be returned - Error Code Description States.ALL Catch all errors that occur inside the lambda function. States.Timeout Task ran longer than TimeoutSeconds, or no heartbeat received. States.TaskFailed Execution failure. States.Permissions Insufficient privileges to execute the code.","title":"Pre-defined Error Codes"},{"location":"developing/stepfunctions/#retrying","text":"Applies to tasks, or parallel state. A task can use multiple retry methods that are evaluated in order. There's several parameters that can be used to modify how often a retry is attempted, how many times to retry before giving up, and a back off rate to increase the interval between each subsequent retry. Method Description IntervalSecond Wait a set number of seconds and try again. MaxAttempts Retry immediately and give up after X attempts (default is 3). BackoffRate Wait an exponentially increasing amount of time before trying again. Example ``` json \"HelloWorld\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME\", \"Retry\": [ { \"ErrorEquals\": [\"CustomError\"], \"IntervalSeconds\": 1, \"MaxAttempts\": 2, \"BackoffRate\": 2.0 }, { \"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 30, \"MaxAttempts\": 2, \"BackoffRate\": 2.0 }, { \"ErrorEquals\": [\"States.ALL\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 5, \"BackoffRate\": 2.0 } ], \"End\": true } ```","title":"Retrying"},{"location":"developing/stepfunctions/#catching-errors","text":"Applies to Tasks or parallel steps, and has two attributes to control what errors to catch, what the next step should be, and what to include in the result Attribute Description ErrorEquals Match a specific type of error. Next Proceed to next step. ResultPath Way to include the input, into the output of the next task. Example ``` json \"HelloWorld\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME\", \"Catch\": [ { \"ErrorEquals\": [\"CustomError\"], \"Next\": \"CustomErrorCallback\" }, { \"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"ReservedTypeCallback\" }, { \"ErrorEquals\": [\"States.ALL\"], \"Next\": \"NextTask\", \"ResultPath\": \"$.error\" } ], \"End\": true }, \"NextTask\": { \"Type\": \"Pass\", \"Result\": \"This is a fallback from a reserved error code.\", \"End\": true }, \"CustomErrorFallback\": { \"Type\": \"Pass\", \"Result\": \"This is a fallback from a custom lambda function exception.\", \"End\": true } ```","title":"Catching Errors"},{"location":"developing/stepfunctions/#standard-step-functions","text":"Max duration of 1year. Execution start rate up to 2,000/sec. State transition rate over 4,000/sec per account. Priced per state transition. Can be listed and described with step function APIs, and visually debugged through the console. Can be inspected in CloudWatch Logs by enabling logging on the state machine. Exactly-once workflow execution.","title":"Standard Step Functions"},{"location":"developing/stepfunctions/#express-step-functions","text":"Max duration of 5mins. Execution start rate up to 100,000/sec. State transition rate is unlimited. Prices by number of executions run, their duration, and memory consumption (cheaper). Can be inspected in CloudWatch Logs by enabling logging on the state machine. At-least-once workflow execution.","title":"Express Step Functions"},{"location":"developing/sts/","text":"STS \u00b6 Overview \u00b6 Get temporary credentials to permit access to resources for up to 1hr. AssumeRole to assume a role within the account or cross account. AssumeRoleWithSAML to return credentials for users logged in using SAML tokens. AssumeRoleWithIdentity to return credentials for users logged in with an Identity Provider (Facebook, Google, OIDC compatible). Try to use Cognito Identity Pools instead. GetSessionToken for MFA. From a user, or an AWS account root user. GetFederatedToken for temporary credentials for a federated user. GetCallerIdentity to get details about an IAM user or role used in the API call. DecodeAuthorizationMessage to decode an error message when an AWS API is denied. Assuming roles using STS \u00b6 Define an IAM role. Define the principals that can access the role. Use the AssumeRole API to get credentials and impersonate the IAM role. Credentials will be valid from 15mins to an hour. MFA using STS \u00b6 Use GetSessionToken to authenticate using an MFA code. Create an IAM policy with the appropriate IAM conditions. Add aws:MultiFactorAuthPresent:true to the IAM policy. GetSessionToken returns - Access ID Secret Key Secret Token Expiration Date","title":"STS"},{"location":"developing/sts/#sts","text":"","title":"STS"},{"location":"developing/sts/#overview","text":"Get temporary credentials to permit access to resources for up to 1hr. AssumeRole to assume a role within the account or cross account. AssumeRoleWithSAML to return credentials for users logged in using SAML tokens. AssumeRoleWithIdentity to return credentials for users logged in with an Identity Provider (Facebook, Google, OIDC compatible). Try to use Cognito Identity Pools instead. GetSessionToken for MFA. From a user, or an AWS account root user. GetFederatedToken for temporary credentials for a federated user. GetCallerIdentity to get details about an IAM user or role used in the API call. DecodeAuthorizationMessage to decode an error message when an AWS API is denied.","title":"Overview"},{"location":"developing/sts/#assuming-roles-using-sts","text":"Define an IAM role. Define the principals that can access the role. Use the AssumeRole API to get credentials and impersonate the IAM role. Credentials will be valid from 15mins to an hour.","title":"Assuming roles using STS"},{"location":"developing/sts/#mfa-using-sts","text":"Use GetSessionToken to authenticate using an MFA code. Create an IAM policy with the appropriate IAM conditions. Add aws:MultiFactorAuthPresent:true to the IAM policy. GetSessionToken returns - Access ID Secret Key Secret Token Expiration Date","title":"MFA using STS"},{"location":"developing/xray/","text":"X-Ray \u00b6 Overview \u00b6 Visual analysis of applications, particularly useful for micro service architecture. Helps to troubleshoot application performance, dependencies, pinpoint service issues, review request behaviour, find errors/exceptions, check SLAs and where the throttling is happening. Helps identify the users impacted by errors. Compatible with Lambda, Elastic Beanstalk, ECS, ELB, API Gateway, EC2 instances, application servers (including on-prem). Leverages tracing to follow a request (business transaction). Each component adds it's own \"trace\". Tracing is composed of segments. Annotations can be added to traces to provide extra information. Can control trace interval (every request, rate per minute, % of requests etc). IAM for authorization, KMS for encryption at rest. Segments \u00b6 Send by the application/service. Use sub-segments if more details are required in the segment. Calls made between different application components. Trace \u00b6 Collection of segment documents that represents an end to end trace (business transaction). X-Ray daemon/agent needs extra configuration to send traces cross account: IAM permissions need to be correct, because the agent will assume a role. Allows a single account to be used for application tracing. Sampling \u00b6 How frequently requests are sent to X-ray. Reduce the sampling rate to reduce cost. Sampling Rules \u00b6 Doesn't require code changes. By default, the first request each second (reservoir), and 5% of subsequent requests (rate) are recorded. The reservoir and rate can be modified using custom sampling rules, to control the amount of data being recorded. Lower priority value has precedence (ie: priority 1 has precedence over another custom sampling rule with priority 5000). Annotations \u00b6 Key-value pairs that can be used to index traces, and use with filters. Metadata \u00b6 Key-value pairs that are not indexed and can't be used for searching. Enabling AWS X-Ray \u00b6 Application code needs to import the AWS X-Ray SDK. Must be Java, Python, Go, Node.JS or .NET. AWS Application SDK will capture: Calls to AWS services. HTTP/HTTPS requests. Database calls (MySQL, PostgreSQL, DynamoDB). Queue calls (SQS). Need to install the X-Ray daemon (low level UDP packet interceptor), or enable X-Ray AWS integration (Lambda etc). Application needs IAM rights to write data to X-Ray. To enable on Elastic Beanstalk, use .ebextensions/xray-daemon.config . X-Ray Troubleshooting \u00b6 Check the EC2 IAM role has proper permissions. Check the EC2 instance is running the X-Ray daemon. Ensure Lambda has an IAM execution role with the proper policy ( AWSX-RayWriteOnlyAccess ). Ensure X-ray is imported into the application code. X-Ray Instrumentation \u00b6 var app = express(); var AWSXRay = require('aws-xray-sdk'); app.use(AWSXRay.express.openSegment('MyApp')); app.get('/', function (req, res) { res.render('index'); }); app.use(AWSXRay.express.closeSegment()); Use the X-Ray SDK. Can add annotations to the data sent to X-ray, use interceptors, filters, handlers, middleware etc. X-Ray APIs \u00b6 X-Ray Write API \u00b6 Used by the X-Ray daemon to send data to AWS X-Ray. Needs the right IAM policy that permits writing to X-Ray. PutTraceSegments : Upload segments to the X-Ray service. PutTelemetryRecords : Used by the X-Ray daemon to send telemetry - segments received/rejected count, connection errors etc. GetSamplingRules : Retrieve all sampling rules (to know what/when to send). GetSamplingTargets : Also related to sampling rules. X-Ray Read API \u00b6 GetSamplingRules : Retrieve all sample rules (to know what/when to send). GetSamplingTargets : Also related to sampling rules. GetSamplingStatisticSummaries : Also related to sampling rules. BatchGetTraces : Retrieve a list of traces specified by ID. Contains full trace information. GetServiceGraph : Get the main graph visible in the console. GetTraceGraph : Get the service graph for one or more specific trace IDs. GetTraceSummaries : Retrieve the ID and annotations for traces available within a specified time. Can be filtered. GetGroups : ??? GetGroup : ??? GetTimeSeriesServiceStatistics : ??? X-Ray with Elastic Beanstalk \u00b6 Enable the X-Ray daemon via the Elastic Beanstalk console, or Enable the X-Ray daemon via .ebextensions/xray-daemon.config : option_settings: aws:elasticbeanstalk:xray:XRayEnabled: true Assign the right IAM policy to the instance id to allow Elastic Beanstalk to send data to AWS X-Ray. Instrument your code to collect segments, create traces etc. X-Ray with ECS \u00b6 { \"name\": \"xray-daemon\", \"image\": \"123456789012.dkr.ecr.us-east-2.amazonaws.com/xray-daemon\", \"cpu\": 32, \"memoryReservation\": 256, \"portMappings\": [ { \"hostPort\": 0, \"containerPort\": 2000, \"protocol\": \"udp\" } ], }, { \"name\": \"scorekeep-api\", \"image\": \"123456789012.dkr.ecr.us-east-2.amazonaws.com/scorekeep-api\", \"cpu\": 192, \"memoryReservation\": 512, \"environment\": [ { \"name\": \"AWS_REGION\", \"value\": \"us-east-2\" }, { \"name\": \"NOTIFICATION_TOPIC\", \"value\": \"arn:aws:sns:us-east-2:123456789012:scorekeep-notifications\" }, { \"name\": \"AWS_XRAY_DAEMON_ADDRESS\", \"value\": \"xray-daemon:2000\" } ], \"portMappings\": [ { \"hostPort\": 5000, \"containerPort\": 5000 } ], \"links\": [ \"xray-daemon\" ] } ECS Cluster \u00b6 Run the X-Ray Container as a Daemon on each EC2 instance, OR Run the X-Ray Container using the side-car pattern. Fargate Cluster \u00b6 Run the X-Ray Container using the side-car pattern.","title":"X-Ray"},{"location":"developing/xray/#x-ray","text":"","title":"X-Ray"},{"location":"developing/xray/#overview","text":"Visual analysis of applications, particularly useful for micro service architecture. Helps to troubleshoot application performance, dependencies, pinpoint service issues, review request behaviour, find errors/exceptions, check SLAs and where the throttling is happening. Helps identify the users impacted by errors. Compatible with Lambda, Elastic Beanstalk, ECS, ELB, API Gateway, EC2 instances, application servers (including on-prem). Leverages tracing to follow a request (business transaction). Each component adds it's own \"trace\". Tracing is composed of segments. Annotations can be added to traces to provide extra information. Can control trace interval (every request, rate per minute, % of requests etc). IAM for authorization, KMS for encryption at rest.","title":"Overview"},{"location":"developing/xray/#segments","text":"Send by the application/service. Use sub-segments if more details are required in the segment. Calls made between different application components.","title":"Segments"},{"location":"developing/xray/#trace","text":"Collection of segment documents that represents an end to end trace (business transaction). X-Ray daemon/agent needs extra configuration to send traces cross account: IAM permissions need to be correct, because the agent will assume a role. Allows a single account to be used for application tracing.","title":"Trace"},{"location":"developing/xray/#sampling","text":"How frequently requests are sent to X-ray. Reduce the sampling rate to reduce cost.","title":"Sampling"},{"location":"developing/xray/#sampling-rules","text":"Doesn't require code changes. By default, the first request each second (reservoir), and 5% of subsequent requests (rate) are recorded. The reservoir and rate can be modified using custom sampling rules, to control the amount of data being recorded. Lower priority value has precedence (ie: priority 1 has precedence over another custom sampling rule with priority 5000).","title":"Sampling Rules"},{"location":"developing/xray/#annotations","text":"Key-value pairs that can be used to index traces, and use with filters.","title":"Annotations"},{"location":"developing/xray/#metadata","text":"Key-value pairs that are not indexed and can't be used for searching.","title":"Metadata"},{"location":"developing/xray/#enabling-aws-x-ray","text":"Application code needs to import the AWS X-Ray SDK. Must be Java, Python, Go, Node.JS or .NET. AWS Application SDK will capture: Calls to AWS services. HTTP/HTTPS requests. Database calls (MySQL, PostgreSQL, DynamoDB). Queue calls (SQS). Need to install the X-Ray daemon (low level UDP packet interceptor), or enable X-Ray AWS integration (Lambda etc). Application needs IAM rights to write data to X-Ray. To enable on Elastic Beanstalk, use .ebextensions/xray-daemon.config .","title":"Enabling AWS X-Ray"},{"location":"developing/xray/#x-ray-troubleshooting","text":"Check the EC2 IAM role has proper permissions. Check the EC2 instance is running the X-Ray daemon. Ensure Lambda has an IAM execution role with the proper policy ( AWSX-RayWriteOnlyAccess ). Ensure X-ray is imported into the application code.","title":"X-Ray Troubleshooting"},{"location":"developing/xray/#x-ray-instrumentation","text":"var app = express(); var AWSXRay = require('aws-xray-sdk'); app.use(AWSXRay.express.openSegment('MyApp')); app.get('/', function (req, res) { res.render('index'); }); app.use(AWSXRay.express.closeSegment()); Use the X-Ray SDK. Can add annotations to the data sent to X-ray, use interceptors, filters, handlers, middleware etc.","title":"X-Ray Instrumentation"},{"location":"developing/xray/#x-ray-apis","text":"","title":"X-Ray APIs"},{"location":"developing/xray/#x-ray-write-api","text":"Used by the X-Ray daemon to send data to AWS X-Ray. Needs the right IAM policy that permits writing to X-Ray. PutTraceSegments : Upload segments to the X-Ray service. PutTelemetryRecords : Used by the X-Ray daemon to send telemetry - segments received/rejected count, connection errors etc. GetSamplingRules : Retrieve all sampling rules (to know what/when to send). GetSamplingTargets : Also related to sampling rules.","title":"X-Ray Write API"},{"location":"developing/xray/#x-ray-read-api","text":"GetSamplingRules : Retrieve all sample rules (to know what/when to send). GetSamplingTargets : Also related to sampling rules. GetSamplingStatisticSummaries : Also related to sampling rules. BatchGetTraces : Retrieve a list of traces specified by ID. Contains full trace information. GetServiceGraph : Get the main graph visible in the console. GetTraceGraph : Get the service graph for one or more specific trace IDs. GetTraceSummaries : Retrieve the ID and annotations for traces available within a specified time. Can be filtered. GetGroups : ??? GetGroup : ??? GetTimeSeriesServiceStatistics : ???","title":"X-Ray Read API"},{"location":"developing/xray/#x-ray-with-elastic-beanstalk","text":"Enable the X-Ray daemon via the Elastic Beanstalk console, or Enable the X-Ray daemon via .ebextensions/xray-daemon.config : option_settings: aws:elasticbeanstalk:xray:XRayEnabled: true Assign the right IAM policy to the instance id to allow Elastic Beanstalk to send data to AWS X-Ray. Instrument your code to collect segments, create traces etc.","title":"X-Ray with Elastic Beanstalk"},{"location":"developing/xray/#x-ray-with-ecs","text":"{ \"name\": \"xray-daemon\", \"image\": \"123456789012.dkr.ecr.us-east-2.amazonaws.com/xray-daemon\", \"cpu\": 32, \"memoryReservation\": 256, \"portMappings\": [ { \"hostPort\": 0, \"containerPort\": 2000, \"protocol\": \"udp\" } ], }, { \"name\": \"scorekeep-api\", \"image\": \"123456789012.dkr.ecr.us-east-2.amazonaws.com/scorekeep-api\", \"cpu\": 192, \"memoryReservation\": 512, \"environment\": [ { \"name\": \"AWS_REGION\", \"value\": \"us-east-2\" }, { \"name\": \"NOTIFICATION_TOPIC\", \"value\": \"arn:aws:sns:us-east-2:123456789012:scorekeep-notifications\" }, { \"name\": \"AWS_XRAY_DAEMON_ADDRESS\", \"value\": \"xray-daemon:2000\" } ], \"portMappings\": [ { \"hostPort\": 5000, \"containerPort\": 5000 } ], \"links\": [ \"xray-daemon\" ] }","title":"X-Ray with ECS"},{"location":"developing/xray/#ecs-cluster","text":"Run the X-Ray Container as a Daemon on each EC2 instance, OR Run the X-Ray Container using the side-car pattern.","title":"ECS Cluster"},{"location":"developing/xray/#fargate-cluster","text":"Run the X-Ray Container using the side-car pattern.","title":"Fargate Cluster"},{"location":"fundamentals/acm/","text":"AWS Certificate Manager (ACM) \u00b6 Overview \u00b6 Service used to host public SSL certificates in AWS. Private certificates can also be managed, and you can upload existing certificates via the cli. Note Certificates provisioned via ACM are free. Integration \u00b6 ACM certificates can be loaded onto Load Balancers (including ones created by Elastic Beanstalk). Cloudfront distributions. APIs on API gateways.","title":"AWS Certificate Manager (ACM)"},{"location":"fundamentals/acm/#aws-certificate-manager-acm","text":"","title":"AWS Certificate Manager (ACM)"},{"location":"fundamentals/acm/#overview","text":"Service used to host public SSL certificates in AWS. Private certificates can also be managed, and you can upload existing certificates via the cli. Note Certificates provisioned via ACM are free.","title":"Overview"},{"location":"fundamentals/acm/#integration","text":"ACM certificates can be loaded onto Load Balancers (including ones created by Elastic Beanstalk). Cloudfront distributions. APIs on API gateways.","title":"Integration"},{"location":"fundamentals/asg/","text":"Auto-Scaling Groups \u00b6 Overview \u00b6 ASGs allow EC2 instances to be scaled out/in to match load. New instances will be automatically registered with a load balancer and inherit the IAM roles that are attached to the ASG. A min/max EC2 instance count can be set to control costs/capacity. Use launch templates to manage ASGs. Avoid legacy launch configurations. An ASG can only be updated by providing a new launch configuration/template. When EC2 instances are terminated, the ASG will automatically create new instances to maintain capacity. EC2 Instances can be configured to terminate when a load balancer marks them as unhealthy. ASGs are free. Auto-Scaling Group Attributes \u00b6 Launch configuration. AMI + instance type. EC2 User data. EBS volumes. Security groups. SSH key pair. Min/Max size. Initial/desired capacity. Network/subnet info. Load balancer information. Scaling policies. Auto-Scaling Alarms \u00b6 ASGs can be scaled based on CloudWatch alarms, where the alarm can be anything - average CPU etc. Auto-scaling Rules \u00b6 Auto-scaling can be triggered according to various rules - Target average CPU usage. Number of requests on the ELB per instance. Average network in or out. Custom metrics send to CloudWatch from the application (connected user count etc). Schedule (usage patterns). Configuration \u00b6 The ASG can use the launch template that's assigned to it, or you can specify the mix of instance types and purchase options. For example, keep a percentage of reserved instances, and auto-scale with On-Demand/Spot instances during peaks. Health Checks \u00b6 There's two types of health checks available - EC2 - Automatically terminate any unhealthy instances and provision replacements. ELB - Automatically terminate any instances flagged as unhealthy by a load balancer, and provision replacements. Scaling \u00b6 Scaling Policies \u00b6 Target Tracking Scaling \u00b6 Simplest and easiest to setup. Example I want the average ASG CPU to stay at around 40% Simple/Step Scaling \u00b6 Based on CloudWatch alarms. Example When a CloudWatch alarm is triggered, scale out (ex: CPU > 70%). Scheduled Actions \u00b6 Anticipate scaling based on usage patterns. Example Increase min capacity at 5pm on Fridays. Scaling Cool-down \u00b6 Scaling cooldown helps ensure that the ASG doesn't launch/terminate instances before the next scaling event takes place. They're usually used to manage scale in policies, which terminate instances based on a condition. This gives EC2 auto-scaling more time to determine if it needs to terminated additional instances. You can create cool-down periods that apply to a specific simple scaling policy. A scaling specific cool-down overrides the default cooldown period. Scaling cool-down can be used to scale in faster to help reduce costs. Modify cool-down timers and the CloudWatch alarm period if the application is scaling in and out too many times each hour. The default cool-down period is 300 seconds. You can disable scale-in in the scaling policy, to make it scale-out only.","title":"Auto-Scaling Groups"},{"location":"fundamentals/asg/#auto-scaling-groups","text":"","title":"Auto-Scaling Groups"},{"location":"fundamentals/asg/#overview","text":"ASGs allow EC2 instances to be scaled out/in to match load. New instances will be automatically registered with a load balancer and inherit the IAM roles that are attached to the ASG. A min/max EC2 instance count can be set to control costs/capacity. Use launch templates to manage ASGs. Avoid legacy launch configurations. An ASG can only be updated by providing a new launch configuration/template. When EC2 instances are terminated, the ASG will automatically create new instances to maintain capacity. EC2 Instances can be configured to terminate when a load balancer marks them as unhealthy. ASGs are free.","title":"Overview"},{"location":"fundamentals/asg/#auto-scaling-group-attributes","text":"Launch configuration. AMI + instance type. EC2 User data. EBS volumes. Security groups. SSH key pair. Min/Max size. Initial/desired capacity. Network/subnet info. Load balancer information. Scaling policies.","title":"Auto-Scaling Group Attributes"},{"location":"fundamentals/asg/#auto-scaling-alarms","text":"ASGs can be scaled based on CloudWatch alarms, where the alarm can be anything - average CPU etc.","title":"Auto-Scaling Alarms"},{"location":"fundamentals/asg/#auto-scaling-rules","text":"Auto-scaling can be triggered according to various rules - Target average CPU usage. Number of requests on the ELB per instance. Average network in or out. Custom metrics send to CloudWatch from the application (connected user count etc). Schedule (usage patterns).","title":"Auto-scaling Rules"},{"location":"fundamentals/asg/#configuration","text":"The ASG can use the launch template that's assigned to it, or you can specify the mix of instance types and purchase options. For example, keep a percentage of reserved instances, and auto-scale with On-Demand/Spot instances during peaks.","title":"Configuration"},{"location":"fundamentals/asg/#health-checks","text":"There's two types of health checks available - EC2 - Automatically terminate any unhealthy instances and provision replacements. ELB - Automatically terminate any instances flagged as unhealthy by a load balancer, and provision replacements.","title":"Health Checks"},{"location":"fundamentals/asg/#scaling","text":"","title":"Scaling"},{"location":"fundamentals/asg/#scaling-policies","text":"","title":"Scaling Policies"},{"location":"fundamentals/asg/#scaling-cool-down","text":"Scaling cooldown helps ensure that the ASG doesn't launch/terminate instances before the next scaling event takes place. They're usually used to manage scale in policies, which terminate instances based on a condition. This gives EC2 auto-scaling more time to determine if it needs to terminated additional instances. You can create cool-down periods that apply to a specific simple scaling policy. A scaling specific cool-down overrides the default cooldown period. Scaling cool-down can be used to scale in faster to help reduce costs. Modify cool-down timers and the CloudWatch alarm period if the application is scaling in and out too many times each hour. The default cool-down period is 300 seconds. You can disable scale-in in the scaling policy, to make it scale-out only.","title":"Scaling Cool-down"},{"location":"fundamentals/aurora/","text":"Aurora \u00b6 What is Aurora? \u00b6 Compatible with Postgres & MySQL. Proprietary AWS technology. Cloud optimized. 5x performance improvement over MySQL on RDS, and 3x Postgres on RDS. Uses auto-scaling storage. 10GB, upto 64TB. Supports up to 15 read rpelicates (sub 10ms replica lag). Instantaneous failover. HA by default. 20% more than RDS. Only one Aurora instance (master) takes writes. High Availability \u00b6 Uses shared storage volume, to store 6 copies of data across 3 AZs. Only needs 4 copies out of 6 for writes, and 3 out of 6 for reads. Self healing with peer-to-peer replication to remediate bad data. Storage striped across 100s of volumes. Automated failover for master in less than 30seconds. Supports cross region replication. Read Scaling \u00b6 Read replicas support auto-scaling. Aurora DB Cluster \u00b6 DNS name (writer endpoint) always pointing to master to write data. Reader endpoint manages connection load balancing to connect client to one of the read replicas. Load balancing is done at the connection level. Features \u00b6 Automatic failover. Backup & recovery. Isoltion & security. Industry compliance. Push button scaling. Automated patching with no downtime. Advanced monitoring. Routine maintenance. Backtrack. Security \u00b6 Encryption at rest via KMS. Automated backups, snapshots and replicas are encrypted. Encryption in transit via SSL. Can authenticate using IAM tokens. User responsible for configuring security groups. Can't SSH into instance. Aurora Serverless \u00b6 Automated db instantiation and auto-scaling based on actual usage. Good for infrequent, intermittent or unpredictable workloads. Removes the need for capacity planning. Pay per second, can be more cost effective. Clients connect to a proxy fleet, that proxy the comms to the Aurora instances. Global Aurora \u00b6 Cross-region read replicas. Good for DR. Easy to setup. Aurora Global Database 1 primary region for read/write. Up to 5 secondary read only regions. Replication lag < 1second. Up to 16 read replicas per secondary region. Decreases latency. Promoting to another region due to a DR event takes < 1min.","title":"Aurora"},{"location":"fundamentals/aurora/#aurora","text":"","title":"Aurora"},{"location":"fundamentals/aurora/#what-is-aurora","text":"Compatible with Postgres & MySQL. Proprietary AWS technology. Cloud optimized. 5x performance improvement over MySQL on RDS, and 3x Postgres on RDS. Uses auto-scaling storage. 10GB, upto 64TB. Supports up to 15 read rpelicates (sub 10ms replica lag). Instantaneous failover. HA by default. 20% more than RDS. Only one Aurora instance (master) takes writes.","title":"What is Aurora?"},{"location":"fundamentals/aurora/#high-availability","text":"Uses shared storage volume, to store 6 copies of data across 3 AZs. Only needs 4 copies out of 6 for writes, and 3 out of 6 for reads. Self healing with peer-to-peer replication to remediate bad data. Storage striped across 100s of volumes. Automated failover for master in less than 30seconds. Supports cross region replication.","title":"High Availability"},{"location":"fundamentals/aurora/#read-scaling","text":"Read replicas support auto-scaling.","title":"Read Scaling"},{"location":"fundamentals/aurora/#aurora-db-cluster","text":"DNS name (writer endpoint) always pointing to master to write data. Reader endpoint manages connection load balancing to connect client to one of the read replicas. Load balancing is done at the connection level.","title":"Aurora DB Cluster"},{"location":"fundamentals/aurora/#features","text":"Automatic failover. Backup & recovery. Isoltion & security. Industry compliance. Push button scaling. Automated patching with no downtime. Advanced monitoring. Routine maintenance. Backtrack.","title":"Features"},{"location":"fundamentals/aurora/#security","text":"Encryption at rest via KMS. Automated backups, snapshots and replicas are encrypted. Encryption in transit via SSL. Can authenticate using IAM tokens. User responsible for configuring security groups. Can't SSH into instance.","title":"Security"},{"location":"fundamentals/aurora/#aurora-serverless","text":"Automated db instantiation and auto-scaling based on actual usage. Good for infrequent, intermittent or unpredictable workloads. Removes the need for capacity planning. Pay per second, can be more cost effective. Clients connect to a proxy fleet, that proxy the comms to the Aurora instances.","title":"Aurora Serverless"},{"location":"fundamentals/aurora/#global-aurora","text":"Cross-region read replicas. Good for DR. Easy to setup. Aurora Global Database 1 primary region for read/write. Up to 5 secondary read only regions. Replication lag < 1second. Up to 16 read replicas per secondary region. Decreases latency. Promoting to another region due to a DR event takes < 1min.","title":"Global Aurora"},{"location":"fundamentals/databases/","text":"Databases \u00b6 RDS \u00b6 Relational databases, that are good for online transaction processing (OLTP). Provisioned database that scales vertically, not horizontally. PostgreSQL, MySQL, Oracle, Aurora and Aurora Serverless. DynamoDB \u00b6 NoSQL managed database. Key/value store to keep documents upto 400KB. Serverless. Can be provisioned with RCU/WCU and auto-scaling, or use On-Demand. ElastiCache \u00b6 In memory database. Redis or Memcached. Caches data coming from other databases, session data or caching queries. Provisioned the same as RDS databases. Redshift (OLAP) \u00b6 Online analytical processing. Data warehousing/data lakes. Analytics queries. Provisioned in advance. Neptune \u00b6 Graph database. DMS \u00b6 Database migration service to move data from one database to another. DocumentDB \u00b6 Managed MongoDB.","title":"Databases"},{"location":"fundamentals/databases/#databases","text":"","title":"Databases"},{"location":"fundamentals/databases/#rds","text":"Relational databases, that are good for online transaction processing (OLTP). Provisioned database that scales vertically, not horizontally. PostgreSQL, MySQL, Oracle, Aurora and Aurora Serverless.","title":"RDS"},{"location":"fundamentals/databases/#dynamodb","text":"NoSQL managed database. Key/value store to keep documents upto 400KB. Serverless. Can be provisioned with RCU/WCU and auto-scaling, or use On-Demand.","title":"DynamoDB"},{"location":"fundamentals/databases/#elasticache","text":"In memory database. Redis or Memcached. Caches data coming from other databases, session data or caching queries. Provisioned the same as RDS databases.","title":"ElastiCache"},{"location":"fundamentals/databases/#redshift-olap","text":"Online analytical processing. Data warehousing/data lakes. Analytics queries. Provisioned in advance.","title":"Redshift (OLAP)"},{"location":"fundamentals/databases/#neptune","text":"Graph database.","title":"Neptune"},{"location":"fundamentals/databases/#dms","text":"Database migration service to move data from one database to another.","title":"DMS"},{"location":"fundamentals/databases/#documentdb","text":"Managed MongoDB.","title":"DocumentDB"},{"location":"fundamentals/ebs/","text":"EBS \u00b6 What's an EBS Volume? \u00b6 EBS is a network attached volume to persist data. Can be de-ttached and re-attached to another EC2 instance in the same AZ. Bound to an AZ. Capacity must be provisioned in advance (IOPS, and volume size). Billed for all provisioned capacity, even if it's not used. Capacity can be increased as needed. Only GP2 and IO1 can be used as boot volumes. Enable \"Delete on Termination\" to delete the EBS volume when the EC2 instance is terminated. By default it's enabled on the root volume. EBS Attributes \u00b6 Size Throughput IOPS EBS Volume Types \u00b6 Volume Type Optimised for Boot Volume Uses Size IOPS GP2 (SSD) Balance price vs performance. Yes General purpose. Low latency interactive apps Virtual desktops Dev/Test environments Recommended for most workloads. 1GB-16TB 3 IOPS/GB Minimum 100 IOPS with burst to 3000. Maximum 16,000 IOPS. IOPS scales with volume size. GP3 (SSD) Balance price vs performance. Yes General purpose. Low latency interactive apps Virtual desktops Dev/Test environments Recommended for most workloads. 1GB-16TB Minimum 3,000 IOPS and 125MB/s. Maximum 16,000 IOPS and/or 1000MB/s. IO1/IO2 (SSD) High performance SSD Yes Large workloads & critical business applications - DB servers etc. Highest performance SSD. Expensive. IO2 should be preferred over IO1 because it gets more IOPS/GB, and more durability. 4GB-16TB 50IOPS/GB Minimum 100 IOPS, upto 32,000. Or 64,00 with Nitro instance. Supports EBS multi-attach. IO2 Block Express (SSD) High performance SSD Yes Large workloads & critical business applications - DB servers etc. Highest performance SSD. Expensive. Sub millisecond latency. Supports EBS multi-attach. 4GB-64TB 1,000IOPS/GB, upto 256,000 IOPS ST1 (HDD) Frequently accessed, throughput intensive workloads. No Kafka Data warehouses Log processing Low cost. 125MB-16TB Maximum 500 IOPS Maximum 500MB/sec through-put. SC1 (HDD) Infrequently accessed data No Cheapest. Good for cold backups, snapshots. 125MB-16TB Maximum 250MB/sec throughput with 250 IOPS. EBS Snapshots \u00b6 Can be copied across regions/AZs. Don't necessarily have to detach the volume to take a snapshot. Can create a new volume from a snapshot. EBS Multi Attach \u00b6 Attach same EBS volume to multiple EC2 instances in the same AZ. Good for clustered applications (eg: teradata). Must use a cluster aware file system. Only for IO1 or IO2.","title":"EBS"},{"location":"fundamentals/ebs/#ebs","text":"","title":"EBS"},{"location":"fundamentals/ebs/#whats-an-ebs-volume","text":"EBS is a network attached volume to persist data. Can be de-ttached and re-attached to another EC2 instance in the same AZ. Bound to an AZ. Capacity must be provisioned in advance (IOPS, and volume size). Billed for all provisioned capacity, even if it's not used. Capacity can be increased as needed. Only GP2 and IO1 can be used as boot volumes. Enable \"Delete on Termination\" to delete the EBS volume when the EC2 instance is terminated. By default it's enabled on the root volume.","title":"What's an EBS Volume?"},{"location":"fundamentals/ebs/#ebs-attributes","text":"Size Throughput IOPS","title":"EBS Attributes"},{"location":"fundamentals/ebs/#ebs-volume-types","text":"Volume Type Optimised for Boot Volume Uses Size IOPS GP2 (SSD) Balance price vs performance. Yes General purpose. Low latency interactive apps Virtual desktops Dev/Test environments Recommended for most workloads. 1GB-16TB 3 IOPS/GB Minimum 100 IOPS with burst to 3000. Maximum 16,000 IOPS. IOPS scales with volume size. GP3 (SSD) Balance price vs performance. Yes General purpose. Low latency interactive apps Virtual desktops Dev/Test environments Recommended for most workloads. 1GB-16TB Minimum 3,000 IOPS and 125MB/s. Maximum 16,000 IOPS and/or 1000MB/s. IO1/IO2 (SSD) High performance SSD Yes Large workloads & critical business applications - DB servers etc. Highest performance SSD. Expensive. IO2 should be preferred over IO1 because it gets more IOPS/GB, and more durability. 4GB-16TB 50IOPS/GB Minimum 100 IOPS, upto 32,000. Or 64,00 with Nitro instance. Supports EBS multi-attach. IO2 Block Express (SSD) High performance SSD Yes Large workloads & critical business applications - DB servers etc. Highest performance SSD. Expensive. Sub millisecond latency. Supports EBS multi-attach. 4GB-64TB 1,000IOPS/GB, upto 256,000 IOPS ST1 (HDD) Frequently accessed, throughput intensive workloads. No Kafka Data warehouses Log processing Low cost. 125MB-16TB Maximum 500 IOPS Maximum 500MB/sec through-put. SC1 (HDD) Infrequently accessed data No Cheapest. Good for cold backups, snapshots. 125MB-16TB Maximum 250MB/sec throughput with 250 IOPS.","title":"EBS Volume Types"},{"location":"fundamentals/ebs/#ebs-snapshots","text":"Can be copied across regions/AZs. Don't necessarily have to detach the volume to take a snapshot. Can create a new volume from a snapshot.","title":"EBS Snapshots"},{"location":"fundamentals/ebs/#ebs-multi-attach","text":"Attach same EBS volume to multiple EC2 instances in the same AZ. Good for clustered applications (eg: teradata). Must use a cluster aware file system. Only for IO1 or IO2.","title":"EBS Multi Attach"},{"location":"fundamentals/ec2/","text":"EC2 \u00b6 What is EC2? \u00b6 Launch/manage VMs. Store data on EBS. ELB for load balancing. Manage auto-scaling groups (ASG). Availability Zones \u00b6 Physical data centres. Isolated power suppliers, cooling, power etc. Usually at least 3 per region. Regions \u00b6 Geographical location with multiple availability zones. Amazon Machine Image (AMI) \u00b6 Base image. EC2 User Data can be used to customise the image. AMIs are bound to a specific region, but can be copied across regions. Why use a Custom AMI? \u00b6 Pre-install required packages. Faster boot time by avoiding EC2 User Data. Pre-configure monitoring/enterprise software. Pre-install applications for faster deployments. Support Active Directory out of the box. Security concerns (need more control of the image). Control the maintenance & updates of AMIs over time. Optimised for a specific purpose. Building AMIs \u00b6 Start an EC2 instance, customise, stop it, and create an AMI using the stopped instance. Instance Types \u00b6 Type Use Case General Purpose Balance between compute, memory & networking. Web servers, code repos etc. Compute Optimised Compute intensive workloads. Batch jobs, transcoding, machine learning, gaming servers etc. Memory Optimised Workloads processing large datasets. Databases, web cache stores, BI, realtime processing of unstructured data. Storage Optimised Workloads needing high IO. Databases, transaction processing, redis cache, distributed file systems, data warehousing. Security Groups \u00b6 Provide access to ports. Control authorised port ranges. Control inbound/outbound traffic. Can attach to multiple instances. EC2 instance can have multiple security groups. Bound to a specific Region/VPC combination. Reside outside of the EC2 instance so the instance never recieves blocked traffic. All inbound traffic is blocked by default. All outbound traffic is permitted by default. Security groups can reference another security group for defining inbound/outbound rules (SG1 is allowed to connect to EC2 instances assigned SG2, but SG3 isn't). Generally a good idea to keep a seperate SSH security group as it's commonly required. Contain ALLOW rules only. Timeout errors mean it's a security group problem. Connection refused means it's an application error/EC2 instance isn't launched. EC2 User Data \u00b6 Runs once, the first time an instance is started. Runs as root. Script is base64 encoded. EC Instance Connect \u00b6 Provides web-based SSH access to EC2 instances. Elastic Network Interfaces \u00b6 Virtual NIC. Has 1 primary private IPv4 address, and one or more secondary IPv4 addresses. Gets 1 Elastic IPv4 address per private IP. Can have 1 public IPv4 address. Can have multiple security groups. Is assigned a Mac address like a physical NIC. Bound to a specific Availability Zone. Can be created independently and moved between EC2 instances on the fly to support failover. Elastic IPs \u00b6 Assigned to an EC2 instance, so that it's IP doesn't change when the instance is stopped, then started. 1 public IPv4 is assigned when the instance is spun up. Elastic IP changes whenever the instance is stopped, then started. The private IP is reverved for the life of the instance. Maximum of 5 elastic IPs per account (but can be extended). Should be avoided as much as possible. Costs money when not in use. EC2 Instance Launch Types \u00b6 Use reserved instance type for base application, and On-Demand/Spot instance types to handle peaks. Zonal reserved instance is bound to an AZ and can be used for capacity reservations. Regional reserved instances don't provide capacity reservations. Five main attributes - RAM, CPU, Memory, IO, Network, GPU. The letter represents the launch types specialty. C = cpu optimised, R = memory optmised etc. M instance types are all-rounders. Medium performance. T2/T3 instance types are burstable, based on burst credits. Burst credits accumulate over time. Once burst credits are exhausted, performance will absolutely tank. T2 has unlimited burst credits, but it costs money if the burst credit limit is exceeded. Launch Type Description Good For Committment On Demmand Predictable pricing. Billed by the second, with a minimum of 60seconds. - No upfront payment. - Highest hourly cost of all the instance launch types. Short, un-interrupted workloads None Reserved Upto 75% cheaper than On-Demand instances. Pay upfront. Must specify the specific instance type to be reserved (SKU). Long workloads, or steady state applications (database etc). 1 - 3 years Convertible Reserved Upto 54% cheaper than On-Demand instances. Instance type can be changed. No upfront payment. Long workloads 1 - 3 years Scheduled Reserved A workload needs to be performed within a specific time window (9am-5pm Mon-Fri etc). 1 year Spot Spot instance is given to the higest bidder. If someone bids more, then you can lose the instance with zero notice. Four types - Load balancing (web services), Flexible (batch, ci/cd), Big data (any size, any AZ) and Defined duration (1-6hrs) 90% cheaper than On-Demand instances. Short workloads - batch jobs, data analysis, image processing, things that are resilient to failure. Dedicated Instance Runs on hardware that is dedicated to a specific account. Hardware may be shared with other EC2 instances within the same account. Can only move hardware after the EC2 instance is stopped. Pay upfront. Per instance billing. None Dedicated Hosts Reserve a physical server Per instance billing Provides visibility of the underlying sockets and hardware, such as physical cores. User has full control over the instance placement. Expensive Companies with strong compliance/regulatory requirements, or for bring-your-own-license models where the license is bound to the number of cores. 3 years Pricing \u00b6 Hourly prices are based on: Region Instance type On-Demand vs Spot vs Reserved vs Dedicated hosts. IS Lots more... Billed by the second, with a minimum of 60seconds. Don't pay for stopped instances. Additional cost for storage, data transfer, public/private IPs, load balancing etc. EC2 Instance Metadata \u00b6 Allows EC2 instances to learn about themselves without needing to use an IAM role. Accessed via HTTP GET http://169.254.169.254/[latest|<version>]/[dynamic|meta-data|user-data] . Can retrieve the IAM role name, but not the policy. EC2 Instance Store \u00b6 Instance stores are ephemeral storage. Physically attached to the machine. Provides better IO performance (up to millions on IOPS). Good for buffering/caching/scratch data/temporary content.. Data survives reboots. Instance store is lost on termination and can't be resized. Backups need to be operated by the user. Up to 7.5TB, striped them to reach 30TB. Viewed on the instance as block storage. Risk of data loss if hardware fails. Some instances don't come with root EBS volumes.","title":"EC2"},{"location":"fundamentals/ec2/#ec2","text":"","title":"EC2"},{"location":"fundamentals/ec2/#what-is-ec2","text":"Launch/manage VMs. Store data on EBS. ELB for load balancing. Manage auto-scaling groups (ASG).","title":"What is EC2?"},{"location":"fundamentals/ec2/#availability-zones","text":"Physical data centres. Isolated power suppliers, cooling, power etc. Usually at least 3 per region.","title":"Availability Zones"},{"location":"fundamentals/ec2/#regions","text":"Geographical location with multiple availability zones.","title":"Regions"},{"location":"fundamentals/ec2/#amazon-machine-image-ami","text":"Base image. EC2 User Data can be used to customise the image. AMIs are bound to a specific region, but can be copied across regions.","title":"Amazon Machine Image (AMI)"},{"location":"fundamentals/ec2/#why-use-a-custom-ami","text":"Pre-install required packages. Faster boot time by avoiding EC2 User Data. Pre-configure monitoring/enterprise software. Pre-install applications for faster deployments. Support Active Directory out of the box. Security concerns (need more control of the image). Control the maintenance & updates of AMIs over time. Optimised for a specific purpose.","title":"Why use a Custom AMI?"},{"location":"fundamentals/ec2/#building-amis","text":"Start an EC2 instance, customise, stop it, and create an AMI using the stopped instance.","title":"Building AMIs"},{"location":"fundamentals/ec2/#instance-types","text":"Type Use Case General Purpose Balance between compute, memory & networking. Web servers, code repos etc. Compute Optimised Compute intensive workloads. Batch jobs, transcoding, machine learning, gaming servers etc. Memory Optimised Workloads processing large datasets. Databases, web cache stores, BI, realtime processing of unstructured data. Storage Optimised Workloads needing high IO. Databases, transaction processing, redis cache, distributed file systems, data warehousing.","title":"Instance Types"},{"location":"fundamentals/ec2/#security-groups","text":"Provide access to ports. Control authorised port ranges. Control inbound/outbound traffic. Can attach to multiple instances. EC2 instance can have multiple security groups. Bound to a specific Region/VPC combination. Reside outside of the EC2 instance so the instance never recieves blocked traffic. All inbound traffic is blocked by default. All outbound traffic is permitted by default. Security groups can reference another security group for defining inbound/outbound rules (SG1 is allowed to connect to EC2 instances assigned SG2, but SG3 isn't). Generally a good idea to keep a seperate SSH security group as it's commonly required. Contain ALLOW rules only. Timeout errors mean it's a security group problem. Connection refused means it's an application error/EC2 instance isn't launched.","title":"Security Groups"},{"location":"fundamentals/ec2/#ec2-user-data","text":"Runs once, the first time an instance is started. Runs as root. Script is base64 encoded.","title":"EC2 User Data"},{"location":"fundamentals/ec2/#ec-instance-connect","text":"Provides web-based SSH access to EC2 instances.","title":"EC Instance Connect"},{"location":"fundamentals/ec2/#elastic-network-interfaces","text":"Virtual NIC. Has 1 primary private IPv4 address, and one or more secondary IPv4 addresses. Gets 1 Elastic IPv4 address per private IP. Can have 1 public IPv4 address. Can have multiple security groups. Is assigned a Mac address like a physical NIC. Bound to a specific Availability Zone. Can be created independently and moved between EC2 instances on the fly to support failover.","title":"Elastic Network Interfaces"},{"location":"fundamentals/ec2/#elastic-ips","text":"Assigned to an EC2 instance, so that it's IP doesn't change when the instance is stopped, then started. 1 public IPv4 is assigned when the instance is spun up. Elastic IP changes whenever the instance is stopped, then started. The private IP is reverved for the life of the instance. Maximum of 5 elastic IPs per account (but can be extended). Should be avoided as much as possible. Costs money when not in use.","title":"Elastic IPs"},{"location":"fundamentals/ec2/#ec2-instance-launch-types","text":"Use reserved instance type for base application, and On-Demand/Spot instance types to handle peaks. Zonal reserved instance is bound to an AZ and can be used for capacity reservations. Regional reserved instances don't provide capacity reservations. Five main attributes - RAM, CPU, Memory, IO, Network, GPU. The letter represents the launch types specialty. C = cpu optimised, R = memory optmised etc. M instance types are all-rounders. Medium performance. T2/T3 instance types are burstable, based on burst credits. Burst credits accumulate over time. Once burst credits are exhausted, performance will absolutely tank. T2 has unlimited burst credits, but it costs money if the burst credit limit is exceeded. Launch Type Description Good For Committment On Demmand Predictable pricing. Billed by the second, with a minimum of 60seconds. - No upfront payment. - Highest hourly cost of all the instance launch types. Short, un-interrupted workloads None Reserved Upto 75% cheaper than On-Demand instances. Pay upfront. Must specify the specific instance type to be reserved (SKU). Long workloads, or steady state applications (database etc). 1 - 3 years Convertible Reserved Upto 54% cheaper than On-Demand instances. Instance type can be changed. No upfront payment. Long workloads 1 - 3 years Scheduled Reserved A workload needs to be performed within a specific time window (9am-5pm Mon-Fri etc). 1 year Spot Spot instance is given to the higest bidder. If someone bids more, then you can lose the instance with zero notice. Four types - Load balancing (web services), Flexible (batch, ci/cd), Big data (any size, any AZ) and Defined duration (1-6hrs) 90% cheaper than On-Demand instances. Short workloads - batch jobs, data analysis, image processing, things that are resilient to failure. Dedicated Instance Runs on hardware that is dedicated to a specific account. Hardware may be shared with other EC2 instances within the same account. Can only move hardware after the EC2 instance is stopped. Pay upfront. Per instance billing. None Dedicated Hosts Reserve a physical server Per instance billing Provides visibility of the underlying sockets and hardware, such as physical cores. User has full control over the instance placement. Expensive Companies with strong compliance/regulatory requirements, or for bring-your-own-license models where the license is bound to the number of cores. 3 years","title":"EC2 Instance Launch Types"},{"location":"fundamentals/ec2/#pricing","text":"Hourly prices are based on: Region Instance type On-Demand vs Spot vs Reserved vs Dedicated hosts. IS Lots more... Billed by the second, with a minimum of 60seconds. Don't pay for stopped instances. Additional cost for storage, data transfer, public/private IPs, load balancing etc.","title":"Pricing"},{"location":"fundamentals/ec2/#ec2-instance-metadata","text":"Allows EC2 instances to learn about themselves without needing to use an IAM role. Accessed via HTTP GET http://169.254.169.254/[latest|<version>]/[dynamic|meta-data|user-data] . Can retrieve the IAM role name, but not the policy.","title":"EC2 Instance Metadata"},{"location":"fundamentals/ec2/#ec2-instance-store","text":"Instance stores are ephemeral storage. Physically attached to the machine. Provides better IO performance (up to millions on IOPS). Good for buffering/caching/scratch data/temporary content.. Data survives reboots. Instance store is lost on termination and can't be resized. Backups need to be operated by the user. Up to 7.5TB, striped them to reach 30TB. Viewed on the instance as block storage. Risk of data loss if hardware fails. Some instances don't come with root EBS volumes.","title":"EC2 Instance Store"},{"location":"fundamentals/efs/","text":"# Elastic File System What is Elastic File System (EFS)? \u00b6 Managed NFS volumes that can be mounted on many EC2 instances, across many AZs. Very expensive. Pay based on what is used (automatic scaling). Highly available. Good for content management, web serving, data sharind, wordpress. Uses NFSv4.1. Uses security groups to control access. Only compatible with linux. Can use lifecycle management to remove files after a number of days. Throughput mode has two options Bursting Provisioned Performance mode has two options General purpose Max I/O Can have a public or private IP. Two modes Standard: Frequently accessed files. Infrequent Access (EFS-IA): cheaper to store files, but additional cost to retrieve files. Mounting EFS volumes \u00b6 Need to install the amazon-efs-utils package or nfs-utils. Mounting using efs helper, or nfs client. Make sure security group allows NFS port 2049. Cleaning up EFS \u00b6 Make sure volumes are removed after deleting EFS volume. Make sure security group(s) are cleaned up. EBS vs EFS \u00b6 EBS can only be attached to one instance at a time. EFS can attach to multiple instances. EBS is bound to a specific AZ, EFS is multi-AZ. EFS only for linux (NFS). EFS is more expensive than EBS. Can use EFS-IA to save money via lifecycle policy. EFS billed for what you use. EBS you pay for provisioned capacity. gp2 IO increases if the disk size increases. io1 can increase IO independantly of disk size. To migrate EBS volume across AZ, take a snapshot, and restore into the new AZ. Uses alot of IO. EBS will be terminated by default when the instance is terminated (can be turned off). Encryption \u00b6 Encryption at rest using KMS keys. Scaling \u00b6 Supports thousands of concurrent clients. Upto 10GB/sec throughput. Can grow to PB scale file systems. Performance mode is set at creation time. Performance Mode \u00b6 General Purpose (default) \u00b6 Use for latency sensitive use cases (web servers etc). Max I/O \u00b6 Higher latency. Use for highly parallel workloads (big data, media processing). Throughput Mode \u00b6 Bursting \u00b6 50MB/sec per TB, up to 100MB/sec bursts. Provisioned \u00b6 Define throughput seperately from storage size. Max 1GB/sec throughput. Storage Tiers \u00b6 Tier Use Case Standard For frequently accessed files. Infrequent Access (IA) Cost to retrieve files, lower price to store.","title":"Elastic File System"},{"location":"fundamentals/efs/#what-is-elastic-file-system-efs","text":"Managed NFS volumes that can be mounted on many EC2 instances, across many AZs. Very expensive. Pay based on what is used (automatic scaling). Highly available. Good for content management, web serving, data sharind, wordpress. Uses NFSv4.1. Uses security groups to control access. Only compatible with linux. Can use lifecycle management to remove files after a number of days. Throughput mode has two options Bursting Provisioned Performance mode has two options General purpose Max I/O Can have a public or private IP. Two modes Standard: Frequently accessed files. Infrequent Access (EFS-IA): cheaper to store files, but additional cost to retrieve files.","title":"What is Elastic File System (EFS)?"},{"location":"fundamentals/efs/#mounting-efs-volumes","text":"Need to install the amazon-efs-utils package or nfs-utils. Mounting using efs helper, or nfs client. Make sure security group allows NFS port 2049.","title":"Mounting EFS volumes"},{"location":"fundamentals/efs/#cleaning-up-efs","text":"Make sure volumes are removed after deleting EFS volume. Make sure security group(s) are cleaned up.","title":"Cleaning up EFS"},{"location":"fundamentals/efs/#ebs-vs-efs","text":"EBS can only be attached to one instance at a time. EFS can attach to multiple instances. EBS is bound to a specific AZ, EFS is multi-AZ. EFS only for linux (NFS). EFS is more expensive than EBS. Can use EFS-IA to save money via lifecycle policy. EFS billed for what you use. EBS you pay for provisioned capacity. gp2 IO increases if the disk size increases. io1 can increase IO independantly of disk size. To migrate EBS volume across AZ, take a snapshot, and restore into the new AZ. Uses alot of IO. EBS will be terminated by default when the instance is terminated (can be turned off).","title":"EBS vs EFS"},{"location":"fundamentals/efs/#encryption","text":"Encryption at rest using KMS keys.","title":"Encryption"},{"location":"fundamentals/efs/#scaling","text":"Supports thousands of concurrent clients. Upto 10GB/sec throughput. Can grow to PB scale file systems. Performance mode is set at creation time.","title":"Scaling"},{"location":"fundamentals/efs/#performance-mode","text":"","title":"Performance Mode"},{"location":"fundamentals/efs/#throughput-mode","text":"","title":"Throughput Mode"},{"location":"fundamentals/efs/#storage-tiers","text":"Tier Use Case Standard For frequently accessed files. Infrequent Access (IA) Cost to retrieve files, lower price to store.","title":"Storage Tiers"},{"location":"fundamentals/elasticache/","text":"ElastiCache \u00b6 What is Elasticache? \u00b6 Managed cache (Redis/Memcached) In-memory database with high performance, low latency. Reduces database load for read instensive workloads. Helps make applications stateless. Write scaling using sharding. Read scaling using read replicas. Mult AZ failover. Data needs to be well structured. Use Cases \u00b6 DB Cache \u00b6 Application queries elasticache, if no hit, then get the data from the RDS. Requires an invalidation strategy to ensure current/relevant data is cached. User Session Store \u00b6 User logs into the application, then writes session date into elasticache. The user hits another instance of the application, it gets the state from elasticache to avoid re-authenticating. Redis \u00b6 Multi AZ with auto failover. Read replicas to scale reads and have HA. Data durability using AOF persistence. Has backup and restore. Can be used as a database, cache or message broker. Need to create a Redis token for applications to authenticate. Support encryption at rest and during transit. Can specify a backup window. Cluster Modes \u00b6 Disabled \u00b6 Each shard has 1 primary node used for read/write, and up to 5 replicas. Asynchronous replication. Additional nodes are used for read only (read replicas). All nodes have all of the data if there's only 1 shard. This guards against data loss on node failure. Multi-AZ fail over enabled by default. Good for scaling read performance. Enabled \u00b6 Data partitioned across many shards (scales writes). Each shard has 1 primary node, and upto 5 replica nodes. Multi-AZ fail over enabled by default. Up to 500 nodes per cluster, 500 shards with 1 master. 250 shards with 1 master and 1 replica. 83 shards with 1 master and 5 replicas. Memcached \u00b6 Multi-node data partitioning (sharding). Cache is non persistent. No backup and restore. Multi-threaded architecture. Caching Strategies \u00b6 Lazy Loading/Cache Aside/Lazy Population \u00b6 Best for read optimistation. Check elasticcache first for data. If cache miss, query the RDS. Write data to cache. Pros \u00b6 Only requested data is cached Node failures are not fatal. Cons \u00b6 Cache miss penalty that results in 3 round trips. Data can be stale. Write Through \u00b6 Add or update cache when the database is updated. Best for write optimisation. Can be combined with lazy loading. On read, retrieve from cache. On write, write to RDS, then write to elasticache. Pros \u00b6 Data is never stale. Reads a quick. Write penalty instead of a read penalty (2 calls required). Cons \u00b6 Data is missing until the RDS is updated. High cache churn. Some data may never be read. Cache Evictions/TTL \u00b6 Three main ways that cache eviction can happen: Cache item is explicitly deleted. Memory is full and data is not recently used (LRU) Set a time to live (TTL). If there's too many evictions, then need to scale elasticache up/out. Avoid for write optimised cache.","title":"ElastiCache"},{"location":"fundamentals/elasticache/#elasticache","text":"","title":"ElastiCache"},{"location":"fundamentals/elasticache/#what-is-elasticache","text":"Managed cache (Redis/Memcached) In-memory database with high performance, low latency. Reduces database load for read instensive workloads. Helps make applications stateless. Write scaling using sharding. Read scaling using read replicas. Mult AZ failover. Data needs to be well structured.","title":"What is Elasticache?"},{"location":"fundamentals/elasticache/#use-cases","text":"","title":"Use Cases"},{"location":"fundamentals/elasticache/#db-cache","text":"Application queries elasticache, if no hit, then get the data from the RDS. Requires an invalidation strategy to ensure current/relevant data is cached.","title":"DB Cache"},{"location":"fundamentals/elasticache/#user-session-store","text":"User logs into the application, then writes session date into elasticache. The user hits another instance of the application, it gets the state from elasticache to avoid re-authenticating.","title":"User Session Store"},{"location":"fundamentals/elasticache/#redis","text":"Multi AZ with auto failover. Read replicas to scale reads and have HA. Data durability using AOF persistence. Has backup and restore. Can be used as a database, cache or message broker. Need to create a Redis token for applications to authenticate. Support encryption at rest and during transit. Can specify a backup window.","title":"Redis"},{"location":"fundamentals/elasticache/#cluster-modes","text":"","title":"Cluster Modes"},{"location":"fundamentals/elasticache/#memcached","text":"Multi-node data partitioning (sharding). Cache is non persistent. No backup and restore. Multi-threaded architecture.","title":"Memcached"},{"location":"fundamentals/elasticache/#caching-strategies","text":"","title":"Caching Strategies"},{"location":"fundamentals/elasticache/#lazy-loadingcache-asidelazy-population","text":"Best for read optimistation. Check elasticcache first for data. If cache miss, query the RDS. Write data to cache.","title":"Lazy Loading/Cache Aside/Lazy Population"},{"location":"fundamentals/elasticache/#write-through","text":"Add or update cache when the database is updated. Best for write optimisation. Can be combined with lazy loading. On read, retrieve from cache. On write, write to RDS, then write to elasticache.","title":"Write Through"},{"location":"fundamentals/elasticache/#cache-evictionsttl","text":"Three main ways that cache eviction can happen: Cache item is explicitly deleted. Memory is full and data is not recently used (LRU) Set a time to live (TTL). If there's too many evictions, then need to scale elasticache up/out. Avoid for write optimised cache.","title":"Cache Evictions/TTL"},{"location":"fundamentals/elb/","text":"Elastic Load Balancer \u00b6 What is EC2 Load Balancer (ELB)? \u00b6 Guaranteed to be working. AWS manages upgrades, maintenance, high availability. More expensive than setting up your own LB, but much less effort. Integrated with alot of AWS services. Use newer gen ELBs (more features). Can be external (public) or internal (private). Can auto-scale, but it's not instantaneous. Can contact AWS for a 'warm-up'. Forward traffic to multiple servers (EC2 instances). Why use a Load Balancer? \u00b6 Spread load. Expose a single endpoint (DNS) to the application. Seamlessly handle downstream failures. Perform regular health checks of instances. SSL termination. Manage sticky sessions. HA across AZs. Segregate public and private traffic. Scalability \u00b6 Handle greater workloads by adapting. Two types - vertical, and horizontal (elasticity). Vertical Scalability \u00b6 Increase the instance size (moar hardware!). Common for non-distributed systems (databases, elasticache etc). Limited scalability. Bound to the available hardware. Horizontal Scalability (Elasticity) \u00b6 Increase instance count (use ALL the infra!). Common for web apps, distributed applications. Use auto-scaling groups. High Availability \u00b6 Run application in multiple locations to support single site loss (multi AZ). Can be active or passive Auto scaling group (multi AZ) Load balancer (multi AZ) Passive \u00b6 Multiple locations with stand-by infrastructure. Active \u00b6 Multiple locations with all infrastructure in active use. Health Checks \u00b6 Inform LB if instance is healthy. A Classic Load Balancer (v1) will perform the health check against a route & port (foo:8080/healthcheck). Stickyness \u00b6 Same client is always redirected to the same instance behind a load balancer. Uses a cookie with an expiration date to track which instance the client should be routed to. Helps to preserve session data. May cause a load imbalance. ELB Types \u00b6 Classic Load Balancer (v1) \u00b6 Supports TCP (Layer 4) and HTTP & HTTPS (Layer 7). Health checks are TCP or HTTP based. Have a fixed hostname . .elb.amazonaws.com. One SSL certificate, doesn't support SNI. Application Load Balancer (v2) \u00b6 Supports HTTP, HTTPS and WebSockets (Layer 7). Load balance multiple HTTP applications across machines (target groups). Load balance multiple HTTP applications on the same machine (containers). Supports HTTP/2 and WebSocket. Supports redirects. Supporting routing tables based on target groups: Path in URL. Hostname in URL. Query string & headers. Good for micros services and container-based applications. Supports port mapping to redirect to dynamic ports (useful for ECS). Would require multiple CLBs to load balance multiple applications, vs 1 ALB. Supports SNI, using multiple listeners with multiple SSL certs. ALB Target Groups \u00b6 EC2 instances (managed via auto-scaling group) - HTTP. ECS tasks (managed by ECS) - HTTP. Lambda functions - HTTP request translated into a JSON event. IP Addresses - Must be private IPs. Route to multiple target groups. Health checks performed at the target group level. Fixed hostname. Application servers don't see the client IP. Client ip/port/protocol are available via the X-Forwarded-For and X-Forwarded-Proto/X-Forwarded-Port headers. Network Load Balancer (NLB) \u00b6 TCP/UDP/TLS traffic (Layer 4). Handle millions of TPS. Less latency (100ms vs 400ms of ALB). One static IP per AZ. Supports assigned Elastic IPs. Used for extreme performance, and TCP/UDP level traffic. Security group doesn't see the traffic as coming from the NLB, which impacts the \"source\" attribute of the security groups rule. Subnets can't be disabled after the NLB is created, but new ones can be added. This means Elastic IP has to be created before the NLB is created (if you want to use an Elastic IP). User SNI to support multiple listeners, with multiple SSL certificates. ELB Security Groups \u00b6 Allow HTTP from (public route) Allow HTTPS from (public route) Allow HTTP from (private route) Allow HTTPS from (private route) ELB Monitoring \u00b6 Access logs log all access requests. CloudWatch metrics provide aggregate stats (connections count etc). Troubleshooting ELB \u00b6 HTTP 503 means ELB is at capacity or there's no registered targets. If ELB can't connect to app, check security groups. ELB Connection Draining \u00b6 Called \"connection draining\" on CLB. Call \"deregistration delay\" on ALB and NLB. Is the time to complete in-flight requests while the instance is being de-registered or is unhealthy. Stops sending new requests to the instance which is de-registering. De-registration delay is 300sec by default. De-registration delay can be anything from 1 to 3600 seconds. Default de-registration delay is 300 seconds. De-registration delay can be set to 0 (disabled). Use low value if requests are short. Cross-Zone Load Balancing \u00b6 Each load balancer instance distributes evenly across all registered instances in all AZs. Disabled by default on CLB. No additional charges for inter AZ data. Enabled on ALB, can't be turned off. No charges for inter AZ data. Disabled by default for NLB. Additional charges for inter AZ data. SLB/TLS Certificates \u00b6 Load balancers use an X.509 certificate. You can use/manage certificates from AWS Certificate Manager (ACM). You can use certificates from IAM. Can also use your own certificates. When configuring the HTTPS listener: Must specify a default certificate. An optional list of certs can be addded to support multiple domains. Clients can use SNI (Server Name Indication) to specify the hostname they reach. Can apply a security policy to support older versions of SSL/TLS. Server Name Indication (SNI) \u00b6 Use multiple SSL certificates on the same web server. Client indicates which hostname of the target server it should connect to during the SSL handshake. Only available on ALB, NLB and Cloudfront. Doesn't work on CLB.","title":"Elastic Load Balancer"},{"location":"fundamentals/elb/#elastic-load-balancer","text":"","title":"Elastic Load Balancer"},{"location":"fundamentals/elb/#what-is-ec2-load-balancer-elb","text":"Guaranteed to be working. AWS manages upgrades, maintenance, high availability. More expensive than setting up your own LB, but much less effort. Integrated with alot of AWS services. Use newer gen ELBs (more features). Can be external (public) or internal (private). Can auto-scale, but it's not instantaneous. Can contact AWS for a 'warm-up'. Forward traffic to multiple servers (EC2 instances).","title":"What is EC2 Load Balancer (ELB)?"},{"location":"fundamentals/elb/#why-use-a-load-balancer","text":"Spread load. Expose a single endpoint (DNS) to the application. Seamlessly handle downstream failures. Perform regular health checks of instances. SSL termination. Manage sticky sessions. HA across AZs. Segregate public and private traffic.","title":"Why use a Load Balancer?"},{"location":"fundamentals/elb/#scalability","text":"Handle greater workloads by adapting. Two types - vertical, and horizontal (elasticity).","title":"Scalability"},{"location":"fundamentals/elb/#vertical-scalability","text":"Increase the instance size (moar hardware!). Common for non-distributed systems (databases, elasticache etc). Limited scalability. Bound to the available hardware.","title":"Vertical Scalability"},{"location":"fundamentals/elb/#horizontal-scalability-elasticity","text":"Increase instance count (use ALL the infra!). Common for web apps, distributed applications. Use auto-scaling groups.","title":"Horizontal Scalability (Elasticity)"},{"location":"fundamentals/elb/#high-availability","text":"Run application in multiple locations to support single site loss (multi AZ). Can be active or passive Auto scaling group (multi AZ) Load balancer (multi AZ)","title":"High Availability"},{"location":"fundamentals/elb/#passive","text":"Multiple locations with stand-by infrastructure.","title":"Passive"},{"location":"fundamentals/elb/#active","text":"Multiple locations with all infrastructure in active use.","title":"Active"},{"location":"fundamentals/elb/#health-checks","text":"Inform LB if instance is healthy. A Classic Load Balancer (v1) will perform the health check against a route & port (foo:8080/healthcheck).","title":"Health Checks"},{"location":"fundamentals/elb/#stickyness","text":"Same client is always redirected to the same instance behind a load balancer. Uses a cookie with an expiration date to track which instance the client should be routed to. Helps to preserve session data. May cause a load imbalance.","title":"Stickyness"},{"location":"fundamentals/elb/#elb-types","text":"","title":"ELB Types"},{"location":"fundamentals/elb/#classic-load-balancer-v1","text":"Supports TCP (Layer 4) and HTTP & HTTPS (Layer 7). Health checks are TCP or HTTP based. Have a fixed hostname . .elb.amazonaws.com. One SSL certificate, doesn't support SNI.","title":"Classic Load Balancer (v1)"},{"location":"fundamentals/elb/#application-load-balancer-v2","text":"Supports HTTP, HTTPS and WebSockets (Layer 7). Load balance multiple HTTP applications across machines (target groups). Load balance multiple HTTP applications on the same machine (containers). Supports HTTP/2 and WebSocket. Supports redirects. Supporting routing tables based on target groups: Path in URL. Hostname in URL. Query string & headers. Good for micros services and container-based applications. Supports port mapping to redirect to dynamic ports (useful for ECS). Would require multiple CLBs to load balance multiple applications, vs 1 ALB. Supports SNI, using multiple listeners with multiple SSL certs.","title":"Application Load Balancer (v2)"},{"location":"fundamentals/elb/#network-load-balancer-nlb","text":"TCP/UDP/TLS traffic (Layer 4). Handle millions of TPS. Less latency (100ms vs 400ms of ALB). One static IP per AZ. Supports assigned Elastic IPs. Used for extreme performance, and TCP/UDP level traffic. Security group doesn't see the traffic as coming from the NLB, which impacts the \"source\" attribute of the security groups rule. Subnets can't be disabled after the NLB is created, but new ones can be added. This means Elastic IP has to be created before the NLB is created (if you want to use an Elastic IP). User SNI to support multiple listeners, with multiple SSL certificates.","title":"Network Load Balancer (NLB)"},{"location":"fundamentals/elb/#elb-security-groups","text":"Allow HTTP from (public route) Allow HTTPS from (public route) Allow HTTP from (private route) Allow HTTPS from (private route)","title":"ELB Security Groups"},{"location":"fundamentals/elb/#elb-monitoring","text":"Access logs log all access requests. CloudWatch metrics provide aggregate stats (connections count etc).","title":"ELB Monitoring"},{"location":"fundamentals/elb/#troubleshooting-elb","text":"HTTP 503 means ELB is at capacity or there's no registered targets. If ELB can't connect to app, check security groups.","title":"Troubleshooting ELB"},{"location":"fundamentals/elb/#elb-connection-draining","text":"Called \"connection draining\" on CLB. Call \"deregistration delay\" on ALB and NLB. Is the time to complete in-flight requests while the instance is being de-registered or is unhealthy. Stops sending new requests to the instance which is de-registering. De-registration delay is 300sec by default. De-registration delay can be anything from 1 to 3600 seconds. Default de-registration delay is 300 seconds. De-registration delay can be set to 0 (disabled). Use low value if requests are short.","title":"ELB Connection Draining"},{"location":"fundamentals/elb/#cross-zone-load-balancing","text":"Each load balancer instance distributes evenly across all registered instances in all AZs. Disabled by default on CLB. No additional charges for inter AZ data. Enabled on ALB, can't be turned off. No charges for inter AZ data. Disabled by default for NLB. Additional charges for inter AZ data.","title":"Cross-Zone Load Balancing"},{"location":"fundamentals/elb/#slbtls-certificates","text":"Load balancers use an X.509 certificate. You can use/manage certificates from AWS Certificate Manager (ACM). You can use certificates from IAM. Can also use your own certificates. When configuring the HTTPS listener: Must specify a default certificate. An optional list of certs can be addded to support multiple domains. Clients can use SNI (Server Name Indication) to specify the hostname they reach. Can apply a security policy to support older versions of SSL/TLS.","title":"SLB/TLS Certificates"},{"location":"fundamentals/elb/#server-name-indication-sni","text":"Use multiple SSL certificates on the same web server. Client indicates which hostname of the target server it should connect to during the SSL handshake. Only available on ALB, NLB and Cloudfront. Doesn't work on CLB.","title":"Server Name Indication (SNI)"},{"location":"fundamentals/iam/","text":"Identity & Access Management \u00b6 Identity & Access Management (IAM) \u00b6 Consists of users (a person), groups (function/team) and roles (internal aws usage, assigned to machines). Policies defined in JSON. Global service, not bound to a region/AZ. Supports MFA. Has pre-defined policies. Should follow least privilege principle. Best Practise \u00b6 Delete the root access keys. Activate MFA. Create an IAM user. Create a group to allocate permissions. Never store AWS credentials in code (duh). Use the credentials chain Use IAM roles as much as possible within AWS. Use environment variables or a named profile when working outside of AWS. IAM Federation \u00b6 Integrates corporate user respositories via SAML (LDAP etc). IAM Roles \u00b6 Common roles are EC2 instance roles, Lambda Function roles and CloudFormation roles. IAM Policies \u00b6 AWS Managed policies are managed by AWS. Good for power users & administrators. They get updated when there's changes to AWS services, or new services made available. Customer Managed policies are re-usable against many principals. They're version controlled, support rollback, and have central change management. Inline policies apply to a single principal. No version control, no rollback, deleted if the IAM principal is deleted, and limited to 2KB. IAM policies apply to users, roles and groups. S3 policies apply to specific buckets. S3 bucket permissions are based on the union of the IAM policy(s) and S3 policy(s). Policy Types \u00b6 Type Description Identity-based Managed/inline policies that allow/deny permissions for a specific identity (user, group, role). Resource-based Inline policies attached to resources to allow/deny access. Permissions Boundaries Defines the maximum permissions for an identity. Doesn't grant permissions. Organisation Service Control Policy (SCP) Defines maximum permissions for account members of an organisation or OU. Only denys permissions. Access Control List (ACL) Grants permissions on resources in other accounts (cross-account). Session Limits permissions when assuming a role or federated user. . Policy Evaluation Order \u00b6 If there's an explicit DENY, deny access. If there's an explicit ALLOW, allow access. Otherwise, deny access. Dynamic Policies \u00b6 Policy variables can be used to insert dynamic information (ie: ${aws:username} ). { \"Sid\" : \"AllowAllS3ActionsInUserFolder\" , \"Action\" : [ \"s3:*\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3:::my-company/home/${aws:username}/*\" ] Creating a Policy \u00b6 Three ways to create a policy via the UI: The visual editor. The AWS policy generator. Hand-crafted json. Via the visual editor - Select the service. Select the allowed actions (list, read, write, GetBucket etc). Select resources that the policy applies to (arn). Specific request conditions. Add additional permissions if needed. Give the policy a name and description. Click Create Policy . Select a role. Attach the policy to the role. Granting User Permissions to pass a role to an AWS Service \u00b6 AWS Services must be passed a role in order to configure them. They'll assume this role to perform actions. Requires the iam:PassRole permission. Usually used in combination with iam:GetRole to view the role being passed. Roles can only be passed to what their Trust allows. Example role passing policy \u00b6 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:*\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::123456789012:role/S3Access\" } ] } Trust Policies \u00b6 Defines which IAM Principle (accounts, users or roles) is allowed to assume a role. Only type of resource policy supported by IAM. Example trust policy \u00b6 { \"Version\" : \"2012-10-17\" , \"Statement\" : { \"Sid\" : \"TrustPolicyStatementThatAllowsEC2ServiceToAssumeTheAttachedRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" }, \"Action\" : \"sts:AssumeRole\" } } IAM Security Tools \u00b6 IAM Credentials Report \u00b6 Account level. Lists the users in an account and the status of their credentials. Access Advisor \u00b6 Operates at the User level. Service permissions granted to a user, and when the services were last accessed. Check for unused roles. Access Analyzer \u00b6 Identify resources that are shared with an external entity (s3 buckets, iam roles etc). Find unintended access to resources & data. AWS Policy Simulator \u00b6 Online tool to test IAM policies. Can test the effect of policies attached to a user, group or role Select the user/group/role. Select the service(s) to test the policy against. Select the API action to test the policy against. AWS CLI Dry Runs \u00b6 Use the --dry-run arg to validate permissions without creating resources. For example: $ aws ec2 run-instances --dry-run --image-id ami-06340c8c12baa6a09 --instance-type t2.micro CloudShell \u00b6 Access from the management console. Default region is the region that you're logged into within the console. Files created within CloudShell are persisted. Can download/upload files. Supports multiple tabs (shells). Only available in some regions.","title":"Identity & Access Management"},{"location":"fundamentals/iam/#identity-access-management","text":"","title":"Identity &amp; Access Management"},{"location":"fundamentals/iam/#identity-access-management-iam","text":"Consists of users (a person), groups (function/team) and roles (internal aws usage, assigned to machines). Policies defined in JSON. Global service, not bound to a region/AZ. Supports MFA. Has pre-defined policies. Should follow least privilege principle.","title":"Identity &amp; Access Management (IAM)"},{"location":"fundamentals/iam/#best-practise","text":"Delete the root access keys. Activate MFA. Create an IAM user. Create a group to allocate permissions. Never store AWS credentials in code (duh). Use the credentials chain Use IAM roles as much as possible within AWS. Use environment variables or a named profile when working outside of AWS.","title":"Best Practise"},{"location":"fundamentals/iam/#iam-federation","text":"Integrates corporate user respositories via SAML (LDAP etc).","title":"IAM Federation"},{"location":"fundamentals/iam/#iam-roles","text":"Common roles are EC2 instance roles, Lambda Function roles and CloudFormation roles.","title":"IAM Roles"},{"location":"fundamentals/iam/#iam-policies","text":"AWS Managed policies are managed by AWS. Good for power users & administrators. They get updated when there's changes to AWS services, or new services made available. Customer Managed policies are re-usable against many principals. They're version controlled, support rollback, and have central change management. Inline policies apply to a single principal. No version control, no rollback, deleted if the IAM principal is deleted, and limited to 2KB. IAM policies apply to users, roles and groups. S3 policies apply to specific buckets. S3 bucket permissions are based on the union of the IAM policy(s) and S3 policy(s).","title":"IAM Policies"},{"location":"fundamentals/iam/#policy-types","text":"Type Description Identity-based Managed/inline policies that allow/deny permissions for a specific identity (user, group, role). Resource-based Inline policies attached to resources to allow/deny access. Permissions Boundaries Defines the maximum permissions for an identity. Doesn't grant permissions. Organisation Service Control Policy (SCP) Defines maximum permissions for account members of an organisation or OU. Only denys permissions. Access Control List (ACL) Grants permissions on resources in other accounts (cross-account). Session Limits permissions when assuming a role or federated user. .","title":"Policy Types"},{"location":"fundamentals/iam/#policy-evaluation-order","text":"If there's an explicit DENY, deny access. If there's an explicit ALLOW, allow access. Otherwise, deny access.","title":"Policy Evaluation Order"},{"location":"fundamentals/iam/#dynamic-policies","text":"Policy variables can be used to insert dynamic information (ie: ${aws:username} ). { \"Sid\" : \"AllowAllS3ActionsInUserFolder\" , \"Action\" : [ \"s3:*\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3:::my-company/home/${aws:username}/*\" ]","title":"Dynamic Policies"},{"location":"fundamentals/iam/#creating-a-policy","text":"Three ways to create a policy via the UI: The visual editor. The AWS policy generator. Hand-crafted json. Via the visual editor - Select the service. Select the allowed actions (list, read, write, GetBucket etc). Select resources that the policy applies to (arn). Specific request conditions. Add additional permissions if needed. Give the policy a name and description. Click Create Policy . Select a role. Attach the policy to the role.","title":"Creating a Policy"},{"location":"fundamentals/iam/#granting-user-permissions-to-pass-a-role-to-an-aws-service","text":"AWS Services must be passed a role in order to configure them. They'll assume this role to perform actions. Requires the iam:PassRole permission. Usually used in combination with iam:GetRole to view the role being passed. Roles can only be passed to what their Trust allows.","title":"Granting User Permissions to pass a role to an AWS Service"},{"location":"fundamentals/iam/#trust-policies","text":"Defines which IAM Principle (accounts, users or roles) is allowed to assume a role. Only type of resource policy supported by IAM.","title":"Trust Policies"},{"location":"fundamentals/iam/#iam-security-tools","text":"","title":"IAM Security Tools"},{"location":"fundamentals/iam/#iam-credentials-report","text":"Account level. Lists the users in an account and the status of their credentials.","title":"IAM Credentials Report"},{"location":"fundamentals/iam/#access-advisor","text":"Operates at the User level. Service permissions granted to a user, and when the services were last accessed. Check for unused roles.","title":"Access Advisor"},{"location":"fundamentals/iam/#access-analyzer","text":"Identify resources that are shared with an external entity (s3 buckets, iam roles etc). Find unintended access to resources & data.","title":"Access Analyzer"},{"location":"fundamentals/iam/#aws-policy-simulator","text":"Online tool to test IAM policies. Can test the effect of policies attached to a user, group or role Select the user/group/role. Select the service(s) to test the policy against. Select the API action to test the policy against.","title":"AWS Policy Simulator"},{"location":"fundamentals/iam/#aws-cli-dry-runs","text":"Use the --dry-run arg to validate permissions without creating resources. For example: $ aws ec2 run-instances --dry-run --image-id ami-06340c8c12baa6a09 --instance-type t2.micro","title":"AWS CLI Dry Runs"},{"location":"fundamentals/iam/#cloudshell","text":"Access from the management console. Default region is the region that you're logged into within the console. Files created within CloudShell are persisted. Can download/upload files. Supports multiple tabs (shells). Only available in some regions.","title":"CloudShell"},{"location":"fundamentals/rds/","text":"Relational Database Service \u00b6 What is RDS? \u00b6 Managed DB service that uses SQL. Database name needs to be unique across the region. Storage can auto-scale. VPC can't be changed once the instance is created. Can connect using IAM users and roles. Types of RDS \u00b6 Postgres MySQL MariaDB Oracle MS SQL Service Aurora (AWS proprietary database). Why RDS instead of DB on EC2? \u00b6 Automated provisioning and OS patching. Continuous backups & point in time restores. Monitoring dashboards. Read replicas for improved read performance. Multi-AZ for DR. Maintenance windows for upgrades. Scaling capabilities (horizontal/vertical). Storage backed by EBS. No SSH access. RDS Backups \u00b6 Automatically enabled. Daily full backups during maintenance windows. Transaction logs backed up every 5mins. 7 day retention of automated backups (can be increased to 35 days). Manually triggered DB snapshots. Snapshots retained as long as needed. RDS Read Replicas \u00b6 Help to scale read performance/capacity. Create upto 5 read replicas within the same AZ, cross AZ, or cross region. Asynchronous replication. A read replica can be promoted to it's own database (effectively cloning the db server). Applications must update the connection string to leverage read replicas. Use case: Run heavy reporting & analytics against an application without impacting production performance. There's a netwok cost transferring data between AZs. Can save money by keeping read replicas in the same AZ. RDS Multi-AZ \u00b6 Used for disaster recovery. Synchronous replication between 2 RDS in different AZs. One DNS name, two hosts. Automatic failover to stand-by database which increases availability. Doesn't require any intervention in the applications. Not used for scaling. Read replicas can be setup as Multi AZ for DR. RDS Encryption \u00b6 Master and read replicas can be encrypted using AWS KMS (AES-256). Encryption must be defined at launch time. Read replicas can't be encrypted if the master isn't. Enable transparent data encryption (TDE) for Oracle and SQL Server. SSL certs to encrypt data in transit. Need to configure the database to ensure SSL. Postsql is a parameter group to enable encryption. Snapshots of un-encrypted databases are also un-encrypted. Snapshots of an encrypted database are also encrypted. Can copy a snapshot into an encrypted one. TDE not supported by Postgresql. Encrypting an un-encrypted database \u00b6 Take snapshot of the un-encrypted database. Copy the snapshot, and enable encryption. Restore the database from the encrypted snapshot. Migrate applications to the new database, and delete the old one. Network Security \u00b6 Deploy to private subnet. Uses security groups that are attached to the RDS instance. Controls IP/security groups that can communicate with RDS. IAM Security \u00b6 IAM policies help control who can manage the RDS. Traditional username/password to login to the database. IAM-based authentication for MySQL and PostgreSQL. Not supported by Oracle. IAM Authentication \u00b6 Get auth token from IAM and RDS API calls. Auth token has a 15min lifetime. EC2 instance is given an IAM role to allow it to request an auth token from the RDS service via API. EC2 instance uses SSL encryption and passes the auth token through to the RDS instance. Benefits of IAM authentication \u00b6 Requires SSL. IAM roles and EC2 instance profiles for easy migration. RDS Shared Responsibility \u00b6 Users \u00b6 Check ports/IP/security group inbound rules. In-database user creation and permissions, or manage via IAM. Create database with/without public access. Ensure parameter groups or DB is configured to only allow SSL connections. AWS \u00b6 No SSH access. No manual DB patching. No manual OS patching. No way to audit the underlying instance.","title":"Relational Database Service"},{"location":"fundamentals/rds/#relational-database-service","text":"","title":"Relational Database Service"},{"location":"fundamentals/rds/#what-is-rds","text":"Managed DB service that uses SQL. Database name needs to be unique across the region. Storage can auto-scale. VPC can't be changed once the instance is created. Can connect using IAM users and roles.","title":"What is RDS?"},{"location":"fundamentals/rds/#types-of-rds","text":"Postgres MySQL MariaDB Oracle MS SQL Service Aurora (AWS proprietary database).","title":"Types of RDS"},{"location":"fundamentals/rds/#why-rds-instead-of-db-on-ec2","text":"Automated provisioning and OS patching. Continuous backups & point in time restores. Monitoring dashboards. Read replicas for improved read performance. Multi-AZ for DR. Maintenance windows for upgrades. Scaling capabilities (horizontal/vertical). Storage backed by EBS. No SSH access.","title":"Why RDS instead of DB on EC2?"},{"location":"fundamentals/rds/#rds-backups","text":"Automatically enabled. Daily full backups during maintenance windows. Transaction logs backed up every 5mins. 7 day retention of automated backups (can be increased to 35 days). Manually triggered DB snapshots. Snapshots retained as long as needed.","title":"RDS Backups"},{"location":"fundamentals/rds/#rds-read-replicas","text":"Help to scale read performance/capacity. Create upto 5 read replicas within the same AZ, cross AZ, or cross region. Asynchronous replication. A read replica can be promoted to it's own database (effectively cloning the db server). Applications must update the connection string to leverage read replicas. Use case: Run heavy reporting & analytics against an application without impacting production performance. There's a netwok cost transferring data between AZs. Can save money by keeping read replicas in the same AZ.","title":"RDS Read Replicas"},{"location":"fundamentals/rds/#rds-multi-az","text":"Used for disaster recovery. Synchronous replication between 2 RDS in different AZs. One DNS name, two hosts. Automatic failover to stand-by database which increases availability. Doesn't require any intervention in the applications. Not used for scaling. Read replicas can be setup as Multi AZ for DR.","title":"RDS Multi-AZ"},{"location":"fundamentals/rds/#rds-encryption","text":"Master and read replicas can be encrypted using AWS KMS (AES-256). Encryption must be defined at launch time. Read replicas can't be encrypted if the master isn't. Enable transparent data encryption (TDE) for Oracle and SQL Server. SSL certs to encrypt data in transit. Need to configure the database to ensure SSL. Postsql is a parameter group to enable encryption. Snapshots of un-encrypted databases are also un-encrypted. Snapshots of an encrypted database are also encrypted. Can copy a snapshot into an encrypted one. TDE not supported by Postgresql.","title":"RDS Encryption"},{"location":"fundamentals/rds/#encrypting-an-un-encrypted-database","text":"Take snapshot of the un-encrypted database. Copy the snapshot, and enable encryption. Restore the database from the encrypted snapshot. Migrate applications to the new database, and delete the old one.","title":"Encrypting an un-encrypted database"},{"location":"fundamentals/rds/#network-security","text":"Deploy to private subnet. Uses security groups that are attached to the RDS instance. Controls IP/security groups that can communicate with RDS.","title":"Network Security"},{"location":"fundamentals/rds/#iam-security","text":"IAM policies help control who can manage the RDS. Traditional username/password to login to the database. IAM-based authentication for MySQL and PostgreSQL. Not supported by Oracle.","title":"IAM Security"},{"location":"fundamentals/rds/#iam-authentication","text":"Get auth token from IAM and RDS API calls. Auth token has a 15min lifetime. EC2 instance is given an IAM role to allow it to request an auth token from the RDS service via API. EC2 instance uses SSL encryption and passes the auth token through to the RDS instance.","title":"IAM Authentication"},{"location":"fundamentals/rds/#rds-shared-responsibility","text":"","title":"RDS Shared Responsibility"},{"location":"fundamentals/rds/#users","text":"Check ports/IP/security group inbound rules. In-database user creation and permissions, or manage via IAM. Create database with/without public access. Ensure parameter groups or DB is configured to only allow SSL connections.","title":"Users"},{"location":"fundamentals/rds/#aws","text":"No SSH access. No manual DB patching. No manual OS patching. No way to audit the underlying instance.","title":"AWS"},{"location":"fundamentals/route53/","text":"Route53 \u00b6 Overview \u00b6 Managed DNS service. Most common records - A (hostname to IPv4) AAAA (hostname to IPv6) CNAME (hostname to hostname) Alias (hostname to AWS resource) Can use public domain names. Can use private domain names that can only be resolved by instances within the VPC. Costs $0.50/month per hosted zone. Global service. Provides advanced features like load balancing, health checks, routing policies. Time to Live (TTL) \u00b6 Web browser caches DNS record for given number of seconds to minimise DNS lookups. Default TTL is 300secs. Common DNS Records \u00b6 CNAME \u00b6 Points a hostname to any another hostname. Can't be used for root domains. Alias \u00b6 Points a hostname to an AWS resource (foo.amazonaws.com) Can be used for root domains (zone apex). Free of charge. Supports native health checks (use the health check configured on the load balancer). Can add multiple addresses to the record. Routing Policies \u00b6 Simple Routing Policy \u00b6 When you need to redirect to a single resource. Doesn't support health checks. Can return multiple values to the client, clients choses a random value to use. Weighted Routing Policy \u00b6 Controls the percentage of requests that go to particular endpoints. Useful way to test new versions of the application on a limited sub-set of requests. Can be associated with health checks. Latency Routing Policy \u00b6 Direct to the server that has the least latency (based on AWS region). A user in Germany might be routed to resources in the US if that has the lowest latency. Health Checks \u00b6 By default, health state changes after 3 successful/unsuccessful health checks in a row. 30second health check interval by default. Fast health checks happen every 10seconds but costs money. Around 15 health checkers will check the endpoint health. So roughly 1 request/sec. Supports HTTP, TCP, HTTPS (no SSL verification) healthchecks. Can integrate healthcheck with CloudWatch. Can use string matching, invert the health check status, and select which regions the healthcheck should run from. Can monitor endpoint, cloudwatch alarm, or do a calculated healthcheck. Failover Routing Policy \u00b6 Failover to secondary site when healthcheck fails. Can only have one primary, and one secondary record (duh). GeoProximity Routing Policy \u00b6 Routed based on user location. Need to have a default policy for users in a location that doesn't have an explicit policy. Increase/decrease bias value to control how much traffic is sent to each region based on where the user is. Multi-value Routing Policy \u00b6 Route traffic to multiple resources Each multi-value record is associated with a route53 health check. Up to 8 healthy records returned per multi-value query. If one resource becomes unavailable, it will be removed from the returned DNS record, and the client can chose one of the remaining available resources.","title":"Route53"},{"location":"fundamentals/route53/#route53","text":"","title":"Route53"},{"location":"fundamentals/route53/#overview","text":"Managed DNS service. Most common records - A (hostname to IPv4) AAAA (hostname to IPv6) CNAME (hostname to hostname) Alias (hostname to AWS resource) Can use public domain names. Can use private domain names that can only be resolved by instances within the VPC. Costs $0.50/month per hosted zone. Global service. Provides advanced features like load balancing, health checks, routing policies.","title":"Overview"},{"location":"fundamentals/route53/#time-to-live-ttl","text":"Web browser caches DNS record for given number of seconds to minimise DNS lookups. Default TTL is 300secs.","title":"Time to Live (TTL)"},{"location":"fundamentals/route53/#common-dns-records","text":"","title":"Common DNS Records"},{"location":"fundamentals/route53/#cname","text":"Points a hostname to any another hostname. Can't be used for root domains.","title":"CNAME"},{"location":"fundamentals/route53/#alias","text":"Points a hostname to an AWS resource (foo.amazonaws.com) Can be used for root domains (zone apex). Free of charge. Supports native health checks (use the health check configured on the load balancer). Can add multiple addresses to the record.","title":"Alias"},{"location":"fundamentals/route53/#routing-policies","text":"","title":"Routing Policies"},{"location":"fundamentals/route53/#simple-routing-policy","text":"When you need to redirect to a single resource. Doesn't support health checks. Can return multiple values to the client, clients choses a random value to use.","title":"Simple Routing Policy"},{"location":"fundamentals/route53/#weighted-routing-policy","text":"Controls the percentage of requests that go to particular endpoints. Useful way to test new versions of the application on a limited sub-set of requests. Can be associated with health checks.","title":"Weighted Routing Policy"},{"location":"fundamentals/route53/#latency-routing-policy","text":"Direct to the server that has the least latency (based on AWS region). A user in Germany might be routed to resources in the US if that has the lowest latency.","title":"Latency Routing Policy"},{"location":"fundamentals/route53/#failover-routing-policy","text":"Failover to secondary site when healthcheck fails. Can only have one primary, and one secondary record (duh).","title":"Failover Routing Policy"},{"location":"fundamentals/route53/#geoproximity-routing-policy","text":"Routed based on user location. Need to have a default policy for users in a location that doesn't have an explicit policy. Increase/decrease bias value to control how much traffic is sent to each region based on where the user is.","title":"GeoProximity Routing Policy"},{"location":"fundamentals/route53/#multi-value-routing-policy","text":"Route traffic to multiple resources Each multi-value record is associated with a route53 health check. Up to 8 healthy records returned per multi-value query. If one resource becomes unavailable, it will be removed from the returned DNS record, and the client can chose one of the remaining available resources.","title":"Multi-value Routing Policy"},{"location":"fundamentals/s3/","text":"Simple Storage Service (S3) \u00b6 Overview \u00b6 S3 buckets are bound to a region. Buckets must have a globally unique name. Public access is blocked by default. Bucket names must be DNS compliant - No uppercase No underscore 3-63 chars Not an IP Must start with a lowercase letter or number S3 Objects \u00b6 Max object size is 5TB. Use multi-part upload for anything over 5GB. Add extra metadata using key/value pairs. Can be tagged. Have a version id if versioning is enabled. Objects within the bucket can have different ACLs, encryption methods, storage classes etc. Use Pre-signed URLs to access an object without using the public URL. Versioning \u00b6 Enabled at the bucket level. New version generated each time an object with the same key is over-written. Files not versioned prior to enabling versioning will have a null version id. Suspending versioning doesn't delete the previous versions. Delete marker is created when a file is deleted. Old versions of the file will remain accessible. Restore a file by deleting the delete marker. Encryption \u00b6 There's four ways to encrypt objects, and a bucket can have a default encryption method. Method Description SSE-S3 Use keys handled and managed by AWS. SSE-KMS Use AWS KMS to manage encryption keys. SSE-C Manage your own encryption keys. Client Side Encryption Customer fully manages the encryption/decryption process on the client side. SSE-S3 \u00b6 Encryption keys managed by AWS S3. Uses AES-256. Must add the header \"x-amz-server-side-encryption\": \"AES256\" . SSE-KMS \u00b6 Encryption keys are managed in AWS KMS. Can control who has access to the encryption keys. Usage is logged in CloudTrail. Customer Master Key (CMK) is used to encrypt S3 objects. Requires the \"x-amz-server-side-encryption\": \"aws:kms\" header. Needs a KMS key policy that authorizes the user/role. SSE-C \u00b6 Uses a data key provided by the user. Key isn't stored by S3. Must use HTTPS. Encryption key needs to be in the HTTP header. Client Side Encryption \u00b6 Customer fully controls the encryption/decryption cycle. Clients must encrypt/decrypt the object themselves. S3 Encryption Client library can help do this. Encryption in Transit (TLS/SS) \u00b6 S3 endpoints can be utilized using HTTP, or HTTPS. Force SSL by creating a DENY rule on the condition \"aws:SecureTransport\": \"false\" . Setting aws:SecureTransport to true would allow anonymous GetObject if using SSL. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowSSLRequestsOnly\" , \"Action\" : \"s3:*\" , \"Effect\" : \"Deny\" , \"Resource\" : [ \"arn:aws:s3:::awsexamplebucket\" , \"arn:aws:s3:::awsexamplebucket/*\" ], \"Condition\" : { \"Bool\" : { \"aws:SecureTransport\" : \"false\" } }, \"Principal\" : \"*\" } ] } - On SSE-KMS , need to limit the encryption headers to SSE-KMS only, and deny on no header. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"DenyIncorrectEncryptionHeader\" , \"Effect\" : \"Deny\" , \"Principal\" : \"*\" , \"Action\" : \"s3:PutObject\" , \"Resource\" : \"arn:aws:s3:::bucket_name/*\" , \"Condition\" : { \"StringNotEquals\" : { \"s3:x-amz-server-side-encryption\" : \"aws:kms\" } } }, { \"Sid\" : \"DenyUnencryptedObjectUploads\" , \"Effect\" : \"Deny\" , \"Action\" : \"s3:PutObject\" , \"Resource\" : \"arn:aws:s3::bucket_name/*\" , \"Condition\" : { \"Null\" : { \"s3:x-amz-server-side-encryption\" : true } } } ] } Default Encryption \u00b6 Bucket policies are evaluated before default encryption. Can use AES-256 via SSE-S3 , or SSE-KMS . S3 Bucket Key \u00b6 Reduces API calls to KMS (reduces costs). Encryption Workflow - AWS KMS generates a S3 bucket key using the CMK. S3 generates a Data Encryption Key (DEK) using the S3 Bucket Key. S3 Objects are encrypted using the Data Encryption Keys (DEKs). S3 Security \u00b6 User Based Security \u00b6 Controlling the API calls that are allowed for a given user using IAM policies. The IAM Principal (user) can access an S3 Object if: The users IAM permissions allow it, or the resource policy allows it. There's no explicit DENY on the bucket policy. Resource Based Security \u00b6 Object ACL can apply ACLs at the object level. Bucket ACL applies to the whole bucket. Not commonly used. Bucket Policy \u00b6 Applies to the whole bucket. Rules are defined in the S3 console, which allows cross-account access. Can be applied to bucket and objects. Blocking public access to buckets and objects can be done through: Access Control Lists (ACLs). A new public bucket, or access point policies. Blocking public and cross-account access to buckets and objects can be done through: Any public bucket or access point policy. Blocking public access to buckets can be set at the account level. Example Bucket Policy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PublicRead\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::mybucket/*\" ] } ] } Networking \u00b6 A VPC endpoint can be used to access a private bucket from other resources in the same VPC. Logging & Auditing \u00b6 S3 access logs can be stored in another S3 bucket. API usage is logged to CloudTrail. S3 Access Logs \u00b6 Logs all S3 Bucket access to another bucket. Use data analysis tools/Athena to analyse the access logs. Don't send logs to the same bucket that's being monitored. Can define a prefix to use on all of the generated logfiles. User Security \u00b6 MFA-Delete \u00b6 Requires MFA to delete an object version (adding a marker), or suspend versioning on the bucket. Versioning must be enabled on the bucket. Can only be enabled/disabled by the bucket owner. Can only be configured via the CLI. Pre-signed URLs \u00b6 Can be created via CLI or the SDK. Default expiration is 3600secs. Change using the --expires-in argument. Users given the pre-signed URL inherit the permissions of the person that generated the URL. The S3 signature version needs to be s3v4. Generating a Pre-signed URL \u00b6 $ aws configure set default.s3.signature_version s3v4 $ aws s3 presign <S3Uri> --expires-in <seconds> --region <bucket region> Use Cases \u00b6 Allow only logged in users to download a resource. Dynamically generating URLs due to high user churn. Temporarily allow a user to upload a file to a specific location in the bucket. S3 Static Websites \u00b6 Accessible via <bucket name>.s3-website-<region>.amazonaws.com Bucket needs to be public, with a S3 policy that allows public reads, otherwise you'll get a 403 error. Bucket objects need to be un-encrypted. Can set a custom index and error page. Can define redirection rules. CORS \u00b6 Allows the website to make requests to other origins than have been white-listed. Uses the Access-Control-Allow-Origin header to control permitted origins. S3 CORS \u00b6 Allow a specific origin via an explicit name, or use \"*\" to allow all origins. CORS headers must be enabled on cross-origin buckets, to allow a client to do a cross-origin request. Consistency Model \u00b6 PUT on new objects has Read after write consistency. PUT / DELETE on existing objects is eventually consistent. There's no way to request strong consistency. S3 Replication \u00b6 Asynchronous. Must enable versioning in the source and destination buckets. Requires an IAM role with permissions to copy between buckets. Only new objects created after replication is enabled will be copied across. Deletes are not replicated. Deleting without a version id will create a delete marker. Deleting with a version id will delete in the source only. No replication chaining. Where bucket1 is replicating into bucket2 , and bucket2 is replicating into bucket3 , objects created in bucket1 won't be replicated to bucket3 . Can replicate the whole bucket, or only specific prefixes/tags. Can replicate objects encrypted with AWS KMS. Cross-Region Replication (CRR) \u00b6 S3 replication between buckets in different regions. Used for compliance, low latency access, replication across accounts. Same Region Replication (SRR) \u00b6 S3 replication between buckets in the same region. Used for log aggregation, live replication between prod and test accounts. S3 Storage Classes \u00b6 S3 Standard - General Purpose \u00b6 11 9's durability. 4 9's availability. Can handle upto 2 concurrent facility failures. General purpose. Minimum storage duration is 30 days. No retrieval fee. Use Cases \u00b6 Big data analytics. Mobile & gaming applications. Content distribution. S3 Standard - Infrequent Access (IA) \u00b6 Data that isn't accessed very often, but needs to be retrieved quickly when needed. 11 9's durability. 3 9's availability. Cheaper than S3 Standard. Minimum storage duration is 30 days. Retrieval fee per GB retrieved. Use Cases \u00b6 DR. Backups. S3 One Zone - Infrequent Access \u00b6 Same as IA, but data stored in a single AZ. 11 9's durability of objects in a single AZ. 99.5% availability. Low latency, high throughput. Supports SSL for encryption at in transit/at rest. 20% cheaper than IA. Minimum storage duration is 30 days. Retrieval fee per GB retrieved. Use Cases \u00b6 Secondary backup copies of on-prem data. Storing data that could be re-created if needed. S3 Intelligent Tiering \u00b6 Low latency, high throughput. Additional cost to monitor objects and automatically move them between two different access tiers. 11 9's durability. 3 9's availability. Resilient to loss of entire AZ. Minimum storage duration is 30 days. No retrieval fee. Glacier \u00b6 Low cost. Intended for archiving/backups. Retaining data for a long time (multiple years). Alternative to magnetic tape storage. 11 9's durability. Each object is called an Archive and can be upto 40TB. Archives are stored in Vaults, not buckets. Minimum storage duration is 90 days. Retrieval fee per GB retrieved. Retrieval Options \u00b6 Expidited (1-5mins). Standard (3-5hrs). Bulk (5-12hrs). Glacier Deep Archive \u00b6 Cheapest option. Minimum storage duration is 180 days. Retrieval Options \u00b6 Standard (12hrs). Bulk (48hrs). Lifecycle Configuration (Policies) \u00b6 Lifecycle rules define when to move an object to another storage class. Expiration actions define when an object/previous versions will be deleted, or when to clean up incomplete multi-part uploads. Rules can be applied against a specific prefix, or object tags. Lifecycle Rule Actions \u00b6 Transition current versions between storage classes. Transition previous versions between storage classes. Expire current versions. Permanently delete previous versions. Delete expired delete markers or incomplete multi-part uploads. Performance \u00b6 Latency of 100-200ms. 3,500 PUT/COPY/POST/DELETE requests per second, per bucket prefix. 5,500 GET/HEAD requests per second, per bucket prefix. Using SSE-KMS will impose KMS limits (fixed number of API requests per second). Optimising \u00b6 Use multi-part uploads for files over 100MB. Mandatory for 5GB+. Use S3 Transfer Acceleration for uploads. Transfers the file to an edge location, and forwards it from there to the S3 bucket using the private AWS network. Use S3 Byte-Range fetches to download files (simlar to multi-part uploads). Only retrieve part of the object (ie: the head of a file). S3 Select/Glacier Select \u00b6 Retrieve less data using SQL via server side filtering. Filter by rows and columns. Reduces network transfer, client-side CPU usage. Useful for structured data (csv files etc). Event Notifications \u00b6 S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication Can filter by object name. Good use case, is generating thumbnails when an image is uploaded to S3. Delivered within seconds. Bucket versioning is required to ensure an event is sent for every successful write. Can send the event to SQS, SNS or Lambda function. S3 Object Lock \u00b6 Write Once, Read Many. Block object version deletion for a specified amount of time. Glacier Vault Lock \u00b6 Write Once, Read Many. Lock the policy for future edits. Useful for compliance & data retention.","title":"Simple Storage Service (S3)"},{"location":"fundamentals/s3/#simple-storage-service-s3","text":"","title":"Simple Storage Service (S3)"},{"location":"fundamentals/s3/#overview","text":"S3 buckets are bound to a region. Buckets must have a globally unique name. Public access is blocked by default. Bucket names must be DNS compliant - No uppercase No underscore 3-63 chars Not an IP Must start with a lowercase letter or number","title":"Overview"},{"location":"fundamentals/s3/#s3-objects","text":"Max object size is 5TB. Use multi-part upload for anything over 5GB. Add extra metadata using key/value pairs. Can be tagged. Have a version id if versioning is enabled. Objects within the bucket can have different ACLs, encryption methods, storage classes etc. Use Pre-signed URLs to access an object without using the public URL.","title":"S3 Objects"},{"location":"fundamentals/s3/#versioning","text":"Enabled at the bucket level. New version generated each time an object with the same key is over-written. Files not versioned prior to enabling versioning will have a null version id. Suspending versioning doesn't delete the previous versions. Delete marker is created when a file is deleted. Old versions of the file will remain accessible. Restore a file by deleting the delete marker.","title":"Versioning"},{"location":"fundamentals/s3/#encryption","text":"There's four ways to encrypt objects, and a bucket can have a default encryption method. Method Description SSE-S3 Use keys handled and managed by AWS. SSE-KMS Use AWS KMS to manage encryption keys. SSE-C Manage your own encryption keys. Client Side Encryption Customer fully manages the encryption/decryption process on the client side.","title":"Encryption"},{"location":"fundamentals/s3/#sse-s3","text":"Encryption keys managed by AWS S3. Uses AES-256. Must add the header \"x-amz-server-side-encryption\": \"AES256\" .","title":"SSE-S3"},{"location":"fundamentals/s3/#sse-kms","text":"Encryption keys are managed in AWS KMS. Can control who has access to the encryption keys. Usage is logged in CloudTrail. Customer Master Key (CMK) is used to encrypt S3 objects. Requires the \"x-amz-server-side-encryption\": \"aws:kms\" header. Needs a KMS key policy that authorizes the user/role.","title":"SSE-KMS"},{"location":"fundamentals/s3/#sse-c","text":"Uses a data key provided by the user. Key isn't stored by S3. Must use HTTPS. Encryption key needs to be in the HTTP header.","title":"SSE-C"},{"location":"fundamentals/s3/#client-side-encryption","text":"Customer fully controls the encryption/decryption cycle. Clients must encrypt/decrypt the object themselves. S3 Encryption Client library can help do this.","title":"Client Side Encryption"},{"location":"fundamentals/s3/#encryption-in-transit-tlsss","text":"S3 endpoints can be utilized using HTTP, or HTTPS. Force SSL by creating a DENY rule on the condition \"aws:SecureTransport\": \"false\" . Setting aws:SecureTransport to true would allow anonymous GetObject if using SSL. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowSSLRequestsOnly\" , \"Action\" : \"s3:*\" , \"Effect\" : \"Deny\" , \"Resource\" : [ \"arn:aws:s3:::awsexamplebucket\" , \"arn:aws:s3:::awsexamplebucket/*\" ], \"Condition\" : { \"Bool\" : { \"aws:SecureTransport\" : \"false\" } }, \"Principal\" : \"*\" } ] } - On SSE-KMS , need to limit the encryption headers to SSE-KMS only, and deny on no header. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"DenyIncorrectEncryptionHeader\" , \"Effect\" : \"Deny\" , \"Principal\" : \"*\" , \"Action\" : \"s3:PutObject\" , \"Resource\" : \"arn:aws:s3:::bucket_name/*\" , \"Condition\" : { \"StringNotEquals\" : { \"s3:x-amz-server-side-encryption\" : \"aws:kms\" } } }, { \"Sid\" : \"DenyUnencryptedObjectUploads\" , \"Effect\" : \"Deny\" , \"Action\" : \"s3:PutObject\" , \"Resource\" : \"arn:aws:s3::bucket_name/*\" , \"Condition\" : { \"Null\" : { \"s3:x-amz-server-side-encryption\" : true } } } ] }","title":"Encryption in Transit (TLS/SS)"},{"location":"fundamentals/s3/#default-encryption","text":"Bucket policies are evaluated before default encryption. Can use AES-256 via SSE-S3 , or SSE-KMS .","title":"Default Encryption"},{"location":"fundamentals/s3/#s3-bucket-key","text":"Reduces API calls to KMS (reduces costs). Encryption Workflow - AWS KMS generates a S3 bucket key using the CMK. S3 generates a Data Encryption Key (DEK) using the S3 Bucket Key. S3 Objects are encrypted using the Data Encryption Keys (DEKs).","title":"S3 Bucket Key"},{"location":"fundamentals/s3/#s3-security","text":"","title":"S3 Security"},{"location":"fundamentals/s3/#user-based-security","text":"Controlling the API calls that are allowed for a given user using IAM policies. The IAM Principal (user) can access an S3 Object if: The users IAM permissions allow it, or the resource policy allows it. There's no explicit DENY on the bucket policy.","title":"User Based Security"},{"location":"fundamentals/s3/#resource-based-security","text":"Object ACL can apply ACLs at the object level. Bucket ACL applies to the whole bucket. Not commonly used.","title":"Resource Based Security"},{"location":"fundamentals/s3/#networking","text":"A VPC endpoint can be used to access a private bucket from other resources in the same VPC.","title":"Networking"},{"location":"fundamentals/s3/#logging-auditing","text":"S3 access logs can be stored in another S3 bucket. API usage is logged to CloudTrail.","title":"Logging &amp; Auditing"},{"location":"fundamentals/s3/#user-security","text":"","title":"User Security"},{"location":"fundamentals/s3/#s3-static-websites","text":"Accessible via <bucket name>.s3-website-<region>.amazonaws.com Bucket needs to be public, with a S3 policy that allows public reads, otherwise you'll get a 403 error. Bucket objects need to be un-encrypted. Can set a custom index and error page. Can define redirection rules.","title":"S3 Static Websites"},{"location":"fundamentals/s3/#cors","text":"Allows the website to make requests to other origins than have been white-listed. Uses the Access-Control-Allow-Origin header to control permitted origins.","title":"CORS"},{"location":"fundamentals/s3/#s3-cors","text":"Allow a specific origin via an explicit name, or use \"*\" to allow all origins. CORS headers must be enabled on cross-origin buckets, to allow a client to do a cross-origin request.","title":"S3 CORS"},{"location":"fundamentals/s3/#consistency-model","text":"PUT on new objects has Read after write consistency. PUT / DELETE on existing objects is eventually consistent. There's no way to request strong consistency.","title":"Consistency Model"},{"location":"fundamentals/s3/#s3-replication","text":"Asynchronous. Must enable versioning in the source and destination buckets. Requires an IAM role with permissions to copy between buckets. Only new objects created after replication is enabled will be copied across. Deletes are not replicated. Deleting without a version id will create a delete marker. Deleting with a version id will delete in the source only. No replication chaining. Where bucket1 is replicating into bucket2 , and bucket2 is replicating into bucket3 , objects created in bucket1 won't be replicated to bucket3 . Can replicate the whole bucket, or only specific prefixes/tags. Can replicate objects encrypted with AWS KMS.","title":"S3 Replication"},{"location":"fundamentals/s3/#cross-region-replication-crr","text":"S3 replication between buckets in different regions. Used for compliance, low latency access, replication across accounts.","title":"Cross-Region Replication (CRR)"},{"location":"fundamentals/s3/#same-region-replication-srr","text":"S3 replication between buckets in the same region. Used for log aggregation, live replication between prod and test accounts.","title":"Same Region Replication (SRR)"},{"location":"fundamentals/s3/#s3-storage-classes","text":"","title":"S3 Storage Classes"},{"location":"fundamentals/s3/#s3-standard-general-purpose","text":"11 9's durability. 4 9's availability. Can handle upto 2 concurrent facility failures. General purpose. Minimum storage duration is 30 days. No retrieval fee.","title":"S3 Standard - General Purpose"},{"location":"fundamentals/s3/#s3-standard-infrequent-access-ia","text":"Data that isn't accessed very often, but needs to be retrieved quickly when needed. 11 9's durability. 3 9's availability. Cheaper than S3 Standard. Minimum storage duration is 30 days. Retrieval fee per GB retrieved.","title":"S3 Standard - Infrequent Access (IA)"},{"location":"fundamentals/s3/#s3-one-zone-infrequent-access","text":"Same as IA, but data stored in a single AZ. 11 9's durability of objects in a single AZ. 99.5% availability. Low latency, high throughput. Supports SSL for encryption at in transit/at rest. 20% cheaper than IA. Minimum storage duration is 30 days. Retrieval fee per GB retrieved.","title":"S3  One Zone - Infrequent Access"},{"location":"fundamentals/s3/#s3-intelligent-tiering","text":"Low latency, high throughput. Additional cost to monitor objects and automatically move them between two different access tiers. 11 9's durability. 3 9's availability. Resilient to loss of entire AZ. Minimum storage duration is 30 days. No retrieval fee.","title":"S3 Intelligent Tiering"},{"location":"fundamentals/s3/#glacier","text":"Low cost. Intended for archiving/backups. Retaining data for a long time (multiple years). Alternative to magnetic tape storage. 11 9's durability. Each object is called an Archive and can be upto 40TB. Archives are stored in Vaults, not buckets. Minimum storage duration is 90 days. Retrieval fee per GB retrieved.","title":"Glacier"},{"location":"fundamentals/s3/#glacier-deep-archive","text":"Cheapest option. Minimum storage duration is 180 days.","title":"Glacier Deep Archive"},{"location":"fundamentals/s3/#lifecycle-configuration-policies","text":"Lifecycle rules define when to move an object to another storage class. Expiration actions define when an object/previous versions will be deleted, or when to clean up incomplete multi-part uploads. Rules can be applied against a specific prefix, or object tags.","title":"Lifecycle Configuration (Policies)"},{"location":"fundamentals/s3/#lifecycle-rule-actions","text":"Transition current versions between storage classes. Transition previous versions between storage classes. Expire current versions. Permanently delete previous versions. Delete expired delete markers or incomplete multi-part uploads.","title":"Lifecycle Rule Actions"},{"location":"fundamentals/s3/#performance","text":"Latency of 100-200ms. 3,500 PUT/COPY/POST/DELETE requests per second, per bucket prefix. 5,500 GET/HEAD requests per second, per bucket prefix. Using SSE-KMS will impose KMS limits (fixed number of API requests per second).","title":"Performance"},{"location":"fundamentals/s3/#optimising","text":"Use multi-part uploads for files over 100MB. Mandatory for 5GB+. Use S3 Transfer Acceleration for uploads. Transfers the file to an edge location, and forwards it from there to the S3 bucket using the private AWS network. Use S3 Byte-Range fetches to download files (simlar to multi-part uploads). Only retrieve part of the object (ie: the head of a file).","title":"Optimising"},{"location":"fundamentals/s3/#s3-selectglacier-select","text":"Retrieve less data using SQL via server side filtering. Filter by rows and columns. Reduces network transfer, client-side CPU usage. Useful for structured data (csv files etc).","title":"S3 Select/Glacier Select"},{"location":"fundamentals/s3/#event-notifications","text":"S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication Can filter by object name. Good use case, is generating thumbnails when an image is uploaded to S3. Delivered within seconds. Bucket versioning is required to ensure an event is sent for every successful write. Can send the event to SQS, SNS or Lambda function.","title":"Event Notifications"},{"location":"fundamentals/s3/#s3-object-lock","text":"Write Once, Read Many. Block object version deletion for a specified amount of time.","title":"S3 Object Lock"},{"location":"fundamentals/s3/#glacier-vault-lock","text":"Write Once, Read Many. Lock the policy for future edits. Useful for compliance & data retention.","title":"Glacier Vault Lock"},{"location":"fundamentals/ses/","text":"Simple Email Service (SES) \u00b6 Overview \u00b6 Allows e-mails to be sent via SMTP or the AWS SDK. Integrates with S3, SNS and Lambda. Integrated with IAM for managing send e-mail permissions.","title":"Simple Email Service (SES)"},{"location":"fundamentals/ses/#simple-email-service-ses","text":"","title":"Simple Email Service (SES)"},{"location":"fundamentals/ses/#overview","text":"Allows e-mails to be sent via SMTP or the AWS SDK. Integrates with S3, SNS and Lambda. Integrated with IAM for managing send e-mail permissions.","title":"Overview"},{"location":"fundamentals/vpc/","text":"VPC \u00b6 What is a VPC? \u00b6 Private network to deploy resources. VPC is bound to a region. Main Components \u00b6 Subnets \u00b6 Partition network inside VPC. Bound to the availability zone. Public subnet can be accessed from the internet. Private subnet is only accessible from within the VPC. Route Tables \u00b6 Internet Gateway \u00b6 Allows VPC instances to connect to the internet. Public subnet has a route to the IGW. NAT Gateways/Instances \u00b6 NAT gateways are managed by AWS. NAT instances are self-managed. Allow VPC instances on private subnets to acess the internet, while remaining private. Network ACL \u00b6 Firewall to control inbound/outbound subnet traffic. Rules only include IP addresses. Operates at the subnet level. Supports ALLOW and DENY rules. Is stateless. Return traffic must be explicitly allowed by rules. Rules are processed in number order. Automatically applies to all instances in the subnet its associated with. Security Groups \u00b6 Firewall to control inbound/outbound EC2/ENI traffic. Rules can use IP addresses as well as other resources. Operates at the ENI/EC2 instance level. Only supports ALLOW rules (not DENY). Is stateful, return traffic is automatically allowed. All rules are evaluated before allowing/blocking the traffic. Applies to instances only if they've been assigned the security group. VPC Flow Logs \u00b6 Captures information about IP traffic VPC flow logs Subnet flow logs ENI flow logs Useful for troubleshooting connectivity issues. Captures network information from any AWS managed interface ELBs Elasicache RDS Audora etc VPC flow log data can be sent to S3/CloudWatch logs. VPC Peering \u00b6 Connect two VPCs privately using the AWS network. Makes VPCs behave as though they're in the same network. Must not have overlapping CIDRs. Peering connection is non-transitive. If VPC A and B have peering setup,and VPC B & C have peering setup, VPC C can't reach A via B. VPC Endpoints \u00b6 Connect to AWS service using a private network instead of public www network. Improves security and reduces latency to access AWS services. Only used within the VPC. VPC Endpoint Gateway \u00b6 For S3 and DynamoDB. Allow private resources to access S3/DynamoDB using private AWS network. VPC Endpoint Interface (ENI) \u00b6 For any other AWS service. Site to Site VPN \u00b6 Connect an on-prem VPN to AWS. Uses the public internet. Comms are encrypted. Can't access VPC endpoints. Direct Connect \u00b6 Physical connection to AWS. Private network. Requires at least 1month to setup. Can't access VPC endpoints.","title":"VPC"},{"location":"fundamentals/vpc/#vpc","text":"","title":"VPC"},{"location":"fundamentals/vpc/#what-is-a-vpc","text":"Private network to deploy resources. VPC is bound to a region.","title":"What is a VPC?"},{"location":"fundamentals/vpc/#main-components","text":"","title":"Main Components"},{"location":"fundamentals/vpc/#subnets","text":"Partition network inside VPC. Bound to the availability zone. Public subnet can be accessed from the internet. Private subnet is only accessible from within the VPC.","title":"Subnets"},{"location":"fundamentals/vpc/#route-tables","text":"","title":"Route Tables"},{"location":"fundamentals/vpc/#internet-gateway","text":"Allows VPC instances to connect to the internet. Public subnet has a route to the IGW.","title":"Internet Gateway"},{"location":"fundamentals/vpc/#nat-gatewaysinstances","text":"NAT gateways are managed by AWS. NAT instances are self-managed. Allow VPC instances on private subnets to acess the internet, while remaining private.","title":"NAT Gateways/Instances"},{"location":"fundamentals/vpc/#network-acl","text":"Firewall to control inbound/outbound subnet traffic. Rules only include IP addresses. Operates at the subnet level. Supports ALLOW and DENY rules. Is stateless. Return traffic must be explicitly allowed by rules. Rules are processed in number order. Automatically applies to all instances in the subnet its associated with.","title":"Network ACL"},{"location":"fundamentals/vpc/#security-groups","text":"Firewall to control inbound/outbound EC2/ENI traffic. Rules can use IP addresses as well as other resources. Operates at the ENI/EC2 instance level. Only supports ALLOW rules (not DENY). Is stateful, return traffic is automatically allowed. All rules are evaluated before allowing/blocking the traffic. Applies to instances only if they've been assigned the security group.","title":"Security Groups"},{"location":"fundamentals/vpc/#vpc-flow-logs","text":"Captures information about IP traffic VPC flow logs Subnet flow logs ENI flow logs Useful for troubleshooting connectivity issues. Captures network information from any AWS managed interface ELBs Elasicache RDS Audora etc VPC flow log data can be sent to S3/CloudWatch logs.","title":"VPC Flow Logs"},{"location":"fundamentals/vpc/#vpc-peering","text":"Connect two VPCs privately using the AWS network. Makes VPCs behave as though they're in the same network. Must not have overlapping CIDRs. Peering connection is non-transitive. If VPC A and B have peering setup,and VPC B & C have peering setup, VPC C can't reach A via B.","title":"VPC Peering"},{"location":"fundamentals/vpc/#vpc-endpoints","text":"Connect to AWS service using a private network instead of public www network. Improves security and reduces latency to access AWS services. Only used within the VPC.","title":"VPC Endpoints"},{"location":"fundamentals/vpc/#vpc-endpoint-gateway","text":"For S3 and DynamoDB. Allow private resources to access S3/DynamoDB using private AWS network.","title":"VPC Endpoint Gateway"},{"location":"fundamentals/vpc/#vpc-endpoint-interface-eni","text":"For any other AWS service.","title":"VPC Endpoint Interface (ENI)"},{"location":"fundamentals/vpc/#site-to-site-vpn","text":"Connect an on-prem VPN to AWS. Uses the public internet. Comms are encrypted. Can't access VPC endpoints.","title":"Site to Site VPN"},{"location":"fundamentals/vpc/#direct-connect","text":"Physical connection to AWS. Private network. Requires at least 1month to setup. Can't access VPC endpoints.","title":"Direct Connect"},{"location":"identitymanagement/cognito/","text":"Cognito \u00b6 Overview \u00b6 Cognito gives users an identity so they can interact with an application. Cognito User Pools (CUP) \u00b6 Cognito User Pools are a serverless database of users for web & mobile applications. Can integrate with API gateway & ALB (with listeners and rules). Uses a username (or email address)/password combination. Supports password reset, email/phone number verification, MFA. Support federated identities (Facebook, Google, Apple, Amazon, SAML, OpenID Connect). User accounts can be blocked if their credentials are comprised somewhere. Login returns a JWT token. Custom CSS and Logo can be added to the login UI. Has triggers for AWS Lambda during the authentication flow. Lambda Triggers \u00b6 Lambda triggers allow Cognito User Pools to invoke a lambda function synchronously when certain events occur. User Pool Flow Operation Description Authentication Pre Authentication Custom validation to accept/deny the sign-in request. Authentication Post Authentication Event logging for custom analytics. Authentication Pre Token Generation Augment or suppress token claims. Sign-Up Pre Sign-up Customer validation to accept/deny sign-up requests. Sign-Up Post Confirmation Custom welcome messages, or event logging for custom analytics. Sign-Up Migrate User Migrate a user from an existing user directory to user pools. Messages Custom Message Advanced customisation and localisation of messages. Token Creation Pre Token Generation Add/remove attributes in Id tokens. Cognito Identity Pools (Federated Identity) \u00b6 Cognito Identity Pools obtain AWS credentials for application users so they can access AWS resources. Can allow unauthenticated (guest) access. Users can login via Facebook, Google, Apple, Amazon, OpenID, SAML, Cognito User Pools, Developer authenticated identities (custom login server). Can use Cognito User Pools as an identity provider. Users are mapped to IAM roles & policies. IAM policies can be customised based on the user_id variable for fine-grained control. Push synchronization pushwa changes made to user settings on one device, to all other devices. Cognito streams pushwa changes to a Kinesis steam for realtime event processing. Cognito events allows lambda functions to run in response to Cognito events. Authentication Process \u00b6 IAM Roles \u00b6 Default roles can be defined for authenticated and guest users. Rules can be used to determine which role is given to each user, based on their user_id . Use poolicy variables to partition users. Cognito Identity Pools obtain IAM credentials via STS. Roles must have a trust policy of Cognito Identity Pools. Guest Policy This policy allows unauthenticated access to my_picture.jpg . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"s3:GetObject\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3::mybucket/assets/my_picture.jpg\" ] } ] } Authenticated User Policy This policy allows an authenticated user to list bucket contents that start with the users user id, and modify any objects that exist under a location that matches the users user id. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"s3:ListBucket\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3::mybucket\" ], \"Condition\" : { \"StringLike\" : { \"s3:prefix\" : [ \"${cognito-identity.amazonaws.com:sub}/*\" ] } } }, { \"Action\" : [ \"s3:GetObject\" , \"s3:PutObject\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3::mybucket/${cognito-identity.amazonaws.com:sub}/*\" ] } ] } Cognito Sync \u00b6 Cognito Sync synchronises data from mobile devices to Cognito. Deprecated and replaced by AppSync. Store preferences, configuration, state of app. Has offline capability. Max size of each data set is 1MB. Up to 20 datasets can be stored. Push sync silently notifies all devices when identity data changes. Cognito stream sends data from Cognito to Kinesis. Cognito events execute lambda functions in response to events. Cognito vs IAM \u00b6 Use Cognito when you need to support hundreds of users, mobile users, or need to authenticate with SAML.","title":"Cognito"},{"location":"identitymanagement/cognito/#cognito","text":"","title":"Cognito"},{"location":"identitymanagement/cognito/#overview","text":"Cognito gives users an identity so they can interact with an application.","title":"Overview"},{"location":"identitymanagement/cognito/#cognito-user-pools-cup","text":"Cognito User Pools are a serverless database of users for web & mobile applications. Can integrate with API gateway & ALB (with listeners and rules). Uses a username (or email address)/password combination. Supports password reset, email/phone number verification, MFA. Support federated identities (Facebook, Google, Apple, Amazon, SAML, OpenID Connect). User accounts can be blocked if their credentials are comprised somewhere. Login returns a JWT token. Custom CSS and Logo can be added to the login UI. Has triggers for AWS Lambda during the authentication flow.","title":"Cognito User Pools (CUP)"},{"location":"identitymanagement/cognito/#lambda-triggers","text":"Lambda triggers allow Cognito User Pools to invoke a lambda function synchronously when certain events occur. User Pool Flow Operation Description Authentication Pre Authentication Custom validation to accept/deny the sign-in request. Authentication Post Authentication Event logging for custom analytics. Authentication Pre Token Generation Augment or suppress token claims. Sign-Up Pre Sign-up Customer validation to accept/deny sign-up requests. Sign-Up Post Confirmation Custom welcome messages, or event logging for custom analytics. Sign-Up Migrate User Migrate a user from an existing user directory to user pools. Messages Custom Message Advanced customisation and localisation of messages. Token Creation Pre Token Generation Add/remove attributes in Id tokens.","title":"Lambda Triggers"},{"location":"identitymanagement/cognito/#cognito-identity-pools-federated-identity","text":"Cognito Identity Pools obtain AWS credentials for application users so they can access AWS resources. Can allow unauthenticated (guest) access. Users can login via Facebook, Google, Apple, Amazon, OpenID, SAML, Cognito User Pools, Developer authenticated identities (custom login server). Can use Cognito User Pools as an identity provider. Users are mapped to IAM roles & policies. IAM policies can be customised based on the user_id variable for fine-grained control. Push synchronization pushwa changes made to user settings on one device, to all other devices. Cognito streams pushwa changes to a Kinesis steam for realtime event processing. Cognito events allows lambda functions to run in response to Cognito events.","title":"Cognito Identity Pools (Federated Identity)"},{"location":"identitymanagement/cognito/#authentication-process","text":"","title":"Authentication Process"},{"location":"identitymanagement/cognito/#iam-roles","text":"Default roles can be defined for authenticated and guest users. Rules can be used to determine which role is given to each user, based on their user_id . Use poolicy variables to partition users. Cognito Identity Pools obtain IAM credentials via STS. Roles must have a trust policy of Cognito Identity Pools. Guest Policy This policy allows unauthenticated access to my_picture.jpg . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"s3:GetObject\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3::mybucket/assets/my_picture.jpg\" ] } ] } Authenticated User Policy This policy allows an authenticated user to list bucket contents that start with the users user id, and modify any objects that exist under a location that matches the users user id. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"s3:ListBucket\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3::mybucket\" ], \"Condition\" : { \"StringLike\" : { \"s3:prefix\" : [ \"${cognito-identity.amazonaws.com:sub}/*\" ] } } }, { \"Action\" : [ \"s3:GetObject\" , \"s3:PutObject\" ], \"Effect\" : \"Allow\" , \"Resource\" : [ \"arn:aws:s3::mybucket/${cognito-identity.amazonaws.com:sub}/*\" ] } ] }","title":"IAM Roles"},{"location":"identitymanagement/cognito/#cognito-sync","text":"Cognito Sync synchronises data from mobile devices to Cognito. Deprecated and replaced by AppSync. Store preferences, configuration, state of app. Has offline capability. Max size of each data set is 1MB. Up to 20 datasets can be stored. Push sync silently notifies all devices when identity data changes. Cognito stream sends data from Cognito to Kinesis. Cognito events execute lambda functions in response to events.","title":"Cognito Sync"},{"location":"identitymanagement/cognito/#cognito-vs-iam","text":"Use Cognito when you need to support hundreds of users, mobile users, or need to authenticate with SAML.","title":"Cognito vs IAM"},{"location":"messaging/kinesis/","text":"Kinesis \u00b6 Overview \u00b6 Managed alternative to Kafka. Data is replicated across 3 AZ. Can quickly become expensive. Use Cases \u00b6 application logs, metrics IoT, Clickstreams. \"Real-time\" big data. Streaming processing frameworks (Spark, NiFi). Shard level metrics can be in 1minute intervals. Kinesis Streams \u00b6 Low latency streaming ingestion at scale. Streams are divided into ordered Shards/Partitions. Increase shards to increase throughput. Data retention is 1 day by default. Can be up to 7 days. Can reprocess/replay data. Multiple applications can consume the same stream. Real-time processing with scale of throughput. Data inserted into Kinesis can't be deleted (immutable). Shards \u00b6 Write 1MB/s, or 1000 messages per shard. Read 2MB/s per shard. Can provision up to 200 shards. Billing is per shard (provisioned). Messages can be batches. Can reshard/merge shards over time. Records are ordered per shard. Kinesis Analytics \u00b6 Real-time analytics on streams using SQL Kinesis Firehose \u00b6 Load streams into S3, Redshift, ElasticSearch etc. Producers \u00b6 Put records are used to send data to Kinesis. Need to provide the partition key. The message key is hashed to determine the shard id. The same key goes to the same partition (helps with ordering for a specific key). Message data must be base64 encoded. Each message is given a sequence number (sequential). Use a partition key that's highly distributed to avoid a \"hot partition\", which would result in all the data being sent to the same shard. user_id if many users. country_id not a good choice is all the users are in one country. Use Batching to reduce cost and increase throughput. Handling ProvisionedThroughputExceeded Exceptions \u00b6 Happens when MB/s or TPS are exceeded on a shard. Three solutions to the problem: Retries with backoff. Increase the shards (scaling). Use a more distributed (unique) partition key. Consumers \u00b6 Can use the SDK, CLI or Kinesis Client Library. Kinesis Client Library uses DynamoDB to checkpoint offsets, track other workers, and share the work amongst available shards. Pass the shart iterator to get records --shard-iterator to retrieve the messages for a given shard id. Kinesis Client Library (KCL) \u00b6 Java library for distributed applications that helps read records from Kinesis Streams, to share the read workload across multiple nodes. Each shard can only be ready by one KCL instance. ie: if there's 4 shards, the maximum number of KCL instances is 4. DynamoDB is used to checkpoint progress and co-ordinate consumption of shards across the KCL instances (requires IAM access to write to DynamoDB). KCL can run on EC2, Elastic Beanstalk, On-Prem applications etc. Record are read in the order that they were placed into the shard. Security \u00b6 IAM policies to control access/authorization. VPC endpoints are available to allow access to Kinesis from within a VPC. Encryption \u00b6 Encryption in transit via HTTPS. Encryption at rest using KMS Possible to use client side encryption. Kinesis Data Analytics \u00b6 Real-time analytics on Kinsesis Streams using SQL. Auto-scales. Fully managed, no services to provision. Continuous analytics (real time). Pay for actual consumption. Create streams out of real-time queries. Kinesis Firehose \u00b6 Fully managed, no administration. Near real time (60 seconds delay). Used for ETLs. Load data into Redshift/S3/ElasticSearch/Splunk. Automatic scaling. Supports alot of data formats. There's a conversion fee to convert between formats. Pay for the amount of data going through Firehose. SQS vs SNS vs Kinesis \u00b6 SQS \u00b6 Consumers pull data. Data is deleted once consumed. Can have as many consumers as you want. Throughput isn't provisioned. Ordering is not guaranteed unless you use a FIFO queue. Messages can be delayed if required. SNS \u00b6 Data is pushed to subscribers (pub/sub model). Up to 10m subscribers. Data is not persisted, it's lost if it isn't delivered. Up to 10,000 topics. Throughput is not provisioned. Integrates with SQS for fan-out architecture. Kinesis \u00b6 Consumers pull data. Can have as many consumers are you want. Data can be replayed. Meant for real-time big data, analytics and ETL. Records within the shard are ordered according to partition key. Data expires after X days. Throughput is provisioned. Kinesis Data Ordering vs SQS FIFO Queue \u00b6 Use the partition key to control the order of messages in a Kinesis Stream. The same partition key will always go to the same shard in a Kinesis Stream. SQS standard isn't ordered at all. SQS FIFO messages without a group id will be consumed by a single consumer in the order they're sent. SQS FIFO messages with a group id, will be consumed by multiple consumers (one consumer per group, messages processed in the order they arrive for that group. Very similar to a Kinesis partition key). Example \u00b6 There are 100 trucks sending GPS data, 5 kinesis shards and 1 SQS FIFO. Kinesis Data Stream \u00b6 On average, 20 trucks per shard (x 5 shards). Data is ordered within each shard. Maximum of 5 consumers (one per shard). Up to 5MB/s of data (1MB/s per shard). SQS FIFO \u00b6 100 Group IDs (one per truck). Data is ordered within each group id (or in the order they arrive into the queue if no group id). Maximum of 100 consumers (one per Group ID). Up to 300 messages/second, or 3,000 if using batching.","title":"Kinesis"},{"location":"messaging/kinesis/#kinesis","text":"","title":"Kinesis"},{"location":"messaging/kinesis/#overview","text":"Managed alternative to Kafka. Data is replicated across 3 AZ. Can quickly become expensive.","title":"Overview"},{"location":"messaging/kinesis/#use-cases","text":"application logs, metrics IoT, Clickstreams. \"Real-time\" big data. Streaming processing frameworks (Spark, NiFi). Shard level metrics can be in 1minute intervals.","title":"Use Cases"},{"location":"messaging/kinesis/#kinesis-streams","text":"Low latency streaming ingestion at scale. Streams are divided into ordered Shards/Partitions. Increase shards to increase throughput. Data retention is 1 day by default. Can be up to 7 days. Can reprocess/replay data. Multiple applications can consume the same stream. Real-time processing with scale of throughput. Data inserted into Kinesis can't be deleted (immutable).","title":"Kinesis Streams"},{"location":"messaging/kinesis/#shards","text":"Write 1MB/s, or 1000 messages per shard. Read 2MB/s per shard. Can provision up to 200 shards. Billing is per shard (provisioned). Messages can be batches. Can reshard/merge shards over time. Records are ordered per shard.","title":"Shards"},{"location":"messaging/kinesis/#kinesis-analytics","text":"Real-time analytics on streams using SQL","title":"Kinesis Analytics"},{"location":"messaging/kinesis/#kinesis-firehose","text":"Load streams into S3, Redshift, ElasticSearch etc.","title":"Kinesis Firehose"},{"location":"messaging/kinesis/#producers","text":"Put records are used to send data to Kinesis. Need to provide the partition key. The message key is hashed to determine the shard id. The same key goes to the same partition (helps with ordering for a specific key). Message data must be base64 encoded. Each message is given a sequence number (sequential). Use a partition key that's highly distributed to avoid a \"hot partition\", which would result in all the data being sent to the same shard. user_id if many users. country_id not a good choice is all the users are in one country. Use Batching to reduce cost and increase throughput.","title":"Producers"},{"location":"messaging/kinesis/#handling-provisionedthroughputexceeded-exceptions","text":"Happens when MB/s or TPS are exceeded on a shard. Three solutions to the problem: Retries with backoff. Increase the shards (scaling). Use a more distributed (unique) partition key.","title":"Handling ProvisionedThroughputExceeded Exceptions"},{"location":"messaging/kinesis/#consumers","text":"Can use the SDK, CLI or Kinesis Client Library. Kinesis Client Library uses DynamoDB to checkpoint offsets, track other workers, and share the work amongst available shards. Pass the shart iterator to get records --shard-iterator to retrieve the messages for a given shard id.","title":"Consumers"},{"location":"messaging/kinesis/#kinesis-client-library-kcl","text":"Java library for distributed applications that helps read records from Kinesis Streams, to share the read workload across multiple nodes. Each shard can only be ready by one KCL instance. ie: if there's 4 shards, the maximum number of KCL instances is 4. DynamoDB is used to checkpoint progress and co-ordinate consumption of shards across the KCL instances (requires IAM access to write to DynamoDB). KCL can run on EC2, Elastic Beanstalk, On-Prem applications etc. Record are read in the order that they were placed into the shard.","title":"Kinesis Client Library (KCL)"},{"location":"messaging/kinesis/#security","text":"IAM policies to control access/authorization. VPC endpoints are available to allow access to Kinesis from within a VPC.","title":"Security"},{"location":"messaging/kinesis/#encryption","text":"Encryption in transit via HTTPS. Encryption at rest using KMS Possible to use client side encryption.","title":"Encryption"},{"location":"messaging/kinesis/#kinesis-data-analytics","text":"Real-time analytics on Kinsesis Streams using SQL. Auto-scales. Fully managed, no services to provision. Continuous analytics (real time). Pay for actual consumption. Create streams out of real-time queries.","title":"Kinesis Data Analytics"},{"location":"messaging/kinesis/#kinesis-firehose_1","text":"Fully managed, no administration. Near real time (60 seconds delay). Used for ETLs. Load data into Redshift/S3/ElasticSearch/Splunk. Automatic scaling. Supports alot of data formats. There's a conversion fee to convert between formats. Pay for the amount of data going through Firehose.","title":"Kinesis Firehose"},{"location":"messaging/kinesis/#sqs-vs-sns-vs-kinesis","text":"","title":"SQS vs SNS vs Kinesis"},{"location":"messaging/kinesis/#sqs","text":"Consumers pull data. Data is deleted once consumed. Can have as many consumers as you want. Throughput isn't provisioned. Ordering is not guaranteed unless you use a FIFO queue. Messages can be delayed if required.","title":"SQS"},{"location":"messaging/kinesis/#sns","text":"Data is pushed to subscribers (pub/sub model). Up to 10m subscribers. Data is not persisted, it's lost if it isn't delivered. Up to 10,000 topics. Throughput is not provisioned. Integrates with SQS for fan-out architecture.","title":"SNS"},{"location":"messaging/kinesis/#kinesis_1","text":"Consumers pull data. Can have as many consumers are you want. Data can be replayed. Meant for real-time big data, analytics and ETL. Records within the shard are ordered according to partition key. Data expires after X days. Throughput is provisioned.","title":"Kinesis"},{"location":"messaging/kinesis/#kinesis-data-ordering-vs-sqs-fifo-queue","text":"Use the partition key to control the order of messages in a Kinesis Stream. The same partition key will always go to the same shard in a Kinesis Stream. SQS standard isn't ordered at all. SQS FIFO messages without a group id will be consumed by a single consumer in the order they're sent. SQS FIFO messages with a group id, will be consumed by multiple consumers (one consumer per group, messages processed in the order they arrive for that group. Very similar to a Kinesis partition key).","title":"Kinesis Data Ordering vs SQS FIFO Queue"},{"location":"messaging/kinesis/#example","text":"There are 100 trucks sending GPS data, 5 kinesis shards and 1 SQS FIFO.","title":"Example"},{"location":"messaging/sns/","text":"Simple Notification Service (SNS) \u00b6 Overview \u00b6 Pub/Sub service. Allows multiple subscribers to receive a message from the SNS topic (queue). Event Producer only sends a message to one SNS topic. One or more Event Recievers (subscriptions) listen to the SNS topic. Each subscription to the topic will get all messages. Messages can optionally be filtered. Up to 10million subscriptions per topic. Up to 100,000 topics. Subscribers can be SQS, HTTP/HTTPS, Lambda, Email, SMS messages or Mobile notifications. Most AWS services can send data to SNS for notifications. Subscription Protocols \u00b6 HTTP HTTPS Email Email-JSON Amazon SQS Amazone Lambda Publishing \u00b6 Topic Publish (Using the SDK) \u00b6 Create a topic. Create a subscription(s). Publish to the topic. Direct Publish (for mobile apps SDK) \u00b6 Create a platform application. Create a platform endpoint. Publish to the platform endpoint. Works with Google GCM, Apple APNS, Amazon ADM etc. Security \u00b6 Encryption \u00b6 Encryption during transit via HTTPS. Encryption at rest using KMS. Client-side encryption if desired. Access Controls \u00b6 IAM policies regulat access to the SNS API. SNS Access Policies, similar to S3 Bucket Policies for cross-account access to SNS topips, or allowing other service to write to an SNS topic. SNS + SQS Fan Out Pattern \u00b6 Push a message to SNS, and it will be recieved by all SQS queues that are subscribers. Fully de-coupled, no data loss. SQS allows data persistence, delayed processing and retries of work. Easy to add more SQS subscribers over time. Useful for micro-services. SQS Queue accesspolicy needs to allow SNS to write. SNS can't send messages to SQS FIFO queues. S3 Events to Multiple Queues \u00b6 Can only have on S3 Event rule for the same event type (eg: object create) & prefix (eg: images). Use Fan-out pattern to send the same S3 event to many SQS queues.","title":"Simple Notification Service (SNS)"},{"location":"messaging/sns/#simple-notification-service-sns","text":"","title":"Simple Notification Service (SNS)"},{"location":"messaging/sns/#overview","text":"Pub/Sub service. Allows multiple subscribers to receive a message from the SNS topic (queue). Event Producer only sends a message to one SNS topic. One or more Event Recievers (subscriptions) listen to the SNS topic. Each subscription to the topic will get all messages. Messages can optionally be filtered. Up to 10million subscriptions per topic. Up to 100,000 topics. Subscribers can be SQS, HTTP/HTTPS, Lambda, Email, SMS messages or Mobile notifications. Most AWS services can send data to SNS for notifications.","title":"Overview"},{"location":"messaging/sns/#subscription-protocols","text":"HTTP HTTPS Email Email-JSON Amazon SQS Amazone Lambda","title":"Subscription Protocols"},{"location":"messaging/sns/#publishing","text":"","title":"Publishing"},{"location":"messaging/sns/#topic-publish-using-the-sdk","text":"Create a topic. Create a subscription(s). Publish to the topic.","title":"Topic Publish (Using the SDK)"},{"location":"messaging/sns/#direct-publish-for-mobile-apps-sdk","text":"Create a platform application. Create a platform endpoint. Publish to the platform endpoint. Works with Google GCM, Apple APNS, Amazon ADM etc.","title":"Direct Publish (for mobile apps SDK)"},{"location":"messaging/sns/#security","text":"","title":"Security"},{"location":"messaging/sns/#encryption","text":"Encryption during transit via HTTPS. Encryption at rest using KMS. Client-side encryption if desired.","title":"Encryption"},{"location":"messaging/sns/#access-controls","text":"IAM policies regulat access to the SNS API. SNS Access Policies, similar to S3 Bucket Policies for cross-account access to SNS topips, or allowing other service to write to an SNS topic.","title":"Access Controls"},{"location":"messaging/sns/#sns-sqs-fan-out-pattern","text":"Push a message to SNS, and it will be recieved by all SQS queues that are subscribers. Fully de-coupled, no data loss. SQS allows data persistence, delayed processing and retries of work. Easy to add more SQS subscribers over time. Useful for micro-services. SQS Queue accesspolicy needs to allow SNS to write. SNS can't send messages to SQS FIFO queues.","title":"SNS + SQS Fan Out Pattern"},{"location":"messaging/sns/#s3-events-to-multiple-queues","text":"Can only have on S3 Event rule for the same event type (eg: object create) & prefix (eg: images). Use Fan-out pattern to send the same S3 event to many SQS queues.","title":"S3 Events to Multiple Queues"},{"location":"messaging/sqs/","text":"Simple Queue Service \u00b6 Overview \u00b6 Simple Queue Service. Two queue types - Standard, and FIFO. SQS - Standard Queue \u00b6 Fully managed. Use to decouple applications. Unlimited throughput & queue size. Default message retention period is 4 days, maximum is 14 days. Low latency (< 10ms public and recieve). 256KB message size limit. Messages can be duplicated, and be delivered out of order (delivered at least once, best effort ordering). After a message has been re-queued multiple times, it can be moved to the Dead Letter Queue (DLQ). Producers \u00b6 Send messages to a SQS queue using the SendMessage API. Message is persisted until a consumer deletes it. Consumers \u00b6 Poll message queues, process the messages, and deleted them off the queue. Running on EC2,Lambda or on-prem infrastructure. Recieves upto 10 messages at a time. Delete messages using the DeleteMessage API. Each SQS queue can have multiple consumers recieving and processing messages. Horizontal scaling to increase message throughput. CloudWatch metric is available called Queue Length , which can be used to increase/decrease the capacity of the ASG based on the volume of messages. Security \u00b6 Encryption during transit using HTTPS (enabled by default). Encryption at rest using KMS. Client-side encryption is supported. Access Policies \u00b6 Similar to S3 bucket policies. Useful for cross-account access or allowing other services to write to a SQS queue. Message Visibility Timeout \u00b6 Default message visibility timeout is 30seconds. Maximum message visibility timeout is 12hrs. Once a message is polled by a consumer, it's invisible to other consumers. Once the visibility timeout is reached, the message will be put back into the queue and made visible to consumers. Use the ChangeMessageVisibility API from the consumer when it needs more time to process a message. If visibility timeout is too high and the consumer crashes, it will take a long time to re-process the message. If visibility timeout is too low, it may generate duplicate messages. Dead Letter Queues \u00b6 Can set a MaximumReceives threshold limiting how many times the queue can go back into the queue. When threshold is reached, message will be moved to the Dead Letter Queue (DLQ) where another app can be used to analyse the message and debug why it wasn't processed. DLQs are subject to retention periods, best practise is to set it to the maximum of 14 days. Delay Queues \u00b6 Delay a message before making it visible to consumers. Maximum delay is 15mins. Can set a default delay at the queue level. Can override the default on specific messages using the DelaySeconds parameter. Long Polling \u00b6 Consumer waits for messages to arrive if there's none in the queue. Decreases the number of API calls made to SQS. As soon as SQS recieves a message, it will send it to the Consumer. Wait time can be between 1-20secs. Enabled at the Queue level, or at the API level using WaitTimeSeconds . Should be preferred to short polling. SQS Extended Client \u00b6 Java library that uses an S3 bucket too store large messages, to work-around the 256KB size limit. Important APIs \u00b6 API Description CreateQueue Creates a queue, use MessageRetentionPeriod to define retention period. DeleteQueue Delete a queue. PurgeQueue Delete all the messages in a queue. SendMessage Used by producer to send a message. Use DelaySeconds to control the message delay. ReceiveMessage Used by consumer to receive a message. DeleteMessage Used by a consumer to delete a message. ReceiveMessageWaitTimeSeconds Used by a consumer to recieve a message using long polling. ChangeMessageVisibility Change the message timeout. SendMesage , DeleteMessage and ChangeMessageVisibility have batch APIs to help decrease costs. SQS - FIFO Queue \u00b6 Messages are processed by the consumer in the order they arrived. Max throughput is 300 messages/sec, or 3000 messages/sec with batching. Exactly-once send capability (removes duplicates). Each message needs to specify its group ID, and provide a token (deduplication ID) used for de-duplication. Deduplication \u00b6 De-duplication interval is 5mins. If the same message is sent multiple times within 5mins, the duplicates will be deleted. Deduplication Methods \u00b6 Content-based deduplication: Based on the SHA256 hash of the message body. Explicitly provide a message deduplication id. Message Grouping \u00b6 If MessageGroupID has the same value in a FIFO queue, you can only have one consumer, and all the messages are processed in order. If different MessageGroupID values are provided, you can group a subset of messages. Each Group ID can have a different consumer to support parallel processing. Ordering across groups isn't guaranteed.","title":"Simple Queue Service"},{"location":"messaging/sqs/#simple-queue-service","text":"","title":"Simple Queue Service"},{"location":"messaging/sqs/#overview","text":"Simple Queue Service. Two queue types - Standard, and FIFO.","title":"Overview"},{"location":"messaging/sqs/#sqs-standard-queue","text":"Fully managed. Use to decouple applications. Unlimited throughput & queue size. Default message retention period is 4 days, maximum is 14 days. Low latency (< 10ms public and recieve). 256KB message size limit. Messages can be duplicated, and be delivered out of order (delivered at least once, best effort ordering). After a message has been re-queued multiple times, it can be moved to the Dead Letter Queue (DLQ).","title":"SQS - Standard Queue"},{"location":"messaging/sqs/#producers","text":"Send messages to a SQS queue using the SendMessage API. Message is persisted until a consumer deletes it.","title":"Producers"},{"location":"messaging/sqs/#consumers","text":"Poll message queues, process the messages, and deleted them off the queue. Running on EC2,Lambda or on-prem infrastructure. Recieves upto 10 messages at a time. Delete messages using the DeleteMessage API. Each SQS queue can have multiple consumers recieving and processing messages. Horizontal scaling to increase message throughput. CloudWatch metric is available called Queue Length , which can be used to increase/decrease the capacity of the ASG based on the volume of messages.","title":"Consumers"},{"location":"messaging/sqs/#security","text":"Encryption during transit using HTTPS (enabled by default). Encryption at rest using KMS. Client-side encryption is supported.","title":"Security"},{"location":"messaging/sqs/#message-visibility-timeout","text":"Default message visibility timeout is 30seconds. Maximum message visibility timeout is 12hrs. Once a message is polled by a consumer, it's invisible to other consumers. Once the visibility timeout is reached, the message will be put back into the queue and made visible to consumers. Use the ChangeMessageVisibility API from the consumer when it needs more time to process a message. If visibility timeout is too high and the consumer crashes, it will take a long time to re-process the message. If visibility timeout is too low, it may generate duplicate messages.","title":"Message Visibility Timeout"},{"location":"messaging/sqs/#dead-letter-queues","text":"Can set a MaximumReceives threshold limiting how many times the queue can go back into the queue. When threshold is reached, message will be moved to the Dead Letter Queue (DLQ) where another app can be used to analyse the message and debug why it wasn't processed. DLQs are subject to retention periods, best practise is to set it to the maximum of 14 days.","title":"Dead Letter Queues"},{"location":"messaging/sqs/#delay-queues","text":"Delay a message before making it visible to consumers. Maximum delay is 15mins. Can set a default delay at the queue level. Can override the default on specific messages using the DelaySeconds parameter.","title":"Delay Queues"},{"location":"messaging/sqs/#long-polling","text":"Consumer waits for messages to arrive if there's none in the queue. Decreases the number of API calls made to SQS. As soon as SQS recieves a message, it will send it to the Consumer. Wait time can be between 1-20secs. Enabled at the Queue level, or at the API level using WaitTimeSeconds . Should be preferred to short polling.","title":"Long Polling"},{"location":"messaging/sqs/#sqs-extended-client","text":"Java library that uses an S3 bucket too store large messages, to work-around the 256KB size limit.","title":"SQS Extended Client"},{"location":"messaging/sqs/#important-apis","text":"API Description CreateQueue Creates a queue, use MessageRetentionPeriod to define retention period. DeleteQueue Delete a queue. PurgeQueue Delete all the messages in a queue. SendMessage Used by producer to send a message. Use DelaySeconds to control the message delay. ReceiveMessage Used by consumer to receive a message. DeleteMessage Used by a consumer to delete a message. ReceiveMessageWaitTimeSeconds Used by a consumer to recieve a message using long polling. ChangeMessageVisibility Change the message timeout. SendMesage , DeleteMessage and ChangeMessageVisibility have batch APIs to help decrease costs.","title":"Important APIs"},{"location":"messaging/sqs/#sqs-fifo-queue","text":"Messages are processed by the consumer in the order they arrived. Max throughput is 300 messages/sec, or 3000 messages/sec with batching. Exactly-once send capability (removes duplicates). Each message needs to specify its group ID, and provide a token (deduplication ID) used for de-duplication.","title":"SQS - FIFO Queue"},{"location":"messaging/sqs/#deduplication","text":"De-duplication interval is 5mins. If the same message is sent multiple times within 5mins, the duplicates will be deleted.","title":"Deduplication"},{"location":"messaging/sqs/#message-grouping","text":"If MessageGroupID has the same value in a FIFO queue, you can only have one consumer, and all the messages are processed in order. If different MessageGroupID values are provided, you can group a subset of messages. Each Group ID can have a different consumer to support parallel processing. Ordering across groups isn't guaranteed.","title":"Message Grouping"},{"location":"security/kms/","text":"AWS Key Management Service (KMS) \u00b6 Managed key management service. Integrated with IAM for authorization. Integrated into most AWS services that need to use encryption. Can be used to create, enable/disable keys, or configure rotation policies. Audit key usage via Cloudtrail. Pay for each call to KMS. Can only encrypt upto 4KB of data per call. For more than 4KB use envelope encryption. To provide access, the Key Policy needs to allow the user, and the IAM policy needs to allow API calls. Keys are bound to a region. Enable encryption helpers in lambda function, and give it the KMS CMK to encrypt/decrypt variables. Use Cases \u00b6 Database passwords. Credentials to external services. SSL cert private keys. Customer Master Keys (CMK) \u00b6 The CMK used to encrypt/decrypt data can't be retrieved by the user. There's 3 types of CMKs - AWS Managed Service (free). User created keys ($1/month). Import keys ($1/month). Symmetric (AES265) Keys \u00b6 A single encryption key is used for encryption & decryption. Mandatory for envelope encryption. Access to the unencrypted key is not possible. Used by AWS services that integrate with KMS. Encrypt API will encrypt up to 4KB of data. Decrypt API will decrypt up to 4KB of data. GenerateDataKey API will generate a unique Data Encryption Key (DEK). It returns a plain-text copy, as well as an encrypted version stored under the CMK. GenerateDataKeyWithoutPlaintext API will generate a DEK and place it under the CMK. Doesn't return a plain-text copy. Use Decrypt later on to get the plain-text version. GenerateRandom API will create a random byte string. Assymetric Keys (RSA and ECC key pairs) \u00b6 Public key for decryption, and private key for encryption (eg: SSL). Used for encrypt/decrypt, or sign/verify operations. Public key is downloadable, private key isn't accessible in unencrypted form. Useful for encryption outside of AWS by users that can't call the KMS API. Copying Snapshots Between Regions \u00b6 Attach a KMS Key Policy that authorizes cross-account access. KMS Key Policies \u00b6 Control access to KMS keys, similar to S3 policies. Can't control access without them. Default KMS Key Policy \u00b6 Root user has complete access to the key. Give users access by attaching an appropriate IAM policy. Custom KMS Key Policy \u00b6 Define users & roles that can access the key. Define who can administer the key. Useful for cross-account access of the key. Encryption Patterns \u00b6 Envelope Encryption \u00b6 Use the GenerateDataKey API to obtain a data encryption key. Use client-side encryption to encrypt the large file using the data encryption key. Place the encrypted DEK provided by AWS, and the encrypted file into a new file (the envelope). To decrypt, send the encrypted DEK to the Decrypt API, then use the decrypted DEK to decrypt the file. Encryption SDK \u00b6 Has a Data Key Caching feature to allow a data key to be re-used, instead of using a new key for every encryption. Reduces the number of calls to KMS, but reduces security. Use LocalCryptoMaterialsCache (max age, max bytes, max number of messages) to define how big the data cache should be. Request Quotas \u00b6 All crypto operations for an account in a particular region are subject to a request/sec quota - Decrypt Encrypt GenerateDataKey GenerateDataKeyWithoutPlaintext GenerateRandom ReEncrypt Sign (asymmetric) Verify (asymmetric) A ThrottlingException will be raised if the request quota is exceeded. There's three ways to deal with the problem: Use an exponential backoff. Use envelope encryption + DEK caching to reduce the number of GenerateDataKey requests. Request a quota increase through the API, or an AWS support ticket. Symmetric Keys \u00b6 5,500 requests/sec. 10,000 in us-east-2, ap-southeast-1, ap-southeast-2, ap-northeast-1, eu-central-1, eu-west-2. 30,000 in us-east-1, us-west-1, eu-west-1. Asymmetric Keys \u00b6 500 for RSA CMKs. 300 for ECC CMKs.","title":"AWS Key Management Service (KMS)"},{"location":"security/kms/#aws-key-management-service-kms","text":"Managed key management service. Integrated with IAM for authorization. Integrated into most AWS services that need to use encryption. Can be used to create, enable/disable keys, or configure rotation policies. Audit key usage via Cloudtrail. Pay for each call to KMS. Can only encrypt upto 4KB of data per call. For more than 4KB use envelope encryption. To provide access, the Key Policy needs to allow the user, and the IAM policy needs to allow API calls. Keys are bound to a region. Enable encryption helpers in lambda function, and give it the KMS CMK to encrypt/decrypt variables.","title":"AWS Key Management Service (KMS)"},{"location":"security/kms/#use-cases","text":"Database passwords. Credentials to external services. SSL cert private keys.","title":"Use Cases"},{"location":"security/kms/#customer-master-keys-cmk","text":"The CMK used to encrypt/decrypt data can't be retrieved by the user. There's 3 types of CMKs - AWS Managed Service (free). User created keys ($1/month). Import keys ($1/month).","title":"Customer Master Keys (CMK)"},{"location":"security/kms/#symmetric-aes265-keys","text":"A single encryption key is used for encryption & decryption. Mandatory for envelope encryption. Access to the unencrypted key is not possible. Used by AWS services that integrate with KMS. Encrypt API will encrypt up to 4KB of data. Decrypt API will decrypt up to 4KB of data. GenerateDataKey API will generate a unique Data Encryption Key (DEK). It returns a plain-text copy, as well as an encrypted version stored under the CMK. GenerateDataKeyWithoutPlaintext API will generate a DEK and place it under the CMK. Doesn't return a plain-text copy. Use Decrypt later on to get the plain-text version. GenerateRandom API will create a random byte string.","title":"Symmetric (AES265) Keys"},{"location":"security/kms/#assymetric-keys-rsa-and-ecc-key-pairs","text":"Public key for decryption, and private key for encryption (eg: SSL). Used for encrypt/decrypt, or sign/verify operations. Public key is downloadable, private key isn't accessible in unencrypted form. Useful for encryption outside of AWS by users that can't call the KMS API.","title":"Assymetric Keys (RSA and ECC key pairs)"},{"location":"security/kms/#copying-snapshots-between-regions","text":"Attach a KMS Key Policy that authorizes cross-account access.","title":"Copying Snapshots Between Regions"},{"location":"security/kms/#kms-key-policies","text":"Control access to KMS keys, similar to S3 policies. Can't control access without them.","title":"KMS Key Policies"},{"location":"security/kms/#default-kms-key-policy","text":"Root user has complete access to the key. Give users access by attaching an appropriate IAM policy.","title":"Default KMS Key Policy"},{"location":"security/kms/#custom-kms-key-policy","text":"Define users & roles that can access the key. Define who can administer the key. Useful for cross-account access of the key.","title":"Custom  KMS Key Policy"},{"location":"security/kms/#encryption-patterns","text":"","title":"Encryption Patterns"},{"location":"security/kms/#envelope-encryption","text":"Use the GenerateDataKey API to obtain a data encryption key. Use client-side encryption to encrypt the large file using the data encryption key. Place the encrypted DEK provided by AWS, and the encrypted file into a new file (the envelope). To decrypt, send the encrypted DEK to the Decrypt API, then use the decrypted DEK to decrypt the file.","title":"Envelope Encryption"},{"location":"security/kms/#encryption-sdk","text":"Has a Data Key Caching feature to allow a data key to be re-used, instead of using a new key for every encryption. Reduces the number of calls to KMS, but reduces security. Use LocalCryptoMaterialsCache (max age, max bytes, max number of messages) to define how big the data cache should be.","title":"Encryption SDK"},{"location":"security/kms/#request-quotas","text":"All crypto operations for an account in a particular region are subject to a request/sec quota - Decrypt Encrypt GenerateDataKey GenerateDataKeyWithoutPlaintext GenerateRandom ReEncrypt Sign (asymmetric) Verify (asymmetric) A ThrottlingException will be raised if the request quota is exceeded. There's three ways to deal with the problem: Use an exponential backoff. Use envelope encryption + DEK caching to reduce the number of GenerateDataKey requests. Request a quota increase through the API, or an AWS support ticket.","title":"Request Quotas"},{"location":"security/kms/#symmetric-keys","text":"5,500 requests/sec. 10,000 in us-east-2, ap-southeast-1, ap-southeast-2, ap-northeast-1, eu-central-1, eu-west-2. 30,000 in us-east-1, us-west-1, eu-west-1.","title":"Symmetric Keys"},{"location":"security/kms/#asymmetric-keys","text":"500 for RSA CMKs. 300 for ECC CMKs.","title":"Asymmetric Keys"},{"location":"security/secrets_manager/","text":"Secrets Manager \u00b6 Overview \u00b6 Can force rotation of secrets (between 30 and 365 days). Automation secrets generation using lambda. Integrates with RDS. Use KMS (CMK) to encrypt secrets. Can pull a Secrets Manager secret via the SSM Parameter Store api.","title":"Secrets Manager"},{"location":"security/secrets_manager/#secrets-manager","text":"","title":"Secrets Manager"},{"location":"security/secrets_manager/#overview","text":"Can force rotation of secrets (between 30 and 365 days). Automation secrets generation using lambda. Integrates with RDS. Use KMS (CMK) to encrypt secrets. Can pull a Secrets Manager secret via the SSM Parameter Store api.","title":"Overview"},{"location":"security/ssm/","text":"SSM Parameter Store \u00b6 Overview \u00b6 Securely store configuration/secrets. Encrypt using KMS. Supports versioning. Supports configuration management via path (pull all secrets within that path) & IAM. Integrates with CloudWatch & CloudFormation. Supported parameter values are String , StringList or SecureString . To get specific parameter(s) - aws ssm get-parameters --names <name 1>, <name 2> . To get parameters via path - aws ssm get-parameters-by-path --path /some/path . Tiering \u00b6 Tier Max Parameters (per account, per region) Max Param Size Suppports Parameter Policies? Cost Storage Cost API price Standard 10,000 4KB No None Free Standard Throughput: Free, Higher throughput: $0.05/10,000 calls. Advanced 100,000 8KB Yes Additional $0.05/param $0.05/10,000 calls. Parameter Policies \u00b6 Assign a TTL to parameter to force a delete/update. Supports multiple policies. Use Expiration , ExpiresNotification or NoChangeNotification to define expiration policies, and when to be notified of lifecycle events. Secure Strings \u00b6 Use the SecureString parameter type, to encrypt using KMS. You can use the current account, or another AWS Account as the KMS key source. Add the --with-decryption to the CLI arg to decrypt the string value using KMS.","title":"SSM Parameter Store"},{"location":"security/ssm/#ssm-parameter-store","text":"","title":"SSM Parameter Store"},{"location":"security/ssm/#overview","text":"Securely store configuration/secrets. Encrypt using KMS. Supports versioning. Supports configuration management via path (pull all secrets within that path) & IAM. Integrates with CloudWatch & CloudFormation. Supported parameter values are String , StringList or SecureString . To get specific parameter(s) - aws ssm get-parameters --names <name 1>, <name 2> . To get parameters via path - aws ssm get-parameters-by-path --path /some/path .","title":"Overview"},{"location":"security/ssm/#tiering","text":"Tier Max Parameters (per account, per region) Max Param Size Suppports Parameter Policies? Cost Storage Cost API price Standard 10,000 4KB No None Free Standard Throughput: Free, Higher throughput: $0.05/10,000 calls. Advanced 100,000 8KB Yes Additional $0.05/param $0.05/10,000 calls.","title":"Tiering"},{"location":"security/ssm/#parameter-policies","text":"Assign a TTL to parameter to force a delete/update. Supports multiple policies. Use Expiration , ExpiresNotification or NoChangeNotification to define expiration policies, and when to be notified of lifecycle events.","title":"Parameter Policies"},{"location":"security/ssm/#secure-strings","text":"Use the SecureString parameter type, to encrypt using KMS. You can use the current account, or another AWS Account as the KMS key source. Add the --with-decryption to the CLI arg to decrypt the string value using KMS.","title":"Secure Strings"}]}